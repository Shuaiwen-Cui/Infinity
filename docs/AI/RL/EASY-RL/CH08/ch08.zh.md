# 第八章 DQN (连续动作)

## 习题

**8-1** 深度Q网络相比基于策略梯度的方法为什么训练效果更好、更平稳？

在深度Q网络中，只要能够估计出Q函数，就可以找到一个比较好的策略。同样地，只要能够估计出Q函数，就可以增强对应的策略。因为估计Q函数是一个比较容易的回归问题，在这个回归问题中，我们可以时刻观察模型训练的效果是不是越来越好（一般情况下我们只需要关注回归的损失有没有下降，就可以判断模型学习得好不好），所以估计Q函数相较于学习一个策略来说是比较容易的。只需要估计Q函数，就可以保证现在一定会得到比较好的策略，同样其也比较容易操作。对比来说，策略梯度方法中的优化目标是最大化总回报，但是我们很难找到一个明确的损失函数来进行优化，其本质上是一个策略搜索问题，也就是一个无约束的优化问题。

**8-2** 深度Q网络在处理连续动作时存在什么样的问题呢？对应的解决方法有哪些呢？

我们在日常生活中常见的问题大都是包含连续动作的，例如智能体要进行自动驾驶，其就需要决定方向盘要左转几度或右转几度，这就是连续的动作；假设智能体是一个机器人，它身上有50个关节，它的每一个动作就对应到这50个关节的角度，这些角度也是连续的。

然而在使用深度Q网络时，很重要的一步是要求能够解决对应的优化问题。当我们预估出Q函数 $Q(s,a)$ 以后，必须要找到一个动作，它可以让 $Q(s,a)$ 最大。假设动作是离散的，那么动作 $a$ 的可能性是有限的。但如果动作是连续的，我们就不能像对离散的动作一样，穷举所有可能的动作了。

为了解决这个问题，有以下几种方案。

（1）第一个方案：我们可以使用采样方法，即随机采样出$N$个可能的动作，然后一个一个代入Q函数中，计算对应的$N$个Q值，并比较哪一个最大。但是这个方案因为使用采样方法所以不会非常精确。

（2）第二个方案：我们将这个连续动作问题，建模为一个优化问题，从而可以用梯度上升去最大化我们的目标函数。具体地，我们将动作视为变量，使用梯度上升更新动作对应的Q值。但是这个方案通常时间花销比较大，因为其需要迭代计算。

（3）第三个方案：设计一个特别的网络架构，即设计一个特别的Q函数，使得求解让Q函数最大化的动作 $a$ 变得非常容易。也就是这里的Q函数不是一个广义的Q函数，我们可以使用特殊方法设计Q函数，使得寻找让这个Q函数最大的动作 $a$ 非常容易。但是这个方案的Q函数不能随意设计，其必须有一些额外的限制。

（4）第四个方案：不用深度Q网络，毕竟用其处理连续动作比较麻烦。
