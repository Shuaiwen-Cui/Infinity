# CH03 - 线性神经网络
在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。 本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。

## 3.1. 线性回归
**回归**（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。

在机器学习领域中的大多数任务通常都与**预测**（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的预测都是回归问题。 在后面的章节中，我们将介绍分类问题。**分类**问题的目标是预测数据属于一组类别中的哪一个。

### 3.1.1. 线性回归的基本元素

线性回归（linear regression）可以追溯到19世纪初， 它在回归的各种标准工具中最简单而且最流行。 线性回归基于几个简单的假设： 首先，假设自变量$x$和因变量$y$之间的关系是线性的， 即$y$可以表示为$x$中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

为了解释线性回归，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为训练数据集（training data set） 或训练集（training set）。 每行数据（比如一次房屋交易相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。 预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。

#### 3.1.1.1. 线性模型

线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和,权重决定了每个特征对我们预测值的影响. 带权重的线性回归是输入特征的一个 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）。

给定一个数据集，我们的目标是寻找模型的权重$w$和偏置$b$， 使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过线性模型的仿射变换决定，仿射变换由所选权重和偏置确定。

而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含$d$个特征时，我们将预测结果$\hat{y}$表示为：

$$\hat{y} = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b$$

用向量表示，可以更简洁地表达为：

$$\hat{y} = \mathbf{w}^\top \mathbf{x} + b$$

其中，$\mathbf{w}$是权重向量，$\mathbf{x}$是输入向量，$b$是偏置。

在开始寻找最好的模型参数（model parameters）$w$和$bb$之前， 我们还需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。

#### 3.1.1.2 损失函数

在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。 损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。

当样本$i$的预测值为$\hat{y}^{(i)}$，真实标签为$y^{(i)}$时，平方误差可以表示为：

$$\mathcal{L}(\mathbf{w},b) = \frac{1}{n} \sum_{i=1}^n \left(\hat{y}^{(i)} - y^{(i)}\right)^2$$

常数$\frac{1}{2}$使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。 但这样不会带来本质的差别。

由于平方误差函数中的二次方项， 估计值$\hat{y}^{(i)}$和观测值$y^{(i)}$之间较大的差异将导致更大的损失。 为了度量模型在整个数据集上的质量，我们需计算在训练集个样本上的损失均值（也等价于求和）。

$$ \mathcal{L}(\mathbf{w},b) = \frac{1}{n} \sum_{i=1}^n \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2 $$

在训练时，我们希望寻找一组模型参数，记为$\mathbf{w}^*,b^*$，使得损失函数最小：

$$\mathcal{L}(\mathbf{w}^*,b^*) = \min_{\mathbf{w},b} \mathcal{L}(\mathbf{w},b)$$

#### 3.1.1.3 解析解
线性回归刚好是一个很简单的优化问题。 与我们将在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。 首先，我们将偏置$b$合并到参数$\mathbf{w}$中。 合并方法是在所有样本上添加一列，该列恒等于1。 因此，我们只需考虑一个权重向量$\mathbf{w}$，且数据样本为$\mathbf{x} \in \mathbb{R}^{d+1}$。 然后，我们可以通过对模型参数$\mathbf{w}$求导并令导数等于0来找到最小化损失函数的参数。 具体来说，我们需要求解以下线性方程组：

$$\mathbf{X}^\top \mathbf{X} \mathbf{w} = \mathbf{X}^\top \mathbf{y}$$

得到的解析解为：

$$\mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}$$

像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

#### 3.1.1.4 随机梯度下降
即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。

本书中我们用到一种名为**梯度下降**（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 **它通过不断地在损失函数递减的方向上更新参数来降低误差。**

梯度下降最简单的用法是**计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。** 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 **因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。**

在每次迭代中，我们首先随机抽样一个小批量$\mathcal{B}$，它包含了$\mathcal{B}$个数据样本。 我们用$\mathcal{L}(\mathbf{w},b;\mathcal{B})$来表示小批量$\mathcal{B}$上的损失， 并且用$\mathbf{w}$和$b$表示参数。 在小批量随机梯度下降中，模型参数在每次迭代时的更新量为：

$$\mathbf{w} \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{\mathbf{w}} \mathcal{L}^{(i)}(\mathbf{w},b)$$

$$b \leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{b} \mathcal{L}^{(i)}(\mathbf{w},b)$$

在上式中，$\eta$（取正数）叫作**学习率**（learning rate）。 由于$\mathcal{L}(\mathbf{w},b;\mathcal{B})$是关于单个样本的损失函数， 因此上式中的梯度也是基于单个样本损失函数的梯度的累加。 给定学习率$\eta$和小批量随机梯度下降所使用的批量大小， 模型参数将如何更新取决于损失曲线的形状。

我们用下面的数学公式来表示这一更新过程（$\partial$符号表示偏导数）：

$$\left( \mathbf{w}, b \right) \leftarrow \left( \mathbf{w}, b \right) - \eta \left( \frac{\partial \mathcal{L}}{\partial \mathbf{w}}, \frac{\partial \mathcal{L}}{\partial b} \right)$$

总结一下，算法的步骤如下：
（1）初始化模型参数的值，如随机初始化；
（2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。

${|\mathcal{B}|}$ 是批量大小，也是小批量随机梯度下降每次迭代的样本数， 通常取几十或几百。学习率$\eta$通常取0.01、0.1、1或者10。需要注意的是，**当批量大小为1时，小批量随机梯度下降退化为随机梯度下降。**

批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）。 调参（hyperparameter tuning）是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的。但是，即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。

线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为**泛化（generalization）。**

#### 3.1.1.5. 用模型进行预测

给定特征估计目标的过程通常称为**预测**（prediction）或**推断**（inference）。

### 3.1.2. 矢量化加速

在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

=== "PYTORCH"

    ```python
    %matplotlib inline
    import math
    import time
    import numpy as np
    import torch
    from d2l import torch as d2l
    ```
=== "TENSORFLOW"

    ```python
    %matplotlib inline
    import math
    import time
    import numpy as np
    import tensorflow as tf
    from d2l import tensorflow as d2l
    ```

### 3.1.3. 正态分布与平方损失

正态分布和线性回归之间的关系很密切。 正态分布（normal distribution），也称为高斯分布（Gaussian distribution）， 最早由德国数学家高斯（Gauss）应用于天文学研究。 简单的说，若随机变量$x$服从均值为$\mu$、方差为$\sigma^2$的正态分布，记为$x \sim \mathcal{N}(\mu,\sigma^2)$，其概率密度函数为：

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right)$$

正态分布的均值为$\mu$，方差为$\sigma^2$。 当$\mu = 0, \sigma = 1$时的正态分布称为标准正态分布（standard normal distribution）。 标准正态分布的概率密度函数简化为：

$$\phi(x) = \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{1}{2} x^2\right)$$

正态分布在机器学习中有着广泛的应用。 例如，我们将在后面的章节中看到，正态分布假设了噪声的分布，而噪声又影响了数据的标签。 事实上，我们将看到许多有用的算法都假设了噪声服从正态分布。

均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。噪声正态分布如下式:

$$ y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0,\sigma^2)$$

因此，我们现在可以写出通过给定的$x$观测到特定$y$的似然：

$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right)$$

现在，根据极大似然估计法，参数$\mathbf{w}$和$b$的最优值是使整个数据集似然最大的值。

根据极大似然估计法选择的估计量称为极大似然估计量。虽然使许多指数函数的乘积最大化看起来很困难， 但是我们可以在不改变目标的前提下，通过最大化似然对数来简化。由于历史原因，优化通常是说最小化而不是最大化。 我们可以改为最小化负对数似然$-\log P(y \mid \mathbf{x})$。由此可以得到的数学公式是：

$$-\log P(y \mid \mathbf{x}) = \frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2 + \frac{1}{2} \log(2 \pi \sigma^2)$$

现在我们只需要假设$\sigma$是一个常数，就可以忽略掉最后一项，因为这个常数不会影响$\mathbf{w}$和$b$的选择。现在，我们的最大似然估计问题转化为最小化均方误差。因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。

### 3.1.4. 从线性回归到深度网络

到目前为止，我们只谈论了线性模型。 尽管神经网络涵盖了更多更为丰富的模型，我们依然可以用描述神经网络的方式来描述线性模型， 从而把线性模型看作一个神经网络。 首先，我们用“层”符号来重写这个模型。

#### 3.1.4.1. 神经网络图
深度学习从业者喜欢绘制图表来可视化模型中正在发生的事情。对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换称为全连接层（fully-connected layer）或称为稠密层（dense layer）。 下一章将详细讨论由这些层组成的网络。

#### 3.1.4.2. 生物学

### 3.1.5. 小结
- 机器学习模型中的关键要素是**训练数据**、**模型**、**损失函数**，以及**优化算法**。

- **矢量化**使数学表达上更简洁，同时运行的更快。

- **最小化目标函数**和**执行极大似然估计**等价。

- **线性回归模型**也是一个简单的神经网络。

## 3.2. 线性回归的从零开始实现
在了解线性回归的关键思想之后，我们可以开始通过代码来动手实现线性回归了。 在这一节中，我们将从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。 同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。 在这一节中，我们将只使用张量和自动求导。 在之后的章节中，我们会充分利用深度学习框架的优势，介绍更简洁的实现方式。
### 3.2.1. 生成数据集
为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。 我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 我们将使用低维数据，这样可以很容易地将其可视化。 在下面的代码中，我们生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。 我们的合成数据集是一个矩阵$\mathbf{X} \in \mathbb{R}^{1000 \times 2}$。

我们使用线性模型参数$\mathbf{w} = [2, -3.4]^\top$、$b = 4.2$和噪声项$\epsilon$生成数据集及其标签：

$$\mathbf{y} = \mathbf{X} \mathbf{w} + b + \mathbf{\epsilon}$$

$\mathbf{\epsilon}$可以视为模型预测和标签时的潜在观测误差。 在这里我们认为标准假设成立，即服从均值为0的正态分布。 为了简化问题，我们将标准差设为0.01。

### 3.2.2. 读取数据集
回想一下，训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。 由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数， 该函数能打乱数据集中的样本并以小批量方式获取数据。
当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。 上面实现的迭代对教学来说很好，但它的执行效率很低，可能会在实际问题上陷入麻烦。 例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。 在深度学习框架中实现的内置迭代器效率要高得多， 它可以处理存储在文件中的数据和数据流提供的数据。

### 3.2.3. 初始化模型参数
在我们开始用小批量随机梯度下降优化我们的模型参数之前， 我们需要先有一些参数。 在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0。
在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。 因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。 我们使用 2.5节中引入的自动微分来计算梯度。

### 3.2.4. 定义模型
接下来，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。回想一下，要计算线性模型的输出， 我们只需计算输入特征$X$和模型权重$w$的矩阵-向量乘法后加上偏置$b$。 注意，上面的$Xw$是一个向量，而$b$是一个标量。回顾向量相加的广播机制，我们可以直接将$b$加到$Xw$的每一行。

### 3.2.5. 定义损失函数
因为需要计算损失函数的梯度，所以我们应该先定义损失函数。 这里我们使用 3.1节中描述的平方损失函数。 在实现中，我们需要将真实值y的形状转换为和预测值y_hat的形状相同。

### 3.2.6. 定义优化算法
!!! note
    在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。 接下来，朝着减少损失的方向更新我们的参数。


### 3.2.7. 训练
现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。 理解这段代码至关重要，因为从事深度学习后， 相同的训练过程几乎一遍又一遍地出现。 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法sgd来更新模型参数。

!!! note
    概括一下，我们将执行以下循环：
    - 初始化参数
    - 重复，直到完成
        - 计算梯度
        - 更新参数

注意，我们不应该想当然地认为我们能够完美地求解参数。 **在机器学习中，我们通常不太关心恢复真正的参数，而更关心如何高度准确预测参数。** 幸运的是，即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解。 其中一个原因是，在深度网络中存在许多参数组合能够实现高度精确的预测。


### 3.2.8. 小结

- 我们学习了深度网络是如何实现和优化的。在这一过程中只使用张量和自动微分，不需要定义层或复杂的优化器。
- 这一节只触及到了表面知识。在下面的部分中，我们将基于刚刚介绍的概念描述其他模型，并学习如何更简洁地实现其他模型。


## 3.3. 线性回归的简洁实现
在过去的几年里，出于对深度学习强烈的兴趣， 许多公司、学者和业余爱好者开发了各种成熟的开源框架。 这些框架可以自动化基于梯度的学习算法中重复性的工作。 在 3.2节中，我们只运用了： （1）通过张量来进行数据存储和线性代数； （2）通过自动微分来计算梯度。 实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库也为我们实现了这些组件。

本节将介绍如何通过使用深度学习框架来简洁地实现 3.2节中的线性回归模型。

### 3.3.1. 生成数据集
~

### 3.3.2. 读取数据集
我们可以调用框架中现有的API来读取数据。 我们将features和labels作为API的参数传递，并通过数据迭代器指定batch_size。 此外，布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。

### 3.3.3. 定义模型
当我们在 3.2节中实现线性回归时， 我们明确定义了模型参数变量，并编写了计算的代码，这样通过基本的线性代数运算得到输出。 但是，如果模型变得更加复杂，且当我们几乎每天都需要实现模型时，自然会想简化这个过程。 这种情况类似于为自己的博客从零开始编写网页。 做一两次是有益的，但如果每个新博客就需要工程师花一个月的时间重新开始编写网页，那并不高效。

对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量net，它是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。 在下面的例子中，我们的模型只包含一个层，因此实际上不需要Sequential。 但是由于以后几乎所有的模型都是多层的，在这里使用Sequential会让你熟悉“标准的流水线”。

回顾单层网络架构， 这一单层被称为全连接层（fully-connected layer）， 因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。

### 3.3.4. 初始化模型参数
在使用net之前，我们需要初始化模型参数。 如在线性回归模型中的权重和偏置。 深度学习框架通常有预定义的方法来初始化参数。 在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样， 偏置参数将初始化为零。

### 3.3.5. 定义损失函数
计算均方误差使用的是MSELoss类，也称为平方范数。 默认情况下，它返回所有样本损失的平均值。

### 3.3.6. 定义优化算法
小批量随机梯度下降算法是一种优化神经网络的标准工具， PyTorch在optim模块中实现了该算法的许多变种。 当我们实例化一个SGD实例时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置lr值，这里设置为0.03。
### 3.3.7. 训练
通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。

回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（train_data）， 不停地从中获取一个小批量的输入和相应的标签。 对于每一个小批量，我们会进行以下步骤:

- 通过调用net(X)生成预测并计算损失l（正向传播）。
- 通过进行反向传播来计算梯度。
- 通过调用优化算法迭代模型参数。

为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。

下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。 要访问参数，我们首先从net访问所需的层，然后读取该层的权重和偏置。 正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近。

### 3.3.8. 小结

- 我们可以使用PyTorch的高级API更简洁地实现模型。

- 在PyTorch中，data模块提供了数据处理工具，nn模块定义了大量的神经网络层和常见损失函数。

- 我们可以通过_结尾的方法将参数替换，从而初始化参数。

## 3.4. softmax回归
在 3.1节中我们介绍了线性回归。 随后，在 3.2节中我们从头实现线性回归。 然后，在 3.3节中我们使用深度学习框架的高级API简洁实现线性回归。

回归可以用于预测多少的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。

事实上，我们也对分类问题感兴趣：不是问“多少”，而是问“哪一个”：

某个电子邮件是否属于垃圾邮件文件夹？

某个用户可能注册或不注册订阅服务？

某个图像描绘的是驴、狗、猫、还是鸡？

某人接下来最有可能看哪部电影？

通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。

### 3.4.1. 分类问题
~
### 3.4.2. 网络架构
~
### 3.4.3. 全连接层的参数开销
~
### 3.4.4. softmax运算
社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上 发明的softmax函数正是这样做的： softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。

$$\hat{y}_1, \ldots, \hat{y}_n = \exp(o_1), \ldots, \exp(o_n)$$

$$\hat{y}_1, \ldots, \hat{y}_n = \frac{\exp(o_1)}{\sum_{i=1}^n \exp(o_i)}, \ldots, \frac{\exp(o_n)}{\sum_{i=1}^n \exp(o_i)}$$

尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。


### 3.4.5. 小批量样本的矢量化
先仿射变换，后softmax运算。 为了提高计算效率，我们通常会对小批量数据执行矢量计算。 假设我们的输入样本数为$|\mathcal{B}|$，输入特征的维度为$n$，输出个数为$q。 仿射变换的输出为一个$|\mathcal{B}| \times q$矩阵，其第$i$行是样本$i$的输出。 因此，我们定义矩阵$\mathbf{X}$为一个$|\mathcal{B}| \times n$的小批量样本特征。 我们得到矩阵$\mathbf{O}$，它是$\mathbf{X}$和$\mathbf{W}$的矩阵-矩阵乘法后再加上偏置$\mathbf{b}$。 其中$\mathbf{W}$是$n \times q$的权重矩阵，$\mathbf{b}$是$q$维的偏置。 对于矩阵$\mathbf{O}$中的任意行$i$，它对应了样本$i$的输出。 因此，我们可以通过对矩阵$\mathbf{O}$中的所有元素做softmax运算来得到输出。  $o_{ij}$表示样本$i$中输出$j$的未归一化预测，$\hat{y}_{ij}$表示样本$i$中输出$j$的预测概率。

### 3.4.6. 损失函数
接下来，我们需要一个损失函数来度量预测的效果。 我们将使用最大似然估计，这与在线性回归 （ 3.1.3节） 中的方法相同。

#### 3.4.6.1 对数似然
~
#### 3.4.6.2 softmax及其导数
~
#### 3.4.6.3 交叉熵损失
~
### 3.4.7. 信息论基础
信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。

#### 3.4.7.1 熵
信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布$P$的熵（entropy）。 可以通过以下方程得到：

$$H[P] = \sum_j - p(j) \log p(j)$$

信息论的基本定理之一指出，为了对从分布$p$中抽取的数据进行编码，我们至少需要$H[P]$“纳特”对其进行编码。 “纳特”相当于比特(bit)，但是对数底为e而不是2.因此，一个纳特是$\frac{1}{\log 2}$比特。

#### 3.4.7.2 信息量

压缩与预测有什么关系呢？ 想象一下，我们有一个要压缩的数据流。 如果我们很容易预测下一个数据，那么这个数据就很容易压缩。 为什么呢？ 举一个极端的例子，假如数据流中的每个数据完全相同，这会是一个非常无聊的数据流。 由于它们总是相同的，我们总是知道下一个数据是什么。 所以，为了传递数据流的内容，我们不必传输任何信息。也就是说，“下一个数据是xx”这个事件毫无信息量。

但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到“惊异”。 克劳德·香农决定用信息量$\log \frac{1}{p(j)} = -\log p(j)$来量化这种惊异。 在观察一个事件$j$时，并赋予它（主观）概率$P(j)$。 当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。 在 (3.4.11)中定义的熵， 是当分配的概率真正匹配数据生成过程时的信息量的期望。

#### 3.4.7.3 重新审视交叉熵
如果把熵$H(p)$想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？交叉熵从$P$到$Q$,记为$H(P, Q)$,我们可以把交叉熵想象为“主观概率为$Q$的观察者在看到根据概率$P$产生的事件时的惊异程度”。 当$P=Q$时，交叉熵达到最低。在这种情况下，从$P$到$Q$的交叉熵是$H(P, P) = H(P)$。 

简而言之，我们可以从两方面来考虑交叉熵分类目标： 
- （i）最大化观测数据的似然；
- （ii）最小化传达标签所需的惊异。

### 3.4.8. 模型预测和评估
在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。 通常我们使用预测概率最高的类别作为输出类别。 如果预测与实际类别（标签）一致，则预测是正确的。 在接下来的实验中，我们将使用精度（accuracy）来评估模型的性能。 精度等于正确预测数与预测总数之间的比率。

### 3.4.9. 小结
- softmax运算获取一个向量并将其映射为概率。

- softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。

- 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。


## 3.5. 图像分类数据集
MNIST数据集 (LeCun et al., 1998) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (Xiao et al., 2017)。

### 3.5.1. 读取数据集
我们可以通过框架中的内置函数将Fashion-MNIST数据集下载并读取到内存中。
### 3.5.2. 读取小批量
为了使我们在读取训练集和测试集时更容易，我们使用内置的数据迭代器，而不是从零开始创建。 回顾一下，在每次迭代中，数据加载器每次都会读取一小批量数据，大小为batch_size。 通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取小批量。
### 3.5.3. 整合所有组件
现在我们定义load_data_fashion_mnist函数，用于获取和读取Fashion-MNIST数据集。 这个函数返回训练集和验证集的数据迭代器。 此外，这个函数还接受一个可选参数resize，用来将图像大小调整为另一种形状。
### 3.5.4. 小结
- Fashion-MNIST是一个服装分类数据集，由10个类别的图像组成。我们将在后续章节中使用此数据集来评估各种分类算法。
- 我们将高度h和宽度w像素的图像的形状记为h×w或（h，w）。
- 数据迭代器是深度学习框架中一个重要的组件。数据迭代器提供了随机读取小批量数据样本的便利方式。

## 3.6. softmax回归的从零开始实现
就像我们从零开始实现线性回归一样， 我们认为softmax回归也是重要的基础，因此应该知道实现softmax回归的细节。 本节我们将使用刚刚在 3.5节中引入的Fashion-MNIST数据集， 并设置数据迭代器的批量大小为256。

### 3.6.1. 初始化模型参数
~
### 3.6.2. 定义softmax操作
~
### 3.6.3. 定义模型
~
### 3.6.4. 定义损失函数
~
### 3.6.5. 分类精度
~
### 3.6.6. 训练
~
### 3.6.7. 预测
~
### 3.6.8. 小结
- 借助softmax回归，我们可以训练多分类的模型。
- 训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。

## 3.7. softmax回归的简洁实现
在 3.3节中， 我们发现通过深度学习框架的高级API能够使实现

线性回归变得更加容易。 同样，通过深度学习框架的高级API也能更方便地实现softmax回归模型。 本节如在 3.6节中一样， 继续使用Fashion-MNIST数据集，并保持批量大小为256。
### 3.7.1. 初始化模型参数
~
### 3.7.2. 重新审视Softmax的实现
~
### 3.7.3. 优化算法
~
### 3.7.4. 训练
~
### 3.7.5. 小结
- 使用深度学习框架的高级API，我们可以更简洁地实现softmax回归。
- 从计算的角度来看，实现softmax回归比较复杂。在许多情况下，深度学习框架在这些著名的技巧之外采取了额外的预防措施，来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇到的陷阱。

