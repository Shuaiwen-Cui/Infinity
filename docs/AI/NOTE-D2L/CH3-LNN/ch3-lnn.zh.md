# CH03 - 线性神经网络
在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。 本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。

## 3.1. 线性回归
**回归**（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。

在机器学习领域中的大多数任务通常都与**预测**（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的预测都是回归问题。 在后面的章节中，我们将介绍分类问题。**分类**问题的目标是预测数据属于一组类别中的哪一个。

### 3.1.1. 线性回归的基本元素

线性回归（linear regression）可以追溯到19世纪初， 它在回归的各种标准工具中最简单而且最流行。 线性回归基于几个简单的假设： 首先，假设自变量$x$和因变量$y$之间的关系是线性的， 即$y$可以表示为$x$中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

为了解释线性回归，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为训练数据集（training data set） 或训练集（training set）。 每行数据（比如一次房屋交易相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。 预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。

#### 3.1.1.1. 线性模型

线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和,权重决定了每个特征对我们预测值的影响. 带权重的线性回归是输入特征的一个 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）。

给定一个数据集，我们的目标是寻找模型的权重$w$和偏置$b$， 使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过线性模型的仿射变换决定，仿射变换由所选权重和偏置确定。

而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含$d$个特征时，我们将预测结果$\hat{y}$表示为：

$$\hat{y} = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b$$

用向量表示，可以更简洁地表达为：

$$\hat{y} = \mathbf{w}^\top \mathbf{x} + b$$

其中，$\mathbf{w}$是权重向量，$\mathbf{x}$是输入向量，$b$是偏置。

在开始寻找最好的模型参数（model parameters）$w$和$bb$之前， 我们还需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。

#### 3.1.1.2 损失函数

在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。 损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。

当样本$i$的预测值为$\hat{y}^{(i)}$，真实标签为$y^{(i)}$时，平方误差可以表示为：

$$\mathcal{L}(\mathbf{w},b) = \frac{1}{n} \sum_{i=1}^n \left(\hat{y}^{(i)} - y^{(i)}\right)^2$$

常数$\frac{1}{2}$使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。 但这样不会带来本质的差别。

由于平方误差函数中的二次方项， 估计值$\hat{y}^{(i)}$和观测值$y^{(i)}$之间较大的差异将导致更大的损失。 为了度量模型在整个数据集上的质量，我们需计算在训练集个样本上的损失均值（也等价于求和）。

$$ \mathcal{L}(\mathbf{w},b) = \frac{1}{n} \sum_{i=1}^n \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2 $$

在训练时，我们希望寻找一组模型参数，记为$\mathbf{w}^*,b^*$，使得损失函数最小：

$$\mathcal{L}(\mathbf{w}^*,b^*) = \min_{\mathbf{w},b} \mathcal{L}(\mathbf{w},b)$$

#### 3.1.1.3 解析解
线性回归刚好是一个很简单的优化问题。 与我们将在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。 首先，我们将偏置$b$合并到参数$\mathbf{w}$中。 合并方法是在所有样本上添加一列，该列恒等于1。 因此，我们只需考虑一个权重向量$\mathbf{w}$，且数据样本为$\mathbf{x} \in \mathbb{R}^{d+1}$。 然后，我们可以通过对模型参数$\mathbf{w}$求导并令导数等于0来找到最小化损失函数的参数。 具体来说，我们需要求解以下线性方程组：

$$\mathbf{X}^\top \mathbf{X} \mathbf{w} = \mathbf{X}^\top \mathbf{y}$$

得到的解析解为：

$$\mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}$$

像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

#### 3.1.1.4 随机梯度下降
即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。

本书中我们用到一种名为**梯度下降**（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 **它通过不断地在损失函数递减的方向上更新参数来降低误差。**

梯度下降最简单的用法是**计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。** 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 **因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。**

在每次迭代中，我们首先随机抽样一个小批量$\mathcal{B}$，它包含了$\mathcal{B}$个数据样本。 我们用$\mathcal{L}(\mathbf{w},b;\mathcal{B})$来表示小批量$\mathcal{B}$上的损失， 并且用$\mathbf{w}$和$b$表示参数。 在小批量随机梯度下降中，模型参数在每次迭代时的更新量为：

$$\mathbf{w} \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{\mathbf{w}} \mathcal{L}^{(i)}(\mathbf{w},b)$$

$$b \leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{b} \mathcal{L}^{(i)}(\mathbf{w},b)$$

在上式中，$\eta$（取正数）叫作**学习率**（learning rate）。 由于$\mathcal{L}(\mathbf{w},b;\mathcal{B})$是关于单个样本的损失函数， 因此上式中的梯度也是基于单个样本损失函数的梯度的累加。 给定学习率$\eta$和小批量随机梯度下降所使用的批量大小， 模型参数将如何更新取决于损失曲线的形状。

我们用下面的数学公式来表示这一更新过程（$\partial$符号表示偏导数）：

$$\left( \mathbf{w}, b \right) \leftarrow \left( \mathbf{w}, b \right) - \eta \left( \frac{\partial \mathcal{L}}{\partial \mathbf{w}}, \frac{\partial \mathcal{L}}{\partial b} \right)$$

总结一下，算法的步骤如下：
（1）初始化模型参数的值，如随机初始化；
（2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。

${|\mathcal{B}|}$ 是批量大小，也是小批量随机梯度下降每次迭代的样本数， 通常取几十或几百。学习率$\eta$通常取0.01、0.1、1或者10。需要注意的是，**当批量大小为1时，小批量随机梯度下降退化为随机梯度下降。**

批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）。 调参（hyperparameter tuning）是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的。但是，即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。

线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为**泛化（generalization）。**

#### 3.1.1.5. 用模型进行预测

给定特征估计目标的过程通常称为**预测**（prediction）或**推断**（inference）。

### 3.1.2. 矢量化加速

在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

=== "PYTORCH"

    ```python
    %matplotlib inline
    import math
    import time
    import numpy as np
    import torch
    from d2l import torch as d2l
    ```
=== "TENSORFLOW"

    ```python
    %matplotlib inline
    import math
    import time
    import numpy as np
    import tensorflow as tf
    from d2l import tensorflow as d2l
    ```
### 3.1.3. 正态分布与平方损失

正态分布和线性回归之间的关系很密切。 正态分布（normal distribution），也称为高斯分布（Gaussian distribution）， 最早由德国数学家高斯（Gauss）应用于天文学研究。 简单的说，若随机变量$x$服从均值为$\mu$、方差为$\sigma^2$的正态分布，记为$x \sim \mathcal{N}(\mu,\sigma^2)$，其概率密度函数为：

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right)$$

正态分布的均值为$\mu$，方差为$\sigma^2$。 当$\mu = 0, \sigma = 1$时的正态分布称为标准正态分布（standard normal distribution）。 标准正态分布的概率密度函数简化为：

$$\phi(x) = \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{1}{2} x^2\right)$$

正态分布在机器学习中有着广泛的应用。 例如，我们将在后面的章节中看到，正态分布假设了噪声的分布，而噪声又影响了数据的标签。 事实上，我们将看到许多有用的算法都假设了噪声服从正态分布。

均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。噪声正态分布如下式:

$$ y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0,\sigma^2)$$

因此，我们现在可以写出通过给定的$x$观测到特定$y$的似然：

$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right)$$

现在，根据极大似然估计法，参数$\mathbf{w}$和$b$的最优值是使整个数据集似然最大的值。

根据极大似然估计法选择的估计量称为极大似然估计量。虽然使许多指数函数的乘积最大化看起来很困难， 但是我们可以在不改变目标的前提下，通过最大化似然对数来简化。由于历史原因，优化通常是说最小化而不是最大化。 我们可以改为最小化负对数似然$-\log P(y \mid \mathbf{x})$。由此可以得到的数学公式是：

$$-\log P(y \mid \mathbf{x}) = \frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2 + \frac{1}{2} \log(2 \pi \sigma^2)$$

现在我们只需要假设$\sigma$是一个常数，就可以忽略掉最后一项，因为这个常数不会影响$\mathbf{w}$和$b$的选择。现在，我们的最大似然估计问题转化为最小化均方误差。因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。

### 3.1.4. 从线性回归到深度网络

到目前为止，我们只谈论了线性模型。 尽管神经网络涵盖了更多更为丰富的模型，我们依然可以用描述神经网络的方式来描述线性模型， 从而把线性模型看作一个神经网络。 首先，我们用“层”符号来重写这个模型。

#### 3.1.4.1. 神经网络图
深度学习从业者喜欢绘制图表来可视化模型中正在发生的事情。对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换称为全连接层（fully-connected layer）或称为稠密层（dense layer）。 下一章将详细讨论由这些层组成的网络。

#### 3.1.4.2. 生物学

### 3.1.5. 小结
- 机器学习模型中的关键要素是训练数据、模型、损失函数，以及优化算法。

- 矢量化使数学表达上更简洁，同时运行的更快。

- 最小化目标函数和执行极大似然估计等价。

- 线性回归模型也是一个简单的神经网络。

## 3.2. 线性回归的从零开始实现
### 3.2.1. 生成数据集
### 3.2.2. 读取数据集
### 3.2.3. 初始化模型参数
### 3.2.4. 定义模型
### 3.2.5. 定义损失函数
### 3.2.6. 定义优化算法
### 3.2.7. 训练
### 3.2.8. 小结

## 3.3. 线性回归的简洁实现
### 3.3.1. 生成数据集
### 3.3.2. 读取数据集
### 3.3.3. 定义模型
### 3.3.4. 初始化模型参数
### 3.3.5. 定义损失函数
### 3.3.6. 定义优化算法
### 3.3.7. 训练
### 3.3.8. 小结

## 3.4. softmax回归
### 3.4.1. 分类问题
### 3.4.2. 网络架构
### 3.4.3. 全连接层的参数开销
### 3.4.4. softmax运算
### 3.4.5. 小批量样本的矢量化
### 3.4.6. 损失函数
### 3.4.7. 信息论基础
### 3.4.8. 模型预测和评估
### 3.4.9. 小结

## 3.5. 图像分类数据集
### 3.5.1. 读取数据集
### 3.5.2. 读取小批量
### 3.5.3. 整合所有组件
### 3.5.4. 小结

## 3.6. softmax回归的从零开始实现
### 3.6.1. 初始化模型参数
### 3.6.2. 定义softmax操作
### 3.6.3. 定义模型
### 3.6.4. 定义损失函数
### 3.6.5. 分类精度
### 3.6.6. 训练
### 3.6.7. 预测
### 3.6.8. 小结

## 3.7. softmax回归的简洁实现
### 3.7.1. 初始化模型参数
### 3.7.2. 重新审视Softmax的实现
### 3.7.3. 优化算法
### 3.7.4. 训练
### 3.7.5. 小结

