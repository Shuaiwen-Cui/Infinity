<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to Shaun's rabbit hole. This site serves as a personal knowledge base for me to record my thoughts and ideas. It is also a place for me to share my knowledge and experience with the world. I hope you find something useful here."><meta name=author content="Shuaiwen Cui"><link href=https://localhost:8000/AI/NOTE-D2L/CH2-PRE/ch2-pre/ rel=canonical><link href=../../CH1-INTRO/ch1-intro/ rel=prev><link href=../../CH3-LNN/ch3-lnn/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../../feed_rss_updated.xml><link rel=icon href=../../../../static/images/logo.png><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.2"><title>CH2-PRE - Eureka!</title><link rel=stylesheet href=../../../../assets/stylesheets/main.50c56a3b.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.06af60db.min.css><style>.md-tag--default-tag{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M0 80v149.5c0 17 6.7 33.3 18.7 45.3l176 176c25 25 65.5 25 90.5 0l133.5-133.5c25-25 25-65.5 0-90.5l-176-176c-12-12-28.3-18.7-45.3-18.7H48C21.5 32 0 53.5 0 80zm112 32a32 32 0 1 1 0 64 32 32 0 1 1 0-64z"/></svg>');}.md-tag--hardware-tag{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M176 24c0-13.3-10.7-24-24-24s-24 10.7-24 24v40c-35.3 0-64 28.7-64 64H24c-13.3 0-24 10.7-24 24s10.7 24 24 24h40v56H24c-13.3 0-24 10.7-24 24s10.7 24 24 24h40v56H24c-13.3 0-24 10.7-24 24s10.7 24 24 24h40c0 35.3 28.7 64 64 64v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40h56v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40h56v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40c35.3 0 64-28.7 64-64h40c13.3 0 24-10.7 24-24s-10.7-24-24-24h-40v-56h40c13.3 0 24-10.7 24-24s-10.7-24-24-24h-40v-56h40c13.3 0 24-10.7 24-24s-10.7-24-24-24h-40c0-35.3-28.7-64-64-64V24c0-13.3-10.7-24-24-24s-24 10.7-24 24v40h-56V24c0-13.3-10.7-24-24-24s-24 10.7-24 24v40h-56V24zm-16 104h192c17.7 0 32 14.3 32 32v192c0 17.7-14.3 32-32 32H160c-17.7 0-32-14.3-32-32V160c0-17.7 14.3-32 32-32zm192 32H160v192h192V160z"/></svg>');}.md-tag--software-tag{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M64 96c0-35.3 28.7-64 64-64h384c35.3 0 64 28.7 64 64v256h-64V96H128v256H64V96zM0 403.2C0 392.6 8.6 384 19.2 384h601.6c10.6 0 19.2 8.6 19.2 19.2 0 42.4-34.4 76.8-76.8 76.8H76.8C34.4 480 0 445.6 0 403.2zM281 209l-31 31 31 31c9.4 9.4 9.4 24.6 0 33.9s-24.6 9.4-33.9 0l-48-48c-9.4-9.4-9.4-24.6 0-33.9l48-48c9.4-9.4 24.6-9.4 33.9 0s9.4 24.6 0 33.9zm112-34 48 48c9.4 9.4 9.4 24.6 0 33.9l-48 48c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l31-31-31-31c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0z"/></svg>');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config",""),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../../../../assets/stylesheets/custom.00c04c01.min.css></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#ch02-preliminaries class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> For updates follow <strong>@ShuaiwenCui</strong> on <a href=https://www.linkedin.com/in/shaun-shuaiwen-cui/ rel=me> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </span> <strong>Linkedin</strong> </a> and <a href=https://twitter.com/ShuaiwenC> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title=Eureka! class="md-header__button md-logo" aria-label=Eureka! data-md-component=logo> <img src=../../../../static/images/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Eureka! </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> CH2-PRE </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8v2m9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1 0 1.71-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5Z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <div class=md-header__option> <div class=md-select> <button class="md-header__button md-icon" aria-label="Select language"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.52 17.52 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04M18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12m-2.62 7 1.62-4.33L19.12 17h-3.24Z"/></svg> </button> <div class=md-select__inner> <ul class=md-select__list> <li class=md-select__item> <a href=./ hreflang=en class=md-select__link> English </a> </li> <li class=md-select__item> <a href=../../../../zh/AI/NOTE-D2L/CH2-PRE/ch2-pre/ hreflang=zh class=md-select__link> ÁÆÄ‰Ωì‰∏≠Êñá </a> </li> </ul> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 320 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/Shuaiwen-Cui/Infinity.git/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> Shuaiwen-Cui/Infinity </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../.. class=md-tabs__link> HOME </a> </li> <li class=md-tabs__item> <a href=../../../../MATH/math/ class=md-tabs__link> MATH </a> </li> <li class=md-tabs__item> <a href=../../../../CS/CS/ class=md-tabs__link> CS </a> </li> <li class=md-tabs__item> <a href=../../../../CODING/coding/ class=md-tabs__link> CODING </a> </li> <li class=md-tabs__item> <a href=../../../../EMBEDDED-SYS/embedded-sys/ class=md-tabs__link> EMBEDDED-SYS </a> </li> <li class=md-tabs__item> <a href=../../../../DSP/dsp/ class=md-tabs__link> DSP </a> </li> <li class=md-tabs__item> <a href=../../../../PERCEPTION/perception/ class=md-tabs__link> PERCEPTION </a> </li> <li class=md-tabs__item> <a href=../../../../CONTROL/control/ class=md-tabs__link> CONTROL </a> </li> <li class=md-tabs__item> <a href=../../../../ACTUATION/actuation/ class=md-tabs__link> ACTUATION </a> </li> <li class=md-tabs__item> <a href=../../../../IOT/iot/ class=md-tabs__link> IOT </a> </li> <li class=md-tabs__item> <a href=../../../../CLOUD/cloud/ class=md-tabs__link> CLOUD </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../BASICS/basics/ class=md-tabs__link> AI </a> </li> <li class=md-tabs__item> <a href=../../../../SHM/shm/ class=md-tabs__link> SHM </a> </li> <li class=md-tabs__item> <a href=../../../../DEV/DEVENV/LATEX/latex/ class=md-tabs__link> DEV </a> </li> <li class=md-tabs__item> <a href=../../../../RESEARCH/research/ class=md-tabs__link> RESEARCH </a> </li> <li class=md-tabs__item> <a href=../../../../PROJECT/project/ class=md-tabs__link> PROJECT </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title=Eureka! class="md-nav__button md-logo" aria-label=Eureka! data-md-component=logo> <img src=../../../../static/images/logo.png alt=logo> </a> Eureka! </label> <div class=md-nav__source> <a href=https://github.com/Shuaiwen-Cui/Infinity.git/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> Shuaiwen-Cui/Infinity </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../../../.. class="md-nav__link "> <span class=md-ellipsis> HOME </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> HOME </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../HOME/about/ class=md-nav__link> <span class=md-ellipsis> ABOUT </span> </a> </li> <li class=md-nav__item> <a href=../../../../HOME/sponsorship/ class=md-nav__link> <span class=md-ellipsis> SPONSORSHIP </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> MATH </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> MATH </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../MATH/math/ class=md-nav__link> <span class=md-ellipsis> Math </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> GEOMETRY-GRAPHICS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> GEOMETRY-GRAPHICS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../MATH/GEOMETRY-GRAPHICS/COMPUTER-GRAPHICS/computer-graphics/ class=md-nav__link> <span class=md-ellipsis> COMPUTER-GRAPHICS-FROM-SCRATCH </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../MATH/OPTIMIZATION/optimization/ class=md-nav__link> <span class=md-ellipsis> OPTIMIZATION üéØ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> CS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> CS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CS/CS/ class=md-nav__link> <span class=md-ellipsis> CS </span> </a> </li> <li class=md-nav__item> <a href=../../../../CS/OS/os/ class=md-nav__link> <span class=md-ellipsis> OS </span> </a> </li> <li class=md-nav__item> <a href=../../../../CS/DATA-BASE/data-base/ class=md-nav__link> <span class=md-ellipsis> DATA-BASE üéØ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> CODING </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> CODING </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CODING/coding/ class=md-nav__link> <span class=md-ellipsis> Coding </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/CODING-ENV/coding-env/ class=md-nav__link> <span class=md-ellipsis> CODING-ENV </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex> <span class=md-ellipsis> C/C++ üéØ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> C/C++ üéØ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C/c/ class=md-nav__link> <span class=md-ellipsis> C </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C-NOTES/c-notes/ class=md-nav__link> <span class=md-ellipsis> C-NOTES </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C%2B%2B/c%2B%2B/ class=md-nav__link> <span class=md-ellipsis> C++ </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_4> <label class=md-nav__link for=__nav_4_3_4 id=__nav_4_3_4_label tabindex=0> <span class=md-ellipsis> C++-NOTES </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_4> <span class="md-nav__icon md-icon"></span> C++-NOTES </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C%2B%2B-NOTES/c%2B%2B-notes/ class=md-nav__link> <span class=md-ellipsis> C++ Language Study Notes </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C%2B%2B-NOTES/CH01/ch01/ class=md-nav__link> <span class=md-ellipsis> CH01 </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C%2B%2B-NOTES/CH02/ch02/ class=md-nav__link> <span class=md-ellipsis> CH02 </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C%2B%2B-NOTES/CH03/ch03/ class=md-nav__link> <span class=md-ellipsis> CH03 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/MAKE-SERIES/make-series/ class=md-nav__link> <span class=md-ellipsis> MAKE-SERIES </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex> <span class=md-ellipsis> PYTHON </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> PYTHON </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CODING/PYTHON/python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/PYTHON/ROADMAP/roadmap/ class=md-nav__link> <span class=md-ellipsis> ROADMAP </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_5> <label class=md-nav__link for=__nav_4_5 id=__nav_4_5_label tabindex> <span class=md-ellipsis> HTML/CSS/JS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> HTML/CSS/JS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CODING/HTML-CSS-JS/HTML/html/ class=md-nav__link> <span class=md-ellipsis> HTML </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/HTML-CSS-JS/CSS/css/ class=md-nav__link> <span class=md-ellipsis> CSS </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/HTML-CSS-JS/JS/js/ class=md-nav__link> <span class=md-ellipsis> JAVASCRIPT </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> EMBEDDED-SYS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> EMBEDDED-SYS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/embedded-sys/ class=md-nav__link> <span class=md-ellipsis> Embedded Systems </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex> <span class=md-ellipsis> THEORY </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> THEORY </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/THEORY/ELECTRONICS/electronics/ class=md-nav__link> <span class=md-ellipsis> ELECTRONICS </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/THEORY/CIRCUITS/circuits/ class=md-nav__link> <span class=md-ellipsis> CIRCUITS </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/THEORY/FREERTOS/freertos/ class=md-nav__link> <span class=md-ellipsis> FREERTOS </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/THEORY/LINUX/linux/ class=md-nav__link> <span class=md-ellipsis> LINUX </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex> <span class=md-ellipsis> BASICS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> BASICS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/BASICS/embed-basics/ class=md-nav__link> <span class=md-ellipsis> EMBED-BASICS </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex> <span class=md-ellipsis> SIMULATION </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> SIMULATION </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/SIMULATION/RENODE/renode/ class=md-nav__link> <span class=md-ellipsis> RENODE </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/SIMULATION/WOKWI/wokwi/ class=md-nav__link> <span class=md-ellipsis> WOKWI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex> <span class=md-ellipsis> DEVKIT üéØ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> DEVKIT üéØ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/ARDUINO/arduino/ class=md-nav__link> <span class=md-ellipsis> ARDUINO </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5_2> <label class=md-nav__link for=__nav_5_5_2 id=__nav_5_5_2_label tabindex=0> <span class=md-ellipsis> STM32 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5_2> <span class="md-nav__icon md-icon"></span> STM32 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32/stm32/ class=md-nav__link> <span class=md-ellipsis> STM32 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32/STM32H743IIT/stm32h743iit/ class=md-nav__link> <span class=md-ellipsis> STM32H743IIT </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32/STM32MP157/stm32mp157/ class=md-nav__link> <span class=md-ellipsis> STM32MP157 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5_3> <label class=md-nav__link for=__nav_5_5_3 id=__nav_5_5_3_label tabindex=0> <span class=md-ellipsis> STM32-NOTES </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5_3> <span class="md-nav__icon md-icon"></span> STM32-NOTES </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5_3_1> <label class=md-nav__link for=__nav_5_5_3_1 id=__nav_5_5_3_1_label tabindex=0> <span class=md-ellipsis> MCU-STM32H743IIT </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_5_5_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5_3_1> <span class="md-nav__icon md-icon"></span> MCU-STM32H743IIT </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MCU-STM32H743IIT/mcu-stm32h743iit/ class=md-nav__link> <span class=md-ellipsis> MCU-STM32H743IIT </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MCU-STM32H743IIT/CH01/mcu-ch01/ class=md-nav__link> <span class=md-ellipsis> CH01 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MCU-STM32H743IIT/CH02/mcu-ch02/ class=md-nav__link> <span class=md-ellipsis> CH02 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MCU-STM32H743IIT/CH03/mcu-ch03/ class=md-nav__link> <span class=md-ellipsis> CH03 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MCU-STM32H743IIT/CH04/mcu-ch04/ class=md-nav__link> <span class=md-ellipsis> CH04 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MCU-STM32H743IIT/CH05/mcu-ch05/ class=md-nav__link> <span class=md-ellipsis> CH05 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MCU-STM32H743IIT/CH06/mcu-ch06/ class=md-nav__link> <span class=md-ellipsis> CH06 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MCU-STM32H743IIT/CH07/mcu-ch07/ class=md-nav__link> <span class=md-ellipsis> CH07 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MCU-STM32H743IIT/CH08/mcu-ch08/ class=md-nav__link> <span class=md-ellipsis> CH08 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32-NOTES/MPU-STM32MP157/mpu-stm32mp157/ class=md-nav__link> <span class=md-ellipsis> MPU-STM32MP157 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/ESP32/esp32/ class=md-nav__link> <span class=md-ellipsis> ESP32 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/RASPBERRYPI/raspberrypi/ class=md-nav__link> <span class=md-ellipsis> RASPBERRYPI </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/JETSON/jetson/ class=md-nav__link> <span class=md-ellipsis> JETSON </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_6> <label class=md-nav__link for=__nav_5_6 id=__nav_5_6_label tabindex> <span class=md-ellipsis> IDE üéØ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6> <span class="md-nav__icon md-icon"></span> IDE üéØ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/ide/ class=md-nav__link> <span class=md-ellipsis> IDE for Embedded Systems </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_6_2> <label class=md-nav__link for=__nav_5_6_2 id=__nav_5_6_2_label tabindex=0> <span class=md-ellipsis> KEIL </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6_2> <span class="md-nav__icon md-icon"></span> KEIL </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/keil/ class=md-nav__link> <span class=md-ellipsis> Keil IDE </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_6_2_2> <label class=md-nav__link for=__nav_5_6_2_2 id=__nav_5_6_2_2_label tabindex=0> <span class=md-ellipsis> OFFICIAL-GUIDE-MDK4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_5_6_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6_2_2> <span class="md-nav__icon md-icon"></span> OFFICIAL-GUIDE-MDK4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE-MDK4/guide-mdk4/ class=md-nav__link> <span class=md-ellipsis> KEIL MDK4 Guide üéØ‚úÖüèÜ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_6_2_3> <label class=md-nav__link for=__nav_5_6_2_3 id=__nav_5_6_2_3_label tabindex=0> <span class=md-ellipsis> OFFICIAL-GUIDE-MDK5 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_5_6_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6_2_3> <span class="md-nav__icon md-icon"></span> OFFICIAL-GUIDE-MDK5 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE-MDK5/PREFACE/preface/ class=md-nav__link> <span class=md-ellipsis> PREFACE </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE-MDK5/MDK-INTRODUCTION/mdk-introduction/ class=md-nav__link> <span class=md-ellipsis> MDK-INTRODUCTION </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE-MDK5/CMSIS/cmsis/ class=md-nav__link> <span class=md-ellipsis> CMSIS </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE-MDK5/SOFTWARE-COMPONENTS/software-components/ class=md-nav__link> <span class=md-ellipsis> SOFTWARE-COMPONENTS </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE-MDK5/CREATE-APP/create-app/ class=md-nav__link> <span class=md-ellipsis> CREATE-APP </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE-MDK5/DEBUG-APP/debug-app/ class=md-nav__link> <span class=md-ellipsis> DEBUG-APP </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE-MDK5/MDK-MIDDLEWARE/mdk-middleware/ class=md-nav__link> <span class=md-ellipsis> MDK-MIDDLEWARE </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/STM32CUBE/stm32cube/ class=md-nav__link> <span class=md-ellipsis> STM32CUBE </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/PLATFORMIO/platformio/ class=md-nav__link> <span class=md-ellipsis> PLATFORMIO </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_7> <label class=md-nav__link for=__nav_5_7 id=__nav_5_7_label tabindex> <span class=md-ellipsis> DEV-FRAMEWORK </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_7_label aria-expanded=false> <label class=md-nav__title for=__nav_5_7> <span class="md-nav__icon md-icon"></span> DEV-FRAMEWORK </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEV-FRAMEWORK/CMSIS/cmsis/ class=md-nav__link> <span class=md-ellipsis> CMSIS üéØ </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEV-FRAMEWORK/ESP-IDF/esp-idf/ class=md-nav__link> <span class=md-ellipsis> ESP-IDF </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/USEFUL-LIB/useful-lib/ class=md-nav__link> <span class=md-ellipsis> USEFUL-LIB üéØ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex> <span class=md-ellipsis> DSP </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> DSP </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DSP/dsp/ class=md-nav__link> <span class=md-ellipsis> Digital Signal Processing </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2> <label class=md-nav__link for=__nav_6_2 id=__nav_6_2_label tabindex> <span class=md-ellipsis> NOTE-DSP-STUFF üéØ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2> <span class="md-nav__icon md-icon"></span> NOTE-DSP-STUFF üéØ </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2_1> <label class=md-nav__link for=__nav_6_2_1 id=__nav_6_2_1_label tabindex=0> <span class=md-ellipsis> TIME-FREQ-ANA </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_6_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2_1> <span class="md-nav__icon md-icon"></span> TIME-FREQ-ANA </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH01/tf-ana-ch01/ class=md-nav__link> <span class=md-ellipsis> CH01 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH02/tf-ana-ch02/ class=md-nav__link> <span class=md-ellipsis> CH02 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH03/tf-ana-ch03/ class=md-nav__link> <span class=md-ellipsis> CH03 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH04/tf-ana-ch04/ class=md-nav__link> <span class=md-ellipsis> CH04 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH05/tf-ana-ch05/ class=md-nav__link> <span class=md-ellipsis> CH05 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH06/tf-ana-ch06/ class=md-nav__link> <span class=md-ellipsis> CH06 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH07/tf-ana-ch07/ class=md-nav__link> <span class=md-ellipsis> CH07 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH08/tf-ana-ch08/ class=md-nav__link> <span class=md-ellipsis> CH08 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH09/tf-ana-ch09/ class=md-nav__link> <span class=md-ellipsis> CH09 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH10/tf-ana-ch10/ class=md-nav__link> <span class=md-ellipsis> CH10 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex> <span class=md-ellipsis> PERCEPTION </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> PERCEPTION </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../PERCEPTION/perception/ class=md-nav__link> <span class=md-ellipsis> Perception </span> </a> </li> <li class=md-nav__item> <a href=../../../../PERCEPTION/SENSORS-CIVIL-ENGINEERING/sensors-ce/ class=md-nav__link> <span class=md-ellipsis> SENSORS-CIVIL-ENGINEERING </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_8> <label class=md-nav__link for=__nav_8 id=__nav_8_label tabindex> <span class=md-ellipsis> CONTROL </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_8_label aria-expanded=false> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> CONTROL </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CONTROL/control/ class=md-nav__link> <span class=md-ellipsis> Control </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9> <label class=md-nav__link for=__nav_9 id=__nav_9_label tabindex> <span class=md-ellipsis> ACTUATION </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_9_label aria-expanded=false> <label class=md-nav__title for=__nav_9> <span class="md-nav__icon md-icon"></span> ACTUATION </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/actuation/ class=md-nav__link> <span class=md-ellipsis> Actuation </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_2> <label class=md-nav__link for=__nav_9_2 id=__nav_9_2_label tabindex> <span class=md-ellipsis> ROS üéØ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_9_2_label aria-expanded=false> <label class=md-nav__title for=__nav_9_2> <span class="md-nav__icon md-icon"></span> ROS üéØ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/ROS/ros/ class=md-nav__link> <span class=md-ellipsis> Robotic Operating System (ROS) </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_2_2> <label class=md-nav__link for=__nav_9_2_2 id=__nav_9_2_2_label tabindex=0> <span class=md-ellipsis> OFFICIAL-GUIDE </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_9_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_9_2_2> <span class="md-nav__icon md-icon"></span> OFFICIAL-GUIDE </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/ROS/OFFICIAL-GUIDE/INSTALLATION/installation/ class=md-nav__link> <span class=md-ellipsis> INSTALLATION </span> </a> </li> <li class=md-nav__item> <a href=../../../../ACTUATION/ROS/OFFICIAL-GUIDE/DISTRIBUTION/distribution/ class=md-nav__link> <span class=md-ellipsis> DISTRIBUTION </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_2_2_3> <label class=md-nav__link for=__nav_9_2_2_3 id=__nav_9_2_2_3_label tabindex=0> <span class=md-ellipsis> TUTORIALS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_9_2_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_9_2_2_3> <span class="md-nav__icon md-icon"></span> TUTORIALS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/ROS/OFFICIAL-GUIDE/TUTORIALS/CLI-TOOLS/cli-tools/ class=md-nav__link> <span class=md-ellipsis> CLI-TOOLS </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_3> <label class=md-nav__link for=__nav_9_3 id=__nav_9_3_label tabindex> <span class=md-ellipsis> HANDSON-ROS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_9_3_label aria-expanded=false> <label class=md-nav__title for=__nav_9_3> <span class="md-nav__icon md-icon"></span> HANDSON-ROS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/HANDSON-ROS/handson-ros/ class=md-nav__link> <span class=md-ellipsis> HANDSON-ROS </span> </a> </li> <li class=md-nav__item> <a href=../../../../ACTUATION/HANDSON-ROS/P1-GET-STARTED/p1-get-started/ class=md-nav__link> <span class=md-ellipsis> P1-GET-STARTED </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_10> <label class=md-nav__link for=__nav_10 id=__nav_10_label tabindex> <span class=md-ellipsis> IOT </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_10_label aria-expanded=false> <label class=md-nav__title for=__nav_10> <span class="md-nav__icon md-icon"></span> IOT </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../IOT/iot/ class=md-nav__link> <span class=md-ellipsis> Internet of Things </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_10_2> <label class=md-nav__link for=__nav_10_2 id=__nav_10_2_label tabindex> <span class=md-ellipsis> WSN üéØ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_10_2_label aria-expanded=false> <label class=md-nav__title for=__nav_10_2> <span class="md-nav__icon md-icon"></span> WSN üéØ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../IOT/WSN/wsn/ class=md-nav__link> <span class=md-ellipsis> Wireless Sensor Network (WSN) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_10_3> <label class=md-nav__link for=__nav_10_3 id=__nav_10_3_label tabindex> <span class=md-ellipsis> MQTT </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_10_3_label aria-expanded=false> <label class=md-nav__title for=__nav_10_3> <span class="md-nav__icon md-icon"></span> MQTT </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../IOT/MQTT/mqtt/ class=md-nav__link> <span class=md-ellipsis> Message Queuing Telemetry Transport (MQTT) </span> </a> </li> <li class=md-nav__item> <a href=../../../../IOT/MQTT/BROKER/broker/ class=md-nav__link> <span class=md-ellipsis> BROKER </span> </a> </li> <li class=md-nav__item> <a href=../../../../IOT/MQTT/CLIENT/client/ class=md-nav__link> <span class=md-ellipsis> CLIENT </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_11> <label class=md-nav__link for=__nav_11 id=__nav_11_label tabindex> <span class=md-ellipsis> CLOUD </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_11_label aria-expanded=false> <label class=md-nav__title for=__nav_11> <span class="md-nav__icon md-icon"></span> CLOUD </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CLOUD/cloud/ class=md-nav__link> <span class=md-ellipsis> Cloud Techs </span> </a> </li> <li class=md-nav__item> <a href=../../../../CLOUD/CLOUD-TECH/cloud-tech/ class=md-nav__link> <span class=md-ellipsis> CLOUD-TECH üéØ </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_11_3> <label class=md-nav__link for=__nav_11_3 id=__nav_11_3_label tabindex> <span class=md-ellipsis> HANDS-ON üéØ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_11_3_label aria-expanded=false> <label class=md-nav__title for=__nav_11_3> <span class="md-nav__icon md-icon"></span> HANDS-ON üéØ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CLOUD/HANDS-ON/001-HAVE-A-SERVER/have-a-server/ class=md-nav__link> <span class=md-ellipsis> Have A Server </span> </a> </li> <li class=md-nav__item> <a href=../../../../CLOUD/HANDS-ON/002-SERVER-CONFIG/server-config/ class=md-nav__link> <span class=md-ellipsis> Server Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../../../CLOUD/HANDS-ON/003-DOMAIN-NAME/domain-name/ class=md-nav__link> <span class=md-ellipsis> Get A Domain Name </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_12 checked> <label class=md-nav__link for=__nav_12 id=__nav_12_label tabindex> <span class=md-ellipsis> AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_12_label aria-expanded=true> <label class=md-nav__title for=__nav_12> <span class="md-nav__icon md-icon"></span> AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../BASICS/basics/ class=md-nav__link> <span class=md-ellipsis> BASICS üéØ </span> </a> </li> <li class=md-nav__item> <a href=../../../FRAMEWORKS/frameworks/ class=md-nav__link> <span class=md-ellipsis> FRAMEWORKS üéØ </span> </a> </li> <li class=md-nav__item> <a href=../../../MLP/mlp/ class=md-nav__link> <span class=md-ellipsis> MLP </span> </a> </li> <li class=md-nav__item> <a href=../../../CNN/cnn/ class=md-nav__link> <span class=md-ellipsis> CNN </span> </a> </li> <li class=md-nav__item> <a href=../../../RNN/rnn/ class=md-nav__link> <span class=md-ellipsis> RNN </span> </a> </li> <li class=md-nav__item> <a href=../../../TRANSFORMER/transformer/ class=md-nav__link> <span class=md-ellipsis> TRANSFORMER </span> </a> </li> <li class=md-nav__item> <a href=../../../GAN/gan/ class=md-nav__link> <span class=md-ellipsis> GAN </span> </a> </li> <li class=md-nav__item> <a href=../../../RL/rl/ class=md-nav__link> <span class=md-ellipsis> RL üéØ </span> </a> </li> <li class=md-nav__item> <a href=../../../FL/fl/ class=md-nav__link> <span class=md-ellipsis> FL üéØ </span> </a> </li> <li class=md-nav__item> <a href=../../../MULTI-AGENT/multi-agent/ class=md-nav__link> <span class=md-ellipsis> MULTI-AGENT üéØ </span> </a> </li> <li class=md-nav__item> <a href=../../../CV/cv/ class=md-nav__link> <span class=md-ellipsis> CV üéØ </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_12_12 checked> <label class=md-nav__link for=__nav_12_12 id=__nav_12_12_label tabindex> <span class=md-ellipsis> NOTE-D2L üéØ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_12_12_label aria-expanded=true> <label class=md-nav__title for=__nav_12_12> <span class="md-nav__icon md-icon"></span> NOTE-D2L üéØ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CH1-INTRO/ch1-intro/ class=md-nav__link> <span class=md-ellipsis> CH1-INTRO </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> CH2-PRE </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> CH2-PRE </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#21-data-manipulation class=md-nav__link> <span class=md-ellipsis> 2.1 Data Manipulation </span> </a> <nav class=md-nav aria-label="2.1 Data Manipulation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#211-data-acquisition class=md-nav__link> <span class=md-ellipsis> 2.1.1 Data Acquisition </span> </a> </li> <li class=md-nav__item> <a href=#212-operations class=md-nav__link> <span class=md-ellipsis> 2.1.2 Operations </span> </a> </li> <li class=md-nav__item> <a href=#213-broadcasting-mechanism class=md-nav__link> <span class=md-ellipsis> 2.1.3 Broadcasting Mechanism </span> </a> </li> <li class=md-nav__item> <a href=#214-indexing-and-slicing class=md-nav__link> <span class=md-ellipsis> 2.1.4 Indexing and Slicing </span> </a> </li> <li class=md-nav__item> <a href=#215-saving-memory class=md-nav__link> <span class=md-ellipsis> 2.1.5 Saving Memory </span> </a> </li> <li class=md-nav__item> <a href=#216-conversion-to-other-python-objects class=md-nav__link> <span class=md-ellipsis> 2.1.6 Conversion to Other Python Objects </span> </a> </li> <li class=md-nav__item> <a href=#217-summary class=md-nav__link> <span class=md-ellipsis> 2.1.7 Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#22-data-preprocessing class=md-nav__link> <span class=md-ellipsis> 2.2 Data Preprocessing </span> </a> <nav class=md-nav aria-label="2.2 Data Preprocessing"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#221-reading-the-dataset class=md-nav__link> <span class=md-ellipsis> 2.2.1 Reading the Dataset </span> </a> </li> <li class=md-nav__item> <a href=#222-data-preparation class=md-nav__link> <span class=md-ellipsis> 2.2.2 Data Preparation </span> </a> </li> <li class=md-nav__item> <a href=#223-conversion-to-the-tensor-format class=md-nav__link> <span class=md-ellipsis> 2.2.3 Conversion to the Tensor Format </span> </a> </li> <li class=md-nav__item> <a href=#224-summary class=md-nav__link> <span class=md-ellipsis> 2.2.4 Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#23-linear-algebra class=md-nav__link> <span class=md-ellipsis> 2.3 Linear Algebra </span> </a> <nav class=md-nav aria-label="2.3 Linear Algebra"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#231-scalars class=md-nav__link> <span class=md-ellipsis> 2.3.1 Scalars </span> </a> </li> <li class=md-nav__item> <a href=#232-vectors class=md-nav__link> <span class=md-ellipsis> 2.3.2 Vectors </span> </a> </li> <li class=md-nav__item> <a href=#233-matrices class=md-nav__link> <span class=md-ellipsis> 2.3.3 Matrices </span> </a> </li> <li class=md-nav__item> <a href=#234-tensors class=md-nav__link> <span class=md-ellipsis> 2.3.4 Tensors </span> </a> </li> <li class=md-nav__item> <a href=#235-basic-properties-of-tensor-arithmetic class=md-nav__link> <span class=md-ellipsis> 2.3.5 Basic Properties of Tensor Arithmetic </span> </a> </li> <li class=md-nav__item> <a href=#236-reduction class=md-nav__link> <span class=md-ellipsis> 2.3.6 Reduction </span> </a> </li> <li class=md-nav__item> <a href=#237-non-reduction-sum class=md-nav__link> <span class=md-ellipsis> 2.3.7 Non-Reduction Sum </span> </a> </li> <li class=md-nav__item> <a href=#238-dot-products class=md-nav__link> <span class=md-ellipsis> 2.3.8 Dot Products </span> </a> </li> <li class=md-nav__item> <a href=#239-matrix-vector-products class=md-nav__link> <span class=md-ellipsis> 2.3.9 Matrix-Vector Products </span> </a> </li> <li class=md-nav__item> <a href=#2310-matrix-matrix-multiplication class=md-nav__link> <span class=md-ellipsis> 2.3.10 Matrix-Matrix Multiplication </span> </a> </li> <li class=md-nav__item> <a href=#2311-norms class=md-nav__link> <span class=md-ellipsis> 2.3.11 Norms </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#24-calculus class=md-nav__link> <span class=md-ellipsis> 2.4 Calculus </span> </a> <nav class=md-nav aria-label="2.4 Calculus"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#241-derivatives-and-differentiation class=md-nav__link> <span class=md-ellipsis> 2.4.1. Derivatives and Differentiation </span> </a> </li> <li class=md-nav__item> <a href=#242-partial-derivatives class=md-nav__link> <span class=md-ellipsis> 2.4.2. Partial Derivatives </span> </a> </li> <li class=md-nav__item> <a href=#243-gradients class=md-nav__link> <span class=md-ellipsis> 2.4.3. Gradients </span> </a> </li> <li class=md-nav__item> <a href=#244-chain-rule class=md-nav__link> <span class=md-ellipsis> 2.4.4. Chain Rule </span> </a> </li> <li class=md-nav__item> <a href=#245-discussion class=md-nav__link> <span class=md-ellipsis> 2.4.5. Discussion </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#25-automatic-differentiation class=md-nav__link> <span class=md-ellipsis> 2.5 Automatic Differentiation </span> </a> <nav class=md-nav aria-label="2.5 Automatic Differentiation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#251-a-simple-example class=md-nav__link> <span class=md-ellipsis> 2.5.1. A Simple Example </span> </a> </li> <li class=md-nav__item> <a href=#252-backward-for-non-scalar-variables class=md-nav__link> <span class=md-ellipsis> 2.5.2. Backward for Non-Scalar Variables </span> </a> </li> <li class=md-nav__item> <a href=#253-detaching-computation class=md-nav__link> <span class=md-ellipsis> 2.5.3. Detaching Computation </span> </a> </li> <li class=md-nav__item> <a href=#254-computing-the-gradient-of-python-control-flow class=md-nav__link> <span class=md-ellipsis> 2.5.4. Computing the Gradient of Python Control Flow </span> </a> </li> <li class=md-nav__item> <a href=#255-discussion class=md-nav__link> <span class=md-ellipsis> 2.5.5. Discussion </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#26-probability-and-statistics class=md-nav__link> <span class=md-ellipsis> 2.6 Probability and Statistics </span> </a> <nav class=md-nav aria-label="2.6 Probability and Statistics"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#261-basics class=md-nav__link> <span class=md-ellipsis> 2.6.1. Basics </span> </a> </li> <li class=md-nav__item> <a href=#262-advanced-topics class=md-nav__link> <span class=md-ellipsis> 2.6.2 Advanced Topics </span> </a> </li> <li class=md-nav__item> <a href=#263-random-variables class=md-nav__link> <span class=md-ellipsis> 2.6.3 Random Variables </span> </a> </li> <li class=md-nav__item> <a href=#264-multile-random-variables class=md-nav__link> <span class=md-ellipsis> 2.6.4 Multile Random Variables </span> </a> </li> <li class=md-nav__item> <a href=#265-expectation class=md-nav__link> <span class=md-ellipsis> 2.6.5 Expectation </span> </a> </li> <li class=md-nav__item> <a href=#266-variance class=md-nav__link> <span class=md-ellipsis> 2.6.6 Variance </span> </a> </li> <li class=md-nav__item> <a href=#267-summary class=md-nav__link> <span class=md-ellipsis> 2.6.7 Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#27-documentation class=md-nav__link> <span class=md-ellipsis> 2.7 Documentation </span> </a> <nav class=md-nav aria-label="2.7 Documentation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#271-functions-and-classes-in-a-module class=md-nav__link> <span class=md-ellipsis> 2.7.1. Functions and Classes in a Module </span> </a> </li> <li class=md-nav__item> <a href=#272-specific-functions-and-classes class=md-nav__link> <span class=md-ellipsis> 2.7.2. Specific Functions and Classes </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../CH3-LNN/ch3-lnn/ class=md-nav__link> <span class=md-ellipsis> CH3-LNN üéØ </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_13> <label class=md-nav__link for=__nav_13 id=__nav_13_label tabindex> <span class=md-ellipsis> SHM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_13_label aria-expanded=false> <label class=md-nav__title for=__nav_13> <span class="md-nav__icon md-icon"></span> SHM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../SHM/shm/ class=md-nav__link> <span class=md-ellipsis> Structural Health Monitoring </span> </a> </li> <li class=md-nav__item> <a href=../../../../SHM/NSEL-REPORT/nsel-report/ class=md-nav__link> <span class=md-ellipsis> NSEL-REPORT </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_14> <label class=md-nav__link for=__nav_14 id=__nav_14_label tabindex> <span class=md-ellipsis> DEV </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_14_label aria-expanded=false> <label class=md-nav__title for=__nav_14> <span class="md-nav__icon md-icon"></span> DEV </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_14_1> <label class=md-nav__link for=__nav_14_1 id=__nav_14_1_label tabindex> <span class=md-ellipsis> DEVENV </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_14_1_label aria-expanded=false> <label class=md-nav__title for=__nav_14_1> <span class="md-nav__icon md-icon"></span> DEVENV </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DEV/DEVENV/LATEX/latex/ class=md-nav__link> <span class=md-ellipsis> LATEX </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_14_1_2> <label class=md-nav__link for=__nav_14_1_2 id=__nav_14_1_2_label tabindex=0> <span class=md-ellipsis> REMOTE-CONTROL </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_14_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_14_1_2> <span class="md-nav__icon md-icon"></span> REMOTE-CONTROL </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DEV/DEVENV/REMOTE-CONTROL/remote-control/ class=md-nav__link> <span class=md-ellipsis> Remote Control </span> </a> </li> <li class=md-nav__item> <a href=../../../../DEV/DEVENV/REMOTE-CONTROL/RUSTDESK/rustdesk/ class=md-nav__link> <span class=md-ellipsis> RUSTDESK </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../DEV/DEVENV/CLOUD-SYNC/cloud-sync/ class=md-nav__link> <span class=md-ellipsis> CLOUD-SYNC </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_14_2> <label class=md-nav__link for=__nav_14_2 id=__nav_14_2_label tabindex> <span class=md-ellipsis> EMBEDDED </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_14_2_label aria-expanded=false> <label class=md-nav__title for=__nav_14_2> <span class="md-nav__icon md-icon"></span> EMBEDDED </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DEV/EMBEDDED/STM32MP157/stm32mp157/ class=md-nav__link> <span class=md-ellipsis> STM32MP157 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_14_3> <label class=md-nav__link for=__nav_14_3 id=__nav_14_3_label tabindex> <span class=md-ellipsis> XNODE üéØ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_14_3_label aria-expanded=false> <label class=md-nav__title for=__nav_14_3> <span class="md-nav__icon md-icon"></span> XNODE üéØ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DEV/XNODE/XNODE-DOCS/xnode-docs/ class=md-nav__link> <span class=md-ellipsis> XNODE-DOCS </span> </a> </li> <li class=md-nav__item> <a href=../../../../DEV/XNODE/BASIC-VERSION/basic-version/ class=md-nav__link> <span class=md-ellipsis> BASIC-VERSION </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../DEV/FRONT-END/front-end/ class=md-nav__link> <span class=md-ellipsis> FRONT-END </span> </a> </li> <li class=md-nav__item> <a href=../../../../DEV/BACK-END/back-end/ class=md-nav__link> <span class=md-ellipsis> BACK-END </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_14_6> <label class=md-nav__link for=__nav_14_6 id=__nav_14_6_label tabindex> <span class=md-ellipsis> VISUALIZATION </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_14_6_label aria-expanded=false> <label class=md-nav__title for=__nav_14_6> <span class="md-nav__icon md-icon"></span> VISUALIZATION </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DEV/VISUALIZATION/WEB-BASED/web-based/ class=md-nav__link> <span class=md-ellipsis> WEB-BASED </span> </a> </li> <li class=md-nav__item> <a href=../../../../DEV/VISUALIZATION/CLIENT-BASED/client-based/ class=md-nav__link> <span class=md-ellipsis> CLIENT-BASED </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_15> <label class=md-nav__link for=__nav_15 id=__nav_15_label tabindex> <span class=md-ellipsis> RESEARCH </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_15_label aria-expanded=false> <label class=md-nav__title for=__nav_15> <span class="md-nav__icon md-icon"></span> RESEARCH </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../RESEARCH/research/ class=md-nav__link> <span class=md-ellipsis> Researches </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_16> <label class=md-nav__link for=__nav_16 id=__nav_16_label tabindex> <span class=md-ellipsis> PROJECT </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_16_label aria-expanded=false> <label class=md-nav__title for=__nav_16> <span class="md-nav__icon md-icon"></span> PROJECT </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../PROJECT/project/ class=md-nav__link> <span class=md-ellipsis> Projects </span> </a> </li> <li class=md-nav__item> <a href=../../../../PROJECT/TECH-BLOG/mkdocs_and_material/ class=md-nav__link> <span class=md-ellipsis> TECH-BLOG </span> </a> </li> <li class=md-nav__item> <a href=../../../../PROJECT/WAREHOUSE/warehouse/ class=md-nav__link> <span class=md-ellipsis> WAREHOUSE </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#21-data-manipulation class=md-nav__link> <span class=md-ellipsis> 2.1 Data Manipulation </span> </a> <nav class=md-nav aria-label="2.1 Data Manipulation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#211-data-acquisition class=md-nav__link> <span class=md-ellipsis> 2.1.1 Data Acquisition </span> </a> </li> <li class=md-nav__item> <a href=#212-operations class=md-nav__link> <span class=md-ellipsis> 2.1.2 Operations </span> </a> </li> <li class=md-nav__item> <a href=#213-broadcasting-mechanism class=md-nav__link> <span class=md-ellipsis> 2.1.3 Broadcasting Mechanism </span> </a> </li> <li class=md-nav__item> <a href=#214-indexing-and-slicing class=md-nav__link> <span class=md-ellipsis> 2.1.4 Indexing and Slicing </span> </a> </li> <li class=md-nav__item> <a href=#215-saving-memory class=md-nav__link> <span class=md-ellipsis> 2.1.5 Saving Memory </span> </a> </li> <li class=md-nav__item> <a href=#216-conversion-to-other-python-objects class=md-nav__link> <span class=md-ellipsis> 2.1.6 Conversion to Other Python Objects </span> </a> </li> <li class=md-nav__item> <a href=#217-summary class=md-nav__link> <span class=md-ellipsis> 2.1.7 Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#22-data-preprocessing class=md-nav__link> <span class=md-ellipsis> 2.2 Data Preprocessing </span> </a> <nav class=md-nav aria-label="2.2 Data Preprocessing"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#221-reading-the-dataset class=md-nav__link> <span class=md-ellipsis> 2.2.1 Reading the Dataset </span> </a> </li> <li class=md-nav__item> <a href=#222-data-preparation class=md-nav__link> <span class=md-ellipsis> 2.2.2 Data Preparation </span> </a> </li> <li class=md-nav__item> <a href=#223-conversion-to-the-tensor-format class=md-nav__link> <span class=md-ellipsis> 2.2.3 Conversion to the Tensor Format </span> </a> </li> <li class=md-nav__item> <a href=#224-summary class=md-nav__link> <span class=md-ellipsis> 2.2.4 Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#23-linear-algebra class=md-nav__link> <span class=md-ellipsis> 2.3 Linear Algebra </span> </a> <nav class=md-nav aria-label="2.3 Linear Algebra"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#231-scalars class=md-nav__link> <span class=md-ellipsis> 2.3.1 Scalars </span> </a> </li> <li class=md-nav__item> <a href=#232-vectors class=md-nav__link> <span class=md-ellipsis> 2.3.2 Vectors </span> </a> </li> <li class=md-nav__item> <a href=#233-matrices class=md-nav__link> <span class=md-ellipsis> 2.3.3 Matrices </span> </a> </li> <li class=md-nav__item> <a href=#234-tensors class=md-nav__link> <span class=md-ellipsis> 2.3.4 Tensors </span> </a> </li> <li class=md-nav__item> <a href=#235-basic-properties-of-tensor-arithmetic class=md-nav__link> <span class=md-ellipsis> 2.3.5 Basic Properties of Tensor Arithmetic </span> </a> </li> <li class=md-nav__item> <a href=#236-reduction class=md-nav__link> <span class=md-ellipsis> 2.3.6 Reduction </span> </a> </li> <li class=md-nav__item> <a href=#237-non-reduction-sum class=md-nav__link> <span class=md-ellipsis> 2.3.7 Non-Reduction Sum </span> </a> </li> <li class=md-nav__item> <a href=#238-dot-products class=md-nav__link> <span class=md-ellipsis> 2.3.8 Dot Products </span> </a> </li> <li class=md-nav__item> <a href=#239-matrix-vector-products class=md-nav__link> <span class=md-ellipsis> 2.3.9 Matrix-Vector Products </span> </a> </li> <li class=md-nav__item> <a href=#2310-matrix-matrix-multiplication class=md-nav__link> <span class=md-ellipsis> 2.3.10 Matrix-Matrix Multiplication </span> </a> </li> <li class=md-nav__item> <a href=#2311-norms class=md-nav__link> <span class=md-ellipsis> 2.3.11 Norms </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#24-calculus class=md-nav__link> <span class=md-ellipsis> 2.4 Calculus </span> </a> <nav class=md-nav aria-label="2.4 Calculus"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#241-derivatives-and-differentiation class=md-nav__link> <span class=md-ellipsis> 2.4.1. Derivatives and Differentiation </span> </a> </li> <li class=md-nav__item> <a href=#242-partial-derivatives class=md-nav__link> <span class=md-ellipsis> 2.4.2. Partial Derivatives </span> </a> </li> <li class=md-nav__item> <a href=#243-gradients class=md-nav__link> <span class=md-ellipsis> 2.4.3. Gradients </span> </a> </li> <li class=md-nav__item> <a href=#244-chain-rule class=md-nav__link> <span class=md-ellipsis> 2.4.4. Chain Rule </span> </a> </li> <li class=md-nav__item> <a href=#245-discussion class=md-nav__link> <span class=md-ellipsis> 2.4.5. Discussion </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#25-automatic-differentiation class=md-nav__link> <span class=md-ellipsis> 2.5 Automatic Differentiation </span> </a> <nav class=md-nav aria-label="2.5 Automatic Differentiation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#251-a-simple-example class=md-nav__link> <span class=md-ellipsis> 2.5.1. A Simple Example </span> </a> </li> <li class=md-nav__item> <a href=#252-backward-for-non-scalar-variables class=md-nav__link> <span class=md-ellipsis> 2.5.2. Backward for Non-Scalar Variables </span> </a> </li> <li class=md-nav__item> <a href=#253-detaching-computation class=md-nav__link> <span class=md-ellipsis> 2.5.3. Detaching Computation </span> </a> </li> <li class=md-nav__item> <a href=#254-computing-the-gradient-of-python-control-flow class=md-nav__link> <span class=md-ellipsis> 2.5.4. Computing the Gradient of Python Control Flow </span> </a> </li> <li class=md-nav__item> <a href=#255-discussion class=md-nav__link> <span class=md-ellipsis> 2.5.5. Discussion </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#26-probability-and-statistics class=md-nav__link> <span class=md-ellipsis> 2.6 Probability and Statistics </span> </a> <nav class=md-nav aria-label="2.6 Probability and Statistics"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#261-basics class=md-nav__link> <span class=md-ellipsis> 2.6.1. Basics </span> </a> </li> <li class=md-nav__item> <a href=#262-advanced-topics class=md-nav__link> <span class=md-ellipsis> 2.6.2 Advanced Topics </span> </a> </li> <li class=md-nav__item> <a href=#263-random-variables class=md-nav__link> <span class=md-ellipsis> 2.6.3 Random Variables </span> </a> </li> <li class=md-nav__item> <a href=#264-multile-random-variables class=md-nav__link> <span class=md-ellipsis> 2.6.4 Multile Random Variables </span> </a> </li> <li class=md-nav__item> <a href=#265-expectation class=md-nav__link> <span class=md-ellipsis> 2.6.5 Expectation </span> </a> </li> <li class=md-nav__item> <a href=#266-variance class=md-nav__link> <span class=md-ellipsis> 2.6.6 Variance </span> </a> </li> <li class=md-nav__item> <a href=#267-summary class=md-nav__link> <span class=md-ellipsis> 2.6.7 Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#27-documentation class=md-nav__link> <span class=md-ellipsis> 2.7 Documentation </span> </a> <nav class=md-nav aria-label="2.7 Documentation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#271-functions-and-classes-in-a-module class=md-nav__link> <span class=md-ellipsis> 2.7.1. Functions and Classes in a Module </span> </a> </li> <li class=md-nav__item> <a href=#272-specific-functions-and-classes class=md-nav__link> <span class=md-ellipsis> 2.7.2. Specific Functions and Classes </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/Shuaiwen-Cui/Infinity.git/edit/master/docs/AI/NOTE-D2L/CH2-PRE/ch2-pre.en.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg> </a> <a href=https://github.com/Shuaiwen-Cui/Infinity.git/raw/master/docs/AI/NOTE-D2L/CH2-PRE/ch2-pre.en.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg> </a> <h1 id=ch02-preliminaries>CH02 - Preliminaries<a class=headerlink href=#ch02-preliminaries title="Permanent link">&para;</a></h1> <p>To prepare for your dive into deep learning, you will need a few survival skills: (i) techniques for storing and manipulating data; (ii) libraries for ingesting and preprocessing data from a variety of sources; (iii) knowledge of the basic linear algebraic operations that we apply to high-dimensional data elements; (iv) just enough calculus to determine which direction to adjust each parameter in order to decrease the loss function; (v) the ability to automatically compute derivatives so that you can forget much of the calculus you just learned; (vi) some basic fluency in probability, our primary language for reasoning under uncertainty; and (vii) some aptitude for finding answers in the official documentation when you get stuck.</p> <p>In short, this chapter provides a rapid introduction to the basics that you will need to follow most of the technical content in this book.</p> <h2 id=21-data-manipulation>2.1 Data Manipulation<a class=headerlink href=#21-data-manipulation title="Permanent link">&para;</a></h2> <h3 id=211-data-acquisition>2.1.1 Data Acquisition<a class=headerlink href=#211-data-acquisition title="Permanent link">&para;</a></h3> <p>import <code>torch</code> and <code>torchvision</code> for data acquisition</p> <div class="tabbed-set tabbed-alternate" data-tabs=1:2><input checked=checked id=211-data-acquisition-pytorch name=__tabbed_1 type=radio><input id=211-data-acquisition-tensorflow name=__tabbed_1 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch>PYTORCH</label><label for=211-data-acquisition-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></code></pre></div> </div> </div> </div> <p>tensor is a multi-dimensional array with support for autograd operations like <code>backward()</code>. We can specify the data type of the tensor, e.g., <code>float32</code>, <code>int32</code>, <code>uint8</code>, and <code>bool</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=2:2><input checked=checked id=211-data-acquisition-pytorch_1 name=__tabbed_2 type=radio><input id=211-data-acquisition-tensorflow_1 name=__tabbed_2 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch_1>PYTORCH</label><label for=211-data-acquisition-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>If you want to check the shape of a tensor, you can access its shape property.</p> <div class="tabbed-set tabbed-alternate" data-tabs=3:2><input checked=checked id=211-data-acquisition-pytorch_2 name=__tabbed_3 type=radio><input id=211-data-acquisition-tensorflow_2 name=__tabbed_3 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch_2>PYTORCH</label><label for=211-data-acquisition-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>torch.Size([12])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>TensorShape([12])
</span></code></pre></div> </div> </div> </div> <p>If we just want to know the total number of elements in a tensor, i.e., the product of all of the shape elements, we can inspect its size. Because we are dealing with a vector here, the single element of its shape is identical to its size.</p> <div class="tabbed-set tabbed-alternate" data-tabs=4:2><input checked=checked id=211-data-acquisition-pytorch_3 name=__tabbed_4 type=radio><input id=211-data-acquisition-tensorflow_3 name=__tabbed_4 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch_3>PYTORCH</label><label for=211-data-acquisition-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=n>x</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a>12
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=n>tf</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a>&lt;tf.Tensor: shape=(), dtype=int32, numpy=12&gt;
</span></code></pre></div> </div> </div> </div> <p>To change the shape of a tensor without altering either the number of elements or their values, we can invoke the <code>reshape</code> function. For example, we can transform our tensor, <code>x</code>, from a row vector with shape <code>(12,)</code> to a matrix with shape <code>(3, 4)</code>. This new tensor contains the exact same values as the old tensor, but it views them as a matrix organized as a stack of rows.</p> <div class="tabbed-set tabbed-alternate" data-tabs=5:2><input checked=checked id=211-data-acquisition-pytorch_4 name=__tabbed_5 type=radio><input id=211-data-acquisition-tensorflow_4 name=__tabbed_5 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch_4>PYTORCH</label><label for=211-data-acquisition-tensorflow_4>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a>tensor([[ 0,  1,  2,  3],
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a>        [ 4,  5,  6,  7],
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a>        [ 8,  9, 10, 11]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a>array([[ 0,  1,  2,  3],
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a>       [ 4,  5,  6,  7],
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a>       [ 8,  9, 10, 11]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>If we only know how many columns we want but are unsure about the number of rows, we can specify this with the special value <code>-1</code>. In our case, instead of calling <code>x.reshape(3, 4)</code>, we could have equivalently called <code>x.reshape(-1, 4)</code> or <code>x.reshape(3, -1)</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=6:2><input checked=checked id=211-data-acquisition-pytorch_5 name=__tabbed_6 type=radio><input id=211-data-acquisition-tensorflow_5 name=__tabbed_6 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch_5>PYTORCH</label><label for=211-data-acquisition-tensorflow_5>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a>tensor([[ 0,  1,  2,  3],
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a>        [ 4,  5,  6,  7],
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a>        [ 8,  9, 10, 11]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a>array([[ 0,  1,  2,  3],
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>       [ 4,  5,  6,  7],
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a>       [ 8,  9, 10, 11]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>Sometimes, we want to use all 0 or all 1 values as the initial value for a tensor. We can create a tensor full of zeros with the specified shape by calling <code>zeros</code>. Similarly, we can create tensors filled with 1 by calling <code>ones</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=7:2><input checked=checked id=211-data-acquisition-pytorch_6 name=__tabbed_7 type=radio><input id=211-data-acquisition-tensorflow_6 name=__tabbed_7 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch_6>PYTORCH</label><label for=211-data-acquisition-tensorflow_6>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a>tensor([[[0., 0., 0., 0.],
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a>         [0., 0., 0., 0.],
</span><span id=__span-23-3><a id=__codelineno-23-3 name=__codelineno-23-3 href=#__codelineno-23-3></a>         [0., 0., 0., 0.]],
</span><span id=__span-23-4><a id=__codelineno-23-4 name=__codelineno-23-4 href=#__codelineno-23-4></a>
</span><span id=__span-23-5><a id=__codelineno-23-5 name=__codelineno-23-5 href=#__codelineno-23-5></a>        [[0., 0., 0., 0.],
</span><span id=__span-23-6><a id=__codelineno-23-6 name=__codelineno-23-6 href=#__codelineno-23-6></a>         [0., 0., 0., 0.],
</span><span id=__span-23-7><a id=__codelineno-23-7 name=__codelineno-23-7 href=#__codelineno-23-7></a>         [0., 0., 0., 0.]]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=n>tf</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a>&lt;tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a>array([[[0., 0., 0., 0.],
</span><span id=__span-25-3><a id=__codelineno-25-3 name=__codelineno-25-3 href=#__codelineno-25-3></a>        [0., 0., 0., 0.],
</span><span id=__span-25-4><a id=__codelineno-25-4 name=__codelineno-25-4 href=#__codelineno-25-4></a>        [0., 0., 0., 0.]],
</span><span id=__span-25-5><a id=__codelineno-25-5 name=__codelineno-25-5 href=#__codelineno-25-5></a>
</span><span id=__span-25-6><a id=__codelineno-25-6 name=__codelineno-25-6 href=#__codelineno-25-6></a>       [[0., 0., 0., 0.],
</span><span id=__span-25-7><a id=__codelineno-25-7 name=__codelineno-25-7 href=#__codelineno-25-7></a>        [0., 0., 0., 0.],
</span><span id=__span-25-8><a id=__codelineno-25-8 name=__codelineno-25-8 href=#__codelineno-25-8></a>        [0., 0., 0., 0.]]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>All 1s are not as commonly used as all 0s, but we can still call <code>ones</code> for any tensor shape to get a tensor containing all 1s.</p> <div class="tabbed-set tabbed-alternate" data-tabs=8:2><input checked=checked id=211-data-acquisition-pytorch_7 name=__tabbed_8 type=radio><input id=211-data-acquisition-tensorflow_7 name=__tabbed_8 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch_7>PYTORCH</label><label for=211-data-acquisition-tensorflow_7>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a>tensor([[[1., 1., 1., 1.],
</span><span id=__span-27-2><a id=__codelineno-27-2 name=__codelineno-27-2 href=#__codelineno-27-2></a>         [1., 1., 1., 1.],
</span><span id=__span-27-3><a id=__codelineno-27-3 name=__codelineno-27-3 href=#__codelineno-27-3></a>         [1., 1., 1., 1.]],
</span><span id=__span-27-4><a id=__codelineno-27-4 name=__codelineno-27-4 href=#__codelineno-27-4></a>
</span><span id=__span-27-5><a id=__codelineno-27-5 name=__codelineno-27-5 href=#__codelineno-27-5></a>        [[1., 1., 1., 1.],
</span><span id=__span-27-6><a id=__codelineno-27-6 name=__codelineno-27-6 href=#__codelineno-27-6></a>         [1., 1., 1., 1.],
</span><span id=__span-27-7><a id=__codelineno-27-7 name=__codelineno-27-7 href=#__codelineno-27-7></a>         [1., 1., 1., 1.]]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a><span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a>&lt;tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=
</span><span id=__span-29-2><a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a>array([[[1., 1., 1., 1.],
</span><span id=__span-29-3><a id=__codelineno-29-3 name=__codelineno-29-3 href=#__codelineno-29-3></a>        [1., 1., 1., 1.],
</span><span id=__span-29-4><a id=__codelineno-29-4 name=__codelineno-29-4 href=#__codelineno-29-4></a>        [1., 1., 1., 1.]],
</span><span id=__span-29-5><a id=__codelineno-29-5 name=__codelineno-29-5 href=#__codelineno-29-5></a>
</span><span id=__span-29-6><a id=__codelineno-29-6 name=__codelineno-29-6 href=#__codelineno-29-6></a>       [[1., 1., 1., 1.],
</span><span id=__span-29-7><a id=__codelineno-29-7 name=__codelineno-29-7 href=#__codelineno-29-7></a>        [1., 1., 1., 1.],
</span><span id=__span-29-8><a id=__codelineno-29-8 name=__codelineno-29-8 href=#__codelineno-29-8></a>        [1., 1., 1., 1.]]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>We can also specify the exact values for each element in the desired tensor by supplying a Python list (or list of lists) containing the numerical values.</p> <div class="tabbed-set tabbed-alternate" data-tabs=9:2><input checked=checked id=211-data-acquisition-pytorch_8 name=__tabbed_9 type=radio><input id=211-data-acquisition-tensorflow_8 name=__tabbed_9 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch_8>PYTORCH</label><label for=211-data-acquisition-tensorflow_8>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a>tensor([[2, 1, 4, 3],
</span><span id=__span-31-2><a id=__codelineno-31-2 name=__codelineno-31-2 href=#__codelineno-31-2></a>        [1, 2, 3, 4],
</span><span id=__span-31-3><a id=__codelineno-31-3 name=__codelineno-31-3 href=#__codelineno-31-3></a>        [4, 3, 2, 1]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1 href=#__codelineno-32-1></a><span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1 href=#__codelineno-33-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=
</span><span id=__span-33-2><a id=__codelineno-33-2 name=__codelineno-33-2 href=#__codelineno-33-2></a>array([[2, 1, 4, 3],
</span><span id=__span-33-3><a id=__codelineno-33-3 name=__codelineno-33-3 href=#__codelineno-33-3></a>       [1, 2, 3, 4],
</span><span id=__span-33-4><a id=__codelineno-33-4 name=__codelineno-33-4 href=#__codelineno-33-4></a>       [4, 3, 2, 1]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>We can create tensors whose elements are sampled randomly from a standard Gaussian (normal) distribution with a mean of 0 and a standard deviation of 1.</p> <div class="tabbed-set tabbed-alternate" data-tabs=10:2><input checked=checked id=211-data-acquisition-pytorch_9 name=__tabbed_10 type=radio><input id=211-data-acquisition-tensorflow_9 name=__tabbed_10 type=radio><div class=tabbed-labels><label for=211-data-acquisition-pytorch_9>PYTORCH</label><label for=211-data-acquisition-tensorflow_9>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1 href=#__codelineno-34-1></a><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1 href=#__codelineno-35-1></a>tensor([[ 0.1835,  0.8077,  0.4194, -0.0254],
</span><span id=__span-35-2><a id=__codelineno-35-2 name=__codelineno-35-2 href=#__codelineno-35-2></a>        [-0.3695, -0.1367, -0.2394, -0.0985],
</span><span id=__span-35-3><a id=__codelineno-35-3 name=__codelineno-35-3 href=#__codelineno-35-3></a>        [-0.6557, -0.0235, -0.8858, -0.7724]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1 href=#__codelineno-36-1></a><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1 href=#__codelineno-37-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
</span><span id=__span-37-2><a id=__codelineno-37-2 name=__codelineno-37-2 href=#__codelineno-37-2></a>array([[ 0.1835,  0.8077,  0.4194, -0.0254],
</span><span id=__span-37-3><a id=__codelineno-37-3 name=__codelineno-37-3 href=#__codelineno-37-3></a>       [-0.3695, -0.1367, -0.2394, -0.0985],
</span><span id=__span-37-4><a id=__codelineno-37-4 name=__codelineno-37-4 href=#__codelineno-37-4></a>       [-0.6557, -0.0235, -0.8858, -0.7724]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=212-operations>2.1.2 Operations<a class=headerlink href=#212-operations title="Permanent link">&para;</a></h3> <p>Tensor supports a wide range of element-wise operations, including arithmetic operations (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>**</code>) and various advanced operations, such as <code>sin</code>, <code>cos</code>, and <code>exp</code>. We can call element-wise operations on any two tensors of the same shape. In the following example, we use commas to formulate a 5-element tuple, where each element is the result of an element-wise operation.</p> <div class="tabbed-set tabbed-alternate" data-tabs=11:2><input checked=checked id=212-operations-pytorch name=__tabbed_11 type=radio><input id=212-operations-tensorflow name=__tabbed_11 type=radio><div class=tabbed-labels><label for=212-operations-pytorch>PYTORCH</label><label for=212-operations-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-38-1><a id=__codelineno-38-1 name=__codelineno-38-1 href=#__codelineno-38-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>])</span>
</span><span id=__span-38-2><a id=__codelineno-38-2 name=__codelineno-38-2 href=#__codelineno-38-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span><span id=__span-38-3><a id=__codelineno-38-3 name=__codelineno-38-3 href=#__codelineno-38-3></a><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>/</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>**</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-39-1><a id=__codelineno-39-1 name=__codelineno-39-1 href=#__codelineno-39-1></a>(tensor([ 3.,  4.,  6., 10.]),
</span><span id=__span-39-2><a id=__codelineno-39-2 name=__codelineno-39-2 href=#__codelineno-39-2></a> tensor([-1.,  0.,  2.,  6.]),
</span><span id=__span-39-3><a id=__codelineno-39-3 name=__codelineno-39-3 href=#__codelineno-39-3></a> tensor([ 2.,  4.,  8., 16.]),
</span><span id=__span-39-4><a id=__codelineno-39-4 name=__codelineno-39-4 href=#__codelineno-39-4></a> tensor([0.5000, 1.0000, 2.0000, 4.0000]),
</span><span id=__span-39-5><a id=__codelineno-39-5 name=__codelineno-39-5 href=#__codelineno-39-5></a> tensor([ 1.,  4., 16., 64.]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-40-1><a id=__codelineno-40-1 name=__codelineno-40-1 href=#__codelineno-40-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>1.0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>])</span>
</span><span id=__span-40-2><a id=__codelineno-40-2 name=__codelineno-40-2 href=#__codelineno-40-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span><span id=__span-40-3><a id=__codelineno-40-3 name=__codelineno-40-3 href=#__codelineno-40-3></a><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>/</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>**</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-41-1><a id=__codelineno-41-1 name=__codelineno-41-1 href=#__codelineno-41-1></a>(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.,  4.,  6., 10.], dtype=float32)&gt;,
</span><span id=__span-41-2><a id=__codelineno-41-2 name=__codelineno-41-2 href=#__codelineno-41-2></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.,  0.,  2.,  6.], dtype=float32)&gt;,
</span><span id=__span-41-3><a id=__codelineno-41-3 name=__codelineno-41-3 href=#__codelineno-41-3></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.,  4.,  8., 16.], dtype=float32)&gt;,
</span><span id=__span-41-4><a id=__codelineno-41-4 name=__codelineno-41-4 href=#__codelineno-41-4></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.5, 1. , 2. , 4. ], dtype=float32)&gt;,
</span><span id=__span-41-5><a id=__codelineno-41-5 name=__codelineno-41-5 href=#__codelineno-41-5></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.,  4., 16., 64.], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <p>Many more operations can be applied element-wise, including unary operators like exponentiation.</p> <div class="tabbed-set tabbed-alternate" data-tabs=12:2><input checked=checked id=212-operations-pytorch_1 name=__tabbed_12 type=radio><input id=212-operations-tensorflow_1 name=__tabbed_12 type=radio><div class=tabbed-labels><label for=212-operations-pytorch_1>PYTORCH</label><label for=212-operations-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-42-1><a id=__codelineno-42-1 name=__codelineno-42-1 href=#__codelineno-42-1></a><span class=n>x</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-43-1><a id=__codelineno-43-1 name=__codelineno-43-1 href=#__codelineno-43-1></a>tensor([ 2.7183,  7.3891, 54.5981,  2.7183])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-44-1><a id=__codelineno-44-1 name=__codelineno-44-1 href=#__codelineno-44-1></a><span class=n>tf</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-45-1><a id=__codelineno-45-1 name=__codelineno-45-1 href=#__codelineno-45-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.7183,  7.3891, 54.5981,  2.7183], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>We can also concatenate multiple tensors together, stacking them end-to-end to form a larger tensor. We just need to provide a list of tensors and tell the system along which axis to concatenate. The example below shows what happens when we concatenate tensors along rows (axis 0, the first element of the shape) vs. columns (axis 1, the second element of the shape).</p> <div class="tabbed-set tabbed-alternate" data-tabs=13:2><input checked=checked id=212-operations-pytorch_2 name=__tabbed_13 type=radio><input id=212-operations-tensorflow_2 name=__tabbed_13 type=radio><div class=tabbed-labels><label for=212-operations-pytorch_2>PYTORCH</label><label for=212-operations-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-46-1><a id=__codelineno-46-1 name=__codelineno-46-1 href=#__codelineno-46-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span><span id=__span-46-2><a id=__codelineno-46-2 name=__codelineno-46-2 href=#__codelineno-46-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>2.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span><span id=__span-46-3><a id=__codelineno-46-3 name=__codelineno-46-3 href=#__codelineno-46-3></a><span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-47-1><a id=__codelineno-47-1 name=__codelineno-47-1 href=#__codelineno-47-1></a>(tensor([[ 0.,  1.,  2.,  3.],
</span><span id=__span-47-2><a id=__codelineno-47-2 name=__codelineno-47-2 href=#__codelineno-47-2></a>         [ 4.,  5.,  6.,  7.],
</span><span id=__span-47-3><a id=__codelineno-47-3 name=__codelineno-47-3 href=#__codelineno-47-3></a>         [ 8.,  9., 10., 11.],
</span><span id=__span-47-4><a id=__codelineno-47-4 name=__codelineno-47-4 href=#__codelineno-47-4></a>         [ 2.,  1.,  4.,  3.],
</span><span id=__span-47-5><a id=__codelineno-47-5 name=__codelineno-47-5 href=#__codelineno-47-5></a>         [ 1.,  2.,  3.,  4.],
</span><span id=__span-47-6><a id=__codelineno-47-6 name=__codelineno-47-6 href=#__codelineno-47-6></a>         [ 4.,  3.,  2.,  1.]]),
</span><span id=__span-47-7><a id=__codelineno-47-7 name=__codelineno-47-7 href=#__codelineno-47-7></a> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
</span><span id=__span-47-8><a id=__codelineno-47-8 name=__codelineno-47-8 href=#__codelineno-47-8></a>         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
</span><span id=__span-47-9><a id=__codelineno-47-9 name=__codelineno-47-9 href=#__codelineno-47-9></a>         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-48-1><a id=__codelineno-48-1 name=__codelineno-48-1 href=#__codelineno-48-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>),</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span><span id=__span-48-2><a id=__codelineno-48-2 name=__codelineno-48-2 href=#__codelineno-48-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mf>2.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span><span id=__span-48-3><a id=__codelineno-48-3 name=__codelineno-48-3 href=#__codelineno-48-3></a><span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span> <span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-49-1><a id=__codelineno-49-1 name=__codelineno-49-1 href=#__codelineno-49-1></a>(&lt;tf.Tensor: shape=(6, 4), dtype=float32, numpy=
</span><span id=__span-49-2><a id=__codelineno-49-2 name=__codelineno-49-2 href=#__codelineno-49-2></a>array([[ 0.,  1.,  2.,  3.],
</span><span id=__span-49-3><a id=__codelineno-49-3 name=__codelineno-49-3 href=#__codelineno-49-3></a>       [ 4.,  5.,  6.,  7.],
</span><span id=__span-49-4><a id=__codelineno-49-4 name=__codelineno-49-4 href=#__codelineno-49-4></a>       [ 8.,  9., 10., 11.],
</span><span id=__span-49-5><a id=__codelineno-49-5 name=__codelineno-49-5 href=#__codelineno-49-5></a>       [ 2.,  1.,  4.,  3.],
</span><span id=__span-49-6><a id=__codelineno-49-6 name=__codelineno-49-6 href=#__codelineno-49-6></a>       [ 1.,  2.,  3.,  4.],
</span><span id=__span-49-7><a id=__codelineno-49-7 name=__codelineno-49-7 href=#__codelineno-49-7></a>       [ 4.,  3.,  2.,  1.]], dtype=float32)&gt;,
</span><span id=__span-49-8><a id=__codelineno-49-8 name=__codelineno-49-8 href=#__codelineno-49-8></a> &lt;tf.Tensor: shape=(3, 8), dtype=float32, numpy=
</span><span id=__span-49-9><a id=__codelineno-49-9 name=__codelineno-49-9 href=#__codelineno-49-9></a>array([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
</span><span id=__span-49-10><a id=__codelineno-49-10 name=__codelineno-49-10 href=#__codelineno-49-10></a>       [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
</span><span id=__span-49-11><a id=__codelineno-49-11 name=__codelineno-49-11 href=#__codelineno-49-11></a>       [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <p>Sometimes, we want to construct a binary tensor via logical statements. Take <code>x == y</code> as an example. For each position, if <code>x</code> and <code>y</code> are equal at that position, the corresponding entry in the new tensor takes a value of 1, meaning that the logical statement <code>x == y</code> is true at that position; otherwise that position takes 0.</p> <div class="tabbed-set tabbed-alternate" data-tabs=14:2><input checked=checked id=212-operations-pytorch_3 name=__tabbed_14 type=radio><input id=212-operations-tensorflow_3 name=__tabbed_14 type=radio><div class=tabbed-labels><label for=212-operations-pytorch_3>PYTORCH</label><label for=212-operations-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-50-1><a id=__codelineno-50-1 name=__codelineno-50-1 href=#__codelineno-50-1></a><span class=n>x</span> <span class=o>==</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-51-1><a id=__codelineno-51-1 name=__codelineno-51-1 href=#__codelineno-51-1></a>tensor([[False,  True, False,  True],
</span><span id=__span-51-2><a id=__codelineno-51-2 name=__codelineno-51-2 href=#__codelineno-51-2></a>        [False, False, False, False],
</span><span id=__span-51-3><a id=__codelineno-51-3 name=__codelineno-51-3 href=#__codelineno-51-3></a>        [False, False, False, False]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-52-1><a id=__codelineno-52-1 name=__codelineno-52-1 href=#__codelineno-52-1></a><span class=n>tf</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-53-1><a id=__codelineno-53-1 name=__codelineno-53-1 href=#__codelineno-53-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy=
</span><span id=__span-53-2><a id=__codelineno-53-2 name=__codelineno-53-2 href=#__codelineno-53-2></a>array([[False,  True, False,  True],
</span><span id=__span-53-3><a id=__codelineno-53-3 name=__codelineno-53-3 href=#__codelineno-53-3></a>       [False, False, False, False],
</span><span id=__span-53-4><a id=__codelineno-53-4 name=__codelineno-53-4 href=#__codelineno-53-4></a>       [False, False, False, False]])&gt;
</span></code></pre></div> </div> </div> </div> <p>Summing all the elements in the tensor yields a tensor with only one element.</p> <div class="tabbed-set tabbed-alternate" data-tabs=15:2><input checked=checked id=212-operations-pytorch_4 name=__tabbed_15 type=radio><input id=212-operations-tensorflow_4 name=__tabbed_15 type=radio><div class=tabbed-labels><label for=212-operations-pytorch_4>PYTORCH</label><label for=212-operations-tensorflow_4>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-54-1><a id=__codelineno-54-1 name=__codelineno-54-1 href=#__codelineno-54-1></a><span class=n>x</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-55-1><a id=__codelineno-55-1 name=__codelineno-55-1 href=#__codelineno-55-1></a>tensor(66.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-56-1><a id=__codelineno-56-1 name=__codelineno-56-1 href=#__codelineno-56-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-57-1><a id=__codelineno-57-1 name=__codelineno-57-1 href=#__codelineno-57-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=66.0&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=213-broadcasting-mechanism>2.1.3 Broadcasting Mechanism<a class=headerlink href=#213-broadcasting-mechanism title="Permanent link">&para;</a></h3> <p>In the above section, we saw how to perform element-wise operations on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still perform element-wise operations by invoking the broadcasting mechanism. This mechanism works in the following way: First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape. Second, carry out the element-wise operations on the resulting arrays.</p> <p>In most cases, we broadcast along an axis where an array initially only has length 1, such as in the following example.</p> <div class="tabbed-set tabbed-alternate" data-tabs=16:2><input checked=checked id=213-broadcasting-mechanism-pytorch name=__tabbed_16 type=radio><input id=213-broadcasting-mechanism-tensorflow name=__tabbed_16 type=radio><div class=tabbed-labels><label for=213-broadcasting-mechanism-pytorch>PYTORCH</label><label for=213-broadcasting-mechanism-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-58-1><a id=__codelineno-58-1 name=__codelineno-58-1 href=#__codelineno-58-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span><span id=__span-58-2><a id=__codelineno-58-2 name=__codelineno-58-2 href=#__codelineno-58-2></a><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span><span id=__span-58-3><a id=__codelineno-58-3 name=__codelineno-58-3 href=#__codelineno-58-3></a><span class=n>a</span><span class=p>,</span> <span class=n>b</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-59-1><a id=__codelineno-59-1 name=__codelineno-59-1 href=#__codelineno-59-1></a>(tensor([[0],
</span><span id=__span-59-2><a id=__codelineno-59-2 name=__codelineno-59-2 href=#__codelineno-59-2></a>         [1],
</span><span id=__span-59-3><a id=__codelineno-59-3 name=__codelineno-59-3 href=#__codelineno-59-3></a>         [2]]),
</span><span id=__span-59-4><a id=__codelineno-59-4 name=__codelineno-59-4 href=#__codelineno-59-4></a> tensor([[0, 1]]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-60-1><a id=__codelineno-60-1 name=__codelineno-60-1 href=#__codelineno-60-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>3</span><span class=p>),</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span><span id=__span-60-2><a id=__codelineno-60-2 name=__codelineno-60-2 href=#__codelineno-60-2></a><span class=n>b</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>2</span><span class=p>),</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span><span id=__span-60-3><a id=__codelineno-60-3 name=__codelineno-60-3 href=#__codelineno-60-3></a><span class=n>a</span><span class=p>,</span> <span class=n>b</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-61-1><a id=__codelineno-61-1 name=__codelineno-61-1 href=#__codelineno-61-1></a>(&lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=
</span><span id=__span-61-2><a id=__codelineno-61-2 name=__codelineno-61-2 href=#__codelineno-61-2></a>array([[0],
</span><span id=__span-61-3><a id=__codelineno-61-3 name=__codelineno-61-3 href=#__codelineno-61-3></a>       [1],
</span><span id=__span-61-4><a id=__codelineno-61-4 name=__codelineno-61-4 href=#__codelineno-61-4></a>       [2]], dtype=int32)&gt;,
</span><span id=__span-61-5><a id=__codelineno-61-5 name=__codelineno-61-5 href=#__codelineno-61-5></a> &lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)&gt;)
</span></code></pre></div> </div> </div> </div> <p>Since <code>a</code> and <code>b</code> are <code>(3 √ó 1)</code> and <code>(1 √ó 2)</code> matrices respectively, their shapes do not match up if we want to add them. We broadcast the entries of both matrices into a larger <code>(3 √ó 2)</code> matrix as follows: for matrix <code>a</code> it replicates the columns and for matrix <code>b</code> it replicates the rows before adding up both element-wise.</p> <div class="tabbed-set tabbed-alternate" data-tabs=17:2><input checked=checked id=213-broadcasting-mechanism-pytorch_1 name=__tabbed_17 type=radio><input id=213-broadcasting-mechanism-tensorflow_1 name=__tabbed_17 type=radio><div class=tabbed-labels><label for=213-broadcasting-mechanism-pytorch_1>PYTORCH</label><label for=213-broadcasting-mechanism-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-62-1><a id=__codelineno-62-1 name=__codelineno-62-1 href=#__codelineno-62-1></a><span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-63-1><a id=__codelineno-63-1 name=__codelineno-63-1 href=#__codelineno-63-1></a>tensor([[0, 1],
</span><span id=__span-63-2><a id=__codelineno-63-2 name=__codelineno-63-2 href=#__codelineno-63-2></a>        [1, 2],
</span><span id=__span-63-3><a id=__codelineno-63-3 name=__codelineno-63-3 href=#__codelineno-63-3></a>        [2, 3]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-64-1><a id=__codelineno-64-1 name=__codelineno-64-1 href=#__codelineno-64-1></a><span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-65-1><a id=__codelineno-65-1 name=__codelineno-65-1 href=#__codelineno-65-1></a>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
</span><span id=__span-65-2><a id=__codelineno-65-2 name=__codelineno-65-2 href=#__codelineno-65-2></a>array([[0, 1],
</span><span id=__span-65-3><a id=__codelineno-65-3 name=__codelineno-65-3 href=#__codelineno-65-3></a>       [1, 2],
</span><span id=__span-65-4><a id=__codelineno-65-4 name=__codelineno-65-4 href=#__codelineno-65-4></a>       [2, 3]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=214-indexing-and-slicing>2.1.4 Indexing and Slicing<a class=headerlink href=#214-indexing-and-slicing title="Permanent link">&para;</a></h3> <p>Just as in any other Python array, elements in a tensor can be accessed by index. As in any Python array, the first element has index 0 and ranges are specified to include the first but not the last element. Negative indices are commonly used to access elements relative to the end of the array. As usual, we can modify the contents of an array by specifying an index and assigning a new value.</p> <div class="tabbed-set tabbed-alternate" data-tabs=18:2><input checked=checked id=214-indexing-and-slicing-pytorch name=__tabbed_18 type=radio><input id=214-indexing-and-slicing-tensorflow name=__tabbed_18 type=radio><div class=tabbed-labels><label for=214-indexing-and-slicing-pytorch>PYTORCH</label><label for=214-indexing-and-slicing-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-66-1><a id=__codelineno-66-1 name=__codelineno-66-1 href=#__codelineno-66-1></a><span class=n>x</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-67-1><a id=__codelineno-67-1 name=__codelineno-67-1 href=#__codelineno-67-1></a>tensor([ 8.,  9., 10., 11.])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-68-1><a id=__codelineno-68-1 name=__codelineno-68-1 href=#__codelineno-68-1></a><span class=n>x</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-69-1><a id=__codelineno-69-1 name=__codelineno-69-1 href=#__codelineno-69-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>Beyond reading, writing, and slicing by elements, we can also access ranges of elements. For instance, we can slice <code>x</code> to obtain the first two rows.</p> <div class="tabbed-set tabbed-alternate" data-tabs=19:2><input checked=checked id=214-indexing-and-slicing-pytorch_1 name=__tabbed_19 type=radio><input id=214-indexing-and-slicing-tensorflow_1 name=__tabbed_19 type=radio><div class=tabbed-labels><label for=214-indexing-and-slicing-pytorch_1>PYTORCH</label><label for=214-indexing-and-slicing-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-70-1><a id=__codelineno-70-1 name=__codelineno-70-1 href=#__codelineno-70-1></a><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-71-1><a id=__codelineno-71-1 name=__codelineno-71-1 href=#__codelineno-71-1></a>tensor([[ 4.,  5.,  6.,  7.],
</span><span id=__span-71-2><a id=__codelineno-71-2 name=__codelineno-71-2 href=#__codelineno-71-2></a>        [ 8.,  9., 10., 11.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-72-1><a id=__codelineno-72-1 name=__codelineno-72-1 href=#__codelineno-72-1></a><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-73-1><a id=__codelineno-73-1 name=__codelineno-73-1 href=#__codelineno-73-1></a>&lt;tf.Tensor: shape=(2, 4), dtype=float32, numpy=
</span><span id=__span-73-2><a id=__codelineno-73-2 name=__codelineno-73-2 href=#__codelineno-73-2></a>array([[ 4.,  5.,  6.,  7.],
</span><span id=__span-73-3><a id=__codelineno-73-3 name=__codelineno-73-3 href=#__codelineno-73-3></a>       [ 8.,  9., 10., 11.]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>Besides reading, writing, and slicing single elements or ranges of elements, we can also assign multiple elements with the same value. For instance, <code>[0, 1]</code> is the index for the first and second elements. Rather than assigning them one by one, we can assign them at the same time.</p> <div class="tabbed-set tabbed-alternate" data-tabs=20:2><input checked=checked id=214-indexing-and-slicing-pytorch_2 name=__tabbed_20 type=radio><input id=214-indexing-and-slicing-tensorflow_2 name=__tabbed_20 type=radio><div class=tabbed-labels><label for=214-indexing-and-slicing-pytorch_2>PYTORCH</label><label for=214-indexing-and-slicing-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-74-1><a id=__codelineno-74-1 name=__codelineno-74-1 href=#__codelineno-74-1></a><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=mi>9</span>
</span><span id=__span-74-2><a id=__codelineno-74-2 name=__codelineno-74-2 href=#__codelineno-74-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-75-1><a id=__codelineno-75-1 name=__codelineno-75-1 href=#__codelineno-75-1></a>tensor([[ 0.,  1.,  2.,  3.],
</span><span id=__span-75-2><a id=__codelineno-75-2 name=__codelineno-75-2 href=#__codelineno-75-2></a>        [ 4.,  5.,  9.,  7.],
</span><span id=__span-75-3><a id=__codelineno-75-3 name=__codelineno-75-3 href=#__codelineno-75-3></a>        [ 8.,  9., 10., 11.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-76-1><a id=__codelineno-76-1 name=__codelineno-76-1 href=#__codelineno-76-1></a><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=mi>9</span><span class=p>)</span>
</span><span id=__span-76-2><a id=__codelineno-76-2 name=__codelineno-76-2 href=#__codelineno-76-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-77-1><a id=__codelineno-77-1 name=__codelineno-77-1 href=#__codelineno-77-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
</span><span id=__span-77-2><a id=__codelineno-77-2 name=__codelineno-77-2 href=#__codelineno-77-2></a>array([[ 0.,  1.,  2.,  3.],
</span><span id=__span-77-3><a id=__codelineno-77-3 name=__codelineno-77-3 href=#__codelineno-77-3></a>       [ 4.,  5.,  9.,  7.],
</span><span id=__span-77-4><a id=__codelineno-77-4 name=__codelineno-77-4 href=#__codelineno-77-4></a>       [ 8.,  9., 10., 11.]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>Similarly, if we want to assign multiple elements the same value, we simply index all of them and then assign them the value. Below, we assign each element in the second and third rows to 12.</p> <div class="tabbed-set tabbed-alternate" data-tabs=21:2><input checked=checked id=214-indexing-and-slicing-pytorch_3 name=__tabbed_21 type=radio><input id=214-indexing-and-slicing-tensorflow_3 name=__tabbed_21 type=radio><div class=tabbed-labels><label for=214-indexing-and-slicing-pytorch_3>PYTORCH</label><label for=214-indexing-and-slicing-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-78-1><a id=__codelineno-78-1 name=__codelineno-78-1 href=#__codelineno-78-1></a><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=mi>3</span><span class=p>,</span> <span class=p>:]</span> <span class=o>=</span> <span class=mi>12</span>
</span><span id=__span-78-2><a id=__codelineno-78-2 name=__codelineno-78-2 href=#__codelineno-78-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-79-1><a id=__codelineno-79-1 name=__codelineno-79-1 href=#__codelineno-79-1></a>tensor([[ 0.,  1.,  2.,  3.],
</span><span id=__span-79-2><a id=__codelineno-79-2 name=__codelineno-79-2 href=#__codelineno-79-2></a>        [12., 12., 12., 12.],
</span><span id=__span-79-3><a id=__codelineno-79-3 name=__codelineno-79-3 href=#__codelineno-79-3></a>        [12., 12., 12., 12.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-80-1><a id=__codelineno-80-1 name=__codelineno-80-1 href=#__codelineno-80-1></a><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=mi>3</span><span class=p>,</span> <span class=p>:]</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>
</span><span id=__span-80-2><a id=__codelineno-80-2 name=__codelineno-80-2 href=#__codelineno-80-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-81-1><a id=__codelineno-81-1 name=__codelineno-81-1 href=#__codelineno-81-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
</span><span id=__span-81-2><a id=__codelineno-81-2 name=__codelineno-81-2 href=#__codelineno-81-2></a>array([[ 0.,  1.,  2.,  3.],
</span><span id=__span-81-3><a id=__codelineno-81-3 name=__codelineno-81-3 href=#__codelineno-81-3></a>       [12., 12., 12., 12.],
</span><span id=__span-81-4><a id=__codelineno-81-4 name=__codelineno-81-4 href=#__codelineno-81-4></a>       [12., 12., 12., 12.]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=215-saving-memory>2.1.5 Saving Memory<a class=headerlink href=#215-saving-memory title="Permanent link">&para;</a></h3> <p>In some cases, we might want to perform an operation on the same array several times. If we are not careful, we will allocate new memory to store the results of each operation. For example, if we write <code>y = x + y</code>, we will dereference the tensor that <code>y</code> used to point to and instead point <code>y</code> at the newly allocated memory. In the following example, we demonstrate this with Python's <code>id</code> function, which gives us the exact address of the referenced object in memory. After running <code>y = y + x</code>, we will find that <code>id(y)</code> points to a different location. That is because Python first evaluates <code>y + x</code>, allocating new memory for the result and then subsequently redirects <code>y</code> to point at this new location in memory.</p> <div class="tabbed-set tabbed-alternate" data-tabs=22:2><input checked=checked id=215-saving-memory-pytorch name=__tabbed_22 type=radio><input id=215-saving-memory-tensorflow name=__tabbed_22 type=radio><div class=tabbed-labels><label for=215-saving-memory-pytorch>PYTORCH</label><label for=215-saving-memory-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-82-1><a id=__codelineno-82-1 name=__codelineno-82-1 href=#__codelineno-82-1></a><span class=n>before</span> <span class=o>=</span> <span class=nb>id</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span><span id=__span-82-2><a id=__codelineno-82-2 name=__codelineno-82-2 href=#__codelineno-82-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>y</span> <span class=o>+</span> <span class=n>x</span>
</span><span id=__span-82-3><a id=__codelineno-82-3 name=__codelineno-82-3 href=#__codelineno-82-3></a><span class=nb>id</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>==</span> <span class=n>before</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-83-1><a id=__codelineno-83-1 name=__codelineno-83-1 href=#__codelineno-83-1></a>False
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-84-1><a id=__codelineno-84-1 name=__codelineno-84-1 href=#__codelineno-84-1></a><span class=n>before</span> <span class=o>=</span> <span class=nb>id</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span><span id=__span-84-2><a id=__codelineno-84-2 name=__codelineno-84-2 href=#__codelineno-84-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>y</span> <span class=o>+</span> <span class=n>x</span>
</span><span id=__span-84-3><a id=__codelineno-84-3 name=__codelineno-84-3 href=#__codelineno-84-3></a><span class=nb>id</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>==</span> <span class=n>before</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-85-1><a id=__codelineno-85-1 name=__codelineno-85-1 href=#__codelineno-85-1></a>False
</span></code></pre></div> </div> </div> </div> <p>This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters.</p> <p>Fortunately, performing in-place operations in MXNet is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., <code>y[:] = &lt;expression&gt;</code>. To illustrate the behavior, we first clone the shape of another variable, allocating new memory in the process, and then perform an operation, writing the result to the newly allocated space. In this case, <code>id(y)</code> still points to the same location because we allocated no new memory for the operation.</p> <div class="tabbed-set tabbed-alternate" data-tabs=23:2><input checked=checked id=215-saving-memory-pytorch_1 name=__tabbed_23 type=radio><input id=215-saving-memory-tensorflow_1 name=__tabbed_23 type=radio><div class=tabbed-labels><label for=215-saving-memory-pytorch_1>PYTORCH</label><label for=215-saving-memory-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-86-1><a id=__codelineno-86-1 name=__codelineno-86-1 href=#__codelineno-86-1></a><span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span><span id=__span-86-2><a id=__codelineno-86-2 name=__codelineno-86-2 href=#__codelineno-86-2></a><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;id(z):&#39;</span><span class=p>,</span> <span class=nb>id</span><span class=p>(</span><span class=n>z</span><span class=p>))</span>
</span><span id=__span-86-3><a id=__codelineno-86-3 name=__codelineno-86-3 href=#__codelineno-86-3></a><span class=n>z</span><span class=p>[:]</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span>
</span><span id=__span-86-4><a id=__codelineno-86-4 name=__codelineno-86-4 href=#__codelineno-86-4></a><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;id(z):&#39;</span><span class=p>,</span> <span class=nb>id</span><span class=p>(</span><span class=n>z</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-87-1><a id=__codelineno-87-1 name=__codelineno-87-1 href=#__codelineno-87-1></a>id(z): 140539816595456
</span><span id=__span-87-2><a id=__codelineno-87-2 name=__codelineno-87-2 href=#__codelineno-87-2></a>id(z): 140539816595456
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-88-1><a id=__codelineno-88-1 name=__codelineno-88-1 href=#__codelineno-88-1></a><span class=n>z</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span><span id=__span-88-2><a id=__codelineno-88-2 name=__codelineno-88-2 href=#__codelineno-88-2></a><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;id(z):&#39;</span><span class=p>,</span> <span class=nb>id</span><span class=p>(</span><span class=n>z</span><span class=p>))</span>
</span><span id=__span-88-3><a id=__codelineno-88-3 name=__codelineno-88-3 href=#__codelineno-88-3></a><span class=n>z</span><span class=p>[:]</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span>
</span><span id=__span-88-4><a id=__codelineno-88-4 name=__codelineno-88-4 href=#__codelineno-88-4></a><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;id(z):&#39;</span><span class=p>,</span> <span class=nb>id</span><span class=p>(</span><span class=n>z</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-89-1><a id=__codelineno-89-1 name=__codelineno-89-1 href=#__codelineno-89-1></a>id(z): 140539816595456
</span><span id=__span-89-2><a id=__codelineno-89-2 name=__codelineno-89-2 href=#__codelineno-89-2></a>id(z): 140539816595456
</span></code></pre></div> </div> </div> </div> <h3 id=216-conversion-to-other-python-objects>2.1.6 Conversion to Other Python Objects<a class=headerlink href=#216-conversion-to-other-python-objects title="Permanent link">&para;</a></h3> <p>Converting to a NumPy tensor, or vice versa, is easy. The converted result does not share memory. This minor inconvenience is actually quite important: when you perform operations on the CPU or on GPUs, you do not want to halt computation, waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory.</p> <div class="tabbed-set tabbed-alternate" data-tabs=24:2><input checked=checked id=216-conversion-to-other-python-objects-pytorch name=__tabbed_24 type=radio><input id=216-conversion-to-other-python-objects-tensorflow name=__tabbed_24 type=radio><div class=tabbed-labels><label for=216-conversion-to-other-python-objects-pytorch>PYTORCH</label><label for=216-conversion-to-other-python-objects-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-90-1><a id=__codelineno-90-1 name=__codelineno-90-1 href=#__codelineno-90-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span><span id=__span-90-2><a id=__codelineno-90-2 name=__codelineno-90-2 href=#__codelineno-90-2></a><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span><span id=__span-90-3><a id=__codelineno-90-3 name=__codelineno-90-3 href=#__codelineno-90-3></a><span class=nb>type</span><span class=p>(</span><span class=n>a</span><span class=p>),</span> <span class=nb>type</span><span class=p>(</span><span class=n>b</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-91-1><a id=__codelineno-91-1 name=__codelineno-91-1 href=#__codelineno-91-1></a>(numpy.ndarray, torch.Tensor)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-92-1><a id=__codelineno-92-1 name=__codelineno-92-1 href=#__codelineno-92-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span><span id=__span-92-2><a id=__codelineno-92-2 name=__codelineno-92-2 href=#__codelineno-92-2></a><span class=n>b</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>convert_to_tensor</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span><span id=__span-92-3><a id=__codelineno-92-3 name=__codelineno-92-3 href=#__codelineno-92-3></a><span class=nb>type</span><span class=p>(</span><span class=n>a</span><span class=p>),</span> <span class=nb>type</span><span class=p>(</span><span class=n>b</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-93-1><a id=__codelineno-93-1 name=__codelineno-93-1 href=#__codelineno-93-1></a>(numpy.ndarray, tensorflow.python.framework.ops.EagerTensor)
</span></code></pre></div> </div> </div> </div> <p>To convert a size-1 tensor to a Python scalar, we can invoke the <code>item</code> function or Python's built-in functions.</p> <div class="tabbed-set tabbed-alternate" data-tabs=25:2><input checked=checked id=216-conversion-to-other-python-objects-pytorch_1 name=__tabbed_25 type=radio><input id=216-conversion-to-other-python-objects-tensorflow_1 name=__tabbed_25 type=radio><div class=tabbed-labels><label for=216-conversion-to-other-python-objects-pytorch_1>PYTORCH</label><label for=216-conversion-to-other-python-objects-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-94-1><a id=__codelineno-94-1 name=__codelineno-94-1 href=#__codelineno-94-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>3.5</span><span class=p>])</span>
</span><span id=__span-94-2><a id=__codelineno-94-2 name=__codelineno-94-2 href=#__codelineno-94-2></a><span class=n>a</span><span class=p>,</span> <span class=n>a</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=nb>float</span><span class=p>(</span><span class=n>a</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-95-1><a id=__codelineno-95-1 name=__codelineno-95-1 href=#__codelineno-95-1></a>(tensor([3.5000]), 3.5, 3.5, 3)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-96-1><a id=__codelineno-96-1 name=__codelineno-96-1 href=#__codelineno-96-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>3.5</span><span class=p>])</span>
</span><span id=__span-96-2><a id=__codelineno-96-2 name=__codelineno-96-2 href=#__codelineno-96-2></a><span class=n>a</span><span class=p>,</span> <span class=n>a</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=nb>float</span><span class=p>(</span><span class=n>a</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-97-1><a id=__codelineno-97-1 name=__codelineno-97-1 href=#__codelineno-97-1></a>(&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.5], dtype=float32)&gt;,
</span><span id=__span-97-2><a id=__codelineno-97-2 name=__codelineno-97-2 href=#__codelineno-97-2></a> array([3.5], dtype=float32),
</span><span id=__span-97-3><a id=__codelineno-97-3 name=__codelineno-97-3 href=#__codelineno-97-3></a> 3.5,
</span><span id=__span-97-4><a id=__codelineno-97-4 name=__codelineno-97-4 href=#__codelineno-97-4></a> 3)
</span></code></pre></div> </div> </div> </div> <h3 id=217-summary>2.1.7 Summary<a class=headerlink href=#217-summary title="Permanent link">&para;</a></h3> <p>The tensor class is the main interface for storing and manipulating data in deep learning libraries. Tensors provide a variety of functionalities including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects.</p> <h2 id=22-data-preprocessing>2.2 Data Preprocessing<a class=headerlink href=#22-data-preprocessing title="Permanent link">&para;</a></h2> <p>So far, we have been working with synthetic data that arrived in ready-made tensors. However, to apply deep learning in the wild we must extract messy data stored in arbitrary formats, and preprocess it to suit our needs. Fortunately, the pandas library can do much of the heavy lifting. This section, while no substitute for a proper pandas tutorial, will give you a crash course on some of the most common routines.\</p> <h3 id=221-reading-the-dataset>2.2.1 Reading the Dataset<a class=headerlink href=#221-reading-the-dataset title="Permanent link">&para;</a></h3> <p>Comma-separated values (CSV) files are ubiquitous for the storing of tabular (spreadsheet-like) data. In them, each line corresponds to one record and consists of several (comma-separated) fields, e.g., ‚ÄúAlbert Einstein,March 14 1879,Ulm,Federal polytechnic school,field of gravitational physics‚Äù. To demonstrate how to load CSV files with pandas, we create a CSV file below ../data/house_tiny.csv. This file represents a dataset of homes, where each row corresponds to a distinct home and the columns correspond to the number of rooms (NumRooms), the roof type (RoofType), and the price (Price).</p> <div class="tabbed-set tabbed-alternate" data-tabs=26:1><input checked=checked id=221-reading-the-dataset-pytorch name=__tabbed_26 type=radio><div class=tabbed-labels><label for=221-reading-the-dataset-pytorch>PYTORCH</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-98-1><a id=__codelineno-98-1 name=__codelineno-98-1 href=#__codelineno-98-1></a><span class=kn>import</span> <span class=nn>os</span>
</span><span id=__span-98-2><a id=__codelineno-98-2 name=__codelineno-98-2 href=#__codelineno-98-2></a>
</span><span id=__span-98-3><a id=__codelineno-98-3 name=__codelineno-98-3 href=#__codelineno-98-3></a><span class=n>os</span><span class=o>.</span><span class=n>makedirs</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s1>&#39;..&#39;</span><span class=p>,</span> <span class=s1>&#39;data&#39;</span><span class=p>),</span> <span class=n>exist_ok</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-98-4><a id=__codelineno-98-4 name=__codelineno-98-4 href=#__codelineno-98-4></a><span class=n>data_file</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s1>&#39;..&#39;</span><span class=p>,</span> <span class=s1>&#39;data&#39;</span><span class=p>,</span> <span class=s1>&#39;house_tiny.csv&#39;</span><span class=p>)</span>
</span><span id=__span-98-5><a id=__codelineno-98-5 name=__codelineno-98-5 href=#__codelineno-98-5></a><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>data_file</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span><span id=__span-98-6><a id=__codelineno-98-6 name=__codelineno-98-6 href=#__codelineno-98-6></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;NumRooms,Alley,Price</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>  <span class=c1># Column names</span>
</span><span id=__span-98-7><a id=__codelineno-98-7 name=__codelineno-98-7 href=#__codelineno-98-7></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;NA,Pave,127500</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>  <span class=c1># Each row represents a data example</span>
</span><span id=__span-98-8><a id=__codelineno-98-8 name=__codelineno-98-8 href=#__codelineno-98-8></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;2,NA,106000</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span><span id=__span-98-9><a id=__codelineno-98-9 name=__codelineno-98-9 href=#__codelineno-98-9></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;4,NA,178100</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span><span id=__span-98-10><a id=__codelineno-98-10 name=__codelineno-98-10 href=#__codelineno-98-10></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;NA,NA,140000</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span></code></pre></div> </div> </div> </div> <p>Now let's import pandas and load the dataset with <code>read_csv</code>. The argument <code>na_values</code> means ‚Äúnot available values‚Äù.</p> <div class="tabbed-set tabbed-alternate" data-tabs=27:2><input checked=checked id=221-reading-the-dataset-pytorch_1 name=__tabbed_27 type=radio><input id=221-reading-the-dataset-tensorflow name=__tabbed_27 type=radio><div class=tabbed-labels><label for=221-reading-the-dataset-pytorch_1>PYTORCH</label><label for=221-reading-the-dataset-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-99-1><a id=__codelineno-99-1 name=__codelineno-99-1 href=#__codelineno-99-1></a><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span><span id=__span-99-2><a id=__codelineno-99-2 name=__codelineno-99-2 href=#__codelineno-99-2></a><span class=n>data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>data_file</span><span class=p>)</span>
</span><span id=__span-99-3><a id=__codelineno-99-3 name=__codelineno-99-3 href=#__codelineno-99-3></a><span class=nb>print</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-100-1><a id=__codelineno-100-1 name=__codelineno-100-1 href=#__codelineno-100-1></a>   NumRooms Alley   Price
</span><span id=__span-100-2><a id=__codelineno-100-2 name=__codelineno-100-2 href=#__codelineno-100-2></a>0       NaN  Pave  127500
</span><span id=__span-100-3><a id=__codelineno-100-3 name=__codelineno-100-3 href=#__codelineno-100-3></a>1       2.0   NaN  106000
</span><span id=__span-100-4><a id=__codelineno-100-4 name=__codelineno-100-4 href=#__codelineno-100-4></a>2       4.0   NaN  178100
</span><span id=__span-100-5><a id=__codelineno-100-5 name=__codelineno-100-5 href=#__codelineno-100-5></a>3       NaN   NaN  140000
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-101-1><a id=__codelineno-101-1 name=__codelineno-101-1 href=#__codelineno-101-1></a><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span><span id=__span-101-2><a id=__codelineno-101-2 name=__codelineno-101-2 href=#__codelineno-101-2></a><span class=n>data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>data_file</span><span class=p>)</span>
</span><span id=__span-101-3><a id=__codelineno-101-3 name=__codelineno-101-3 href=#__codelineno-101-3></a><span class=nb>print</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-102-1><a id=__codelineno-102-1 name=__codelineno-102-1 href=#__codelineno-102-1></a>   NumRooms Alley   Price
</span><span id=__span-102-2><a id=__codelineno-102-2 name=__codelineno-102-2 href=#__codelineno-102-2></a>0       NaN  Pave  127500
</span><span id=__span-102-3><a id=__codelineno-102-3 name=__codelineno-102-3 href=#__codelineno-102-3></a>1       2.0   NaN  106000
</span><span id=__span-102-4><a id=__codelineno-102-4 name=__codelineno-102-4 href=#__codelineno-102-4></a>2       4.0   NaN  178100
</span><span id=__span-102-5><a id=__codelineno-102-5 name=__codelineno-102-5 href=#__codelineno-102-5></a>3       NaN   NaN  140000
</span></code></pre></div> </div> </div> </div> <h3 id=222-data-preparation>2.2.2 Data Preparation<a class=headerlink href=#222-data-preparation title="Permanent link">&para;</a></h3> <p>In supervised learning, we train models to predict a designated target value, given some set of input values. Our first step in processing the dataset is to separate out columns corresponding to input versus target values. We can select columns either by name or via integer-location based indexing (iloc).</p> <p>You might have noticed that pandas replaced all CSV entries with value NA with a special NaN (not a number) value. This can also happen whenever an entry is empty, e.g., ‚Äú3,,,270000‚Äù. These are called missing values and they are the ‚Äúbed bugs‚Äù of data science, a persistent menace that you will confront throughout your career. Depending upon the context, missing values might be handled either via imputation or deletion. Imputation replaces missing values with estimates of their values while deletion simply discards either those rows or those columns that contain missing values.</p> <p>Here are some common imputation heuristics. For categorical input fields, we can treat NaN as a category. Since the RoofType column takes values Slate and NaN, pandas can convert this column into two columns RoofType_Slate and RoofType_nan. A row whose roof type is Slate will set values of RoofType_Slate and RoofType_nan to 1 and 0, respectively. The converse holds for a row with a missing RoofType value.</p> <div class="tabbed-set tabbed-alternate" data-tabs=28:2><input checked=checked id=222-data-preparation-pytorch name=__tabbed_28 type=radio><input id=222-data-preparation-tensorflow name=__tabbed_28 type=radio><div class=tabbed-labels><label for=222-data-preparation-pytorch>PYTORCH</label><label for=222-data-preparation-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-103-1><a id=__codelineno-103-1 name=__codelineno-103-1 href=#__codelineno-103-1></a><span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>:</span><span class=mi>2</span><span class=p>],</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>
</span><span id=__span-103-2><a id=__codelineno-103-2 name=__codelineno-103-2 href=#__codelineno-103-2></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>dummy_na</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-103-3><a id=__codelineno-103-3 name=__codelineno-103-3 href=#__codelineno-103-3></a><span class=nb>print</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-104-1><a id=__codelineno-104-1 name=__codelineno-104-1 href=#__codelineno-104-1></a>   NumRooms  RoofType_Slate  RoofType_nan
</span><span id=__span-104-2><a id=__codelineno-104-2 name=__codelineno-104-2 href=#__codelineno-104-2></a>0       NaN           False          True
</span><span id=__span-104-3><a id=__codelineno-104-3 name=__codelineno-104-3 href=#__codelineno-104-3></a>1       2.0           False          True
</span><span id=__span-104-4><a id=__codelineno-104-4 name=__codelineno-104-4 href=#__codelineno-104-4></a>2       4.0            True         False
</span><span id=__span-104-5><a id=__codelineno-104-5 name=__codelineno-104-5 href=#__codelineno-104-5></a>3       NaN           False          True
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-105-1><a id=__codelineno-105-1 name=__codelineno-105-1 href=#__codelineno-105-1></a><span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>:</span><span class=mi>2</span><span class=p>],</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>
</span><span id=__span-105-2><a id=__codelineno-105-2 name=__codelineno-105-2 href=#__codelineno-105-2></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>dummy_na</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-105-3><a id=__codelineno-105-3 name=__codelineno-105-3 href=#__codelineno-105-3></a><span class=nb>print</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-106-1><a id=__codelineno-106-1 name=__codelineno-106-1 href=#__codelineno-106-1></a>   NumRooms  RoofType_Slate  RoofType_nan
</span><span id=__span-106-2><a id=__codelineno-106-2 name=__codelineno-106-2 href=#__codelineno-106-2></a>0       NaN           False          True
</span><span id=__span-106-3><a id=__codelineno-106-3 name=__codelineno-106-3 href=#__codelineno-106-3></a>1       2.0           False          True
</span><span id=__span-106-4><a id=__codelineno-106-4 name=__codelineno-106-4 href=#__codelineno-106-4></a>2       4.0            True         False
</span><span id=__span-106-5><a id=__codelineno-106-5 name=__codelineno-106-5 href=#__codelineno-106-5></a>3       NaN           False          True
</span></code></pre></div> </div> </div> </div> <p>For missing numerical values, one common heuristic is to replace the NaN entries with the mean value of the corresponding column.</p> <div class="tabbed-set tabbed-alternate" data-tabs=29:2><input checked=checked id=222-data-preparation-pytorch_1 name=__tabbed_29 type=radio><input id=222-data-preparation-tensorflow_1 name=__tabbed_29 type=radio><div class=tabbed-labels><label for=222-data-preparation-pytorch_1>PYTORCH</label><label for=222-data-preparation-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-107-1><a id=__codelineno-107-1 name=__codelineno-107-1 href=#__codelineno-107-1></a><span class=c1># Fill missing values with the column mean</span>
</span><span id=__span-107-2><a id=__codelineno-107-2 name=__codelineno-107-2 href=#__codelineno-107-2></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>fillna</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
</span><span id=__span-107-3><a id=__codelineno-107-3 name=__codelineno-107-3 href=#__codelineno-107-3></a><span class=nb>print</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-108-1><a id=__codelineno-108-1 name=__codelineno-108-1 href=#__codelineno-108-1></a>   NumRooms  RoofType_Slate  RoofType_nan
</span><span id=__span-108-2><a id=__codelineno-108-2 name=__codelineno-108-2 href=#__codelineno-108-2></a>0       3.0           False          True
</span><span id=__span-108-3><a id=__codelineno-108-3 name=__codelineno-108-3 href=#__codelineno-108-3></a>1       2.0           False          True
</span><span id=__span-108-4><a id=__codelineno-108-4 name=__codelineno-108-4 href=#__codelineno-108-4></a>2       4.0            True         False
</span><span id=__span-108-5><a id=__codelineno-108-5 name=__codelineno-108-5 href=#__codelineno-108-5></a>3       3.0           False          True
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-109-1><a id=__codelineno-109-1 name=__codelineno-109-1 href=#__codelineno-109-1></a><span class=c1># Fill missing values with the column mean</span>
</span><span id=__span-109-2><a id=__codelineno-109-2 name=__codelineno-109-2 href=#__codelineno-109-2></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>fillna</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
</span><span id=__span-109-3><a id=__codelineno-109-3 name=__codelineno-109-3 href=#__codelineno-109-3></a><span class=nb>print</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-110-1><a id=__codelineno-110-1 name=__codelineno-110-1 href=#__codelineno-110-1></a>   NumRooms  RoofType_Slate  RoofType_nan
</span><span id=__span-110-2><a id=__codelineno-110-2 name=__codelineno-110-2 href=#__codelineno-110-2></a>0       3.0           False          True
</span><span id=__span-110-3><a id=__codelineno-110-3 name=__codelineno-110-3 href=#__codelineno-110-3></a>1       2.0           False          True
</span><span id=__span-110-4><a id=__codelineno-110-4 name=__codelineno-110-4 href=#__codelineno-110-4></a>2       4.0            True         False
</span><span id=__span-110-5><a id=__codelineno-110-5 name=__codelineno-110-5 href=#__codelineno-110-5></a>3       3.0           False          True
</span></code></pre></div> </div> </div> </div> <h3 id=223-conversion-to-the-tensor-format>2.2.3 Conversion to the Tensor Format<a class=headerlink href=#223-conversion-to-the-tensor-format title="Permanent link">&para;</a></h3> <p>Now that all the entries in inputs and targets are numerical, we can load them into a tensor (recall Section 2.1).</p> <div class="tabbed-set tabbed-alternate" data-tabs=30:2><input checked=checked id=223-conversion-to-the-tensor-format-pytorch name=__tabbed_30 type=radio><input id=223-conversion-to-the-tensor-format-tensorflow name=__tabbed_30 type=radio><div class=tabbed-labels><label for=223-conversion-to-the-tensor-format-pytorch>PYTORCH</label><label for=223-conversion-to-the-tensor-format-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-111-1><a id=__codelineno-111-1 name=__codelineno-111-1 href=#__codelineno-111-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-111-2><a id=__codelineno-111-2 name=__codelineno-111-2 href=#__codelineno-111-2></a><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>values</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>targets</span><span class=o>.</span><span class=n>values</span><span class=p>)</span>
</span><span id=__span-111-3><a id=__codelineno-111-3 name=__codelineno-111-3 href=#__codelineno-111-3></a><span class=n>X</span><span class=p>,</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-112-1><a id=__codelineno-112-1 name=__codelineno-112-1 href=#__codelineno-112-1></a>(tensor([[3., 0., 1.],
</span><span id=__span-112-2><a id=__codelineno-112-2 name=__codelineno-112-2 href=#__codelineno-112-2></a>         [2., 0., 1.],
</span><span id=__span-112-3><a id=__codelineno-112-3 name=__codelineno-112-3 href=#__codelineno-112-3></a>         [4., 1., 0.],
</span><span id=__span-112-4><a id=__codelineno-112-4 name=__codelineno-112-4 href=#__codelineno-112-4></a>         [3., 0., 1.]], dtype=torch.float64),
</span><span id=__span-112-5><a id=__codelineno-112-5 name=__codelineno-112-5 href=#__codelineno-112-5></a> tensor([127500, 106000, 178100, 140000]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-113-1><a id=__codelineno-113-1 name=__codelineno-113-1 href=#__codelineno-113-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-113-2><a id=__codelineno-113-2 name=__codelineno-113-2 href=#__codelineno-113-2></a><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>values</span><span class=p>),</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>targets</span><span class=o>.</span><span class=n>values</span><span class=p>)</span>
</span><span id=__span-113-3><a id=__codelineno-113-3 name=__codelineno-113-3 href=#__codelineno-113-3></a><span class=n>X</span><span class=p>,</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-114-1><a id=__codelineno-114-1 name=__codelineno-114-1 href=#__codelineno-114-1></a>(&lt;tf.Tensor: shape=(4, 3), dtype=float64, numpy=
</span><span id=__span-114-2><a id=__codelineno-114-2 name=__codelineno-114-2 href=#__codelineno-114-2></a>array([[3., 0., 1.],
</span><span id=__span-114-3><a id=__codelineno-114-3 name=__codelineno-114-3 href=#__codelineno-114-3></a>       [2., 0., 1.],
</span><span id=__span-114-4><a id=__codelineno-114-4 name=__codelineno-114-4 href=#__codelineno-114-4></a>       [4., 1., 0.],
</span><span id=__span-114-5><a id=__codelineno-114-5 name=__codelineno-114-5 href=#__codelineno-114-5></a>       [3., 0., 1.]])&gt;,
</span><span id=__span-114-6><a id=__codelineno-114-6 name=__codelineno-114-6 href=#__codelineno-114-6></a> &lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([127500, 106000, 178100, 140000])&gt;)
</span></code></pre></div> </div> </div> </div> <h3 id=224-summary>2.2.4 Summary<a class=headerlink href=#224-summary title="Permanent link">&para;</a></h3> <p>You now know how to partition data columns, impute missing variables, and load pandas data into tensors. In Section 5.7, you will pick up some more data processing skills. While this crash course kept things simple, data processing can get hairy. For example, rather than arriving in a single CSV file, our dataset might be spread across multiple files extracted from a relational database. For instance, in an e-commerce application, customer addresses might live in one table and purchase data in another. Moreover, practitioners face myriad data types beyond categorical and numeric, for example, text strings, images, audio data, and point clouds. Oftentimes, advanced tools and efficient algorithms are required in order to prevent data processing from becoming the biggest bottleneck in the machine learning pipeline. These problems will arise when we get to computer vision and natural language processing. Finally, we must pay attention to data quality. Real-world datasets are often plagued by outliers, faulty measurements from sensors, and recording errors, which must be addressed before feeding the data into any model. Data visualization tools such as seaborn, Bokeh, or matplotlib can help you to manually inspect the data and develop intuitions about the type of problems you may need to address.</p> <h2 id=23-linear-algebra>2.3 Linear Algebra<a class=headerlink href=#23-linear-algebra title="Permanent link">&para;</a></h2> <h3 id=231-scalars>2.3.1 Scalars<a class=headerlink href=#231-scalars title="Permanent link">&para;</a></h3> <p>If you never studied linear algebra or machine learning, then you might be surprised to see so many boldfaced letters. These are not typos! We use boldfaced lowercase letters (e.g., <span class=arithmatex>\(\mathbf{x}\)</span>) to refer to vectors and boldfaced uppercase letters (e.g., <span class=arithmatex>\(\mathbf{X}\)</span>) to denote matrices. For scalars (single numbers), we use regular letters (e.g., <span class=arithmatex>\(x\)</span>). We also work with tensors, which are arrays with more than two axes. We use calligraphic font (e.g., <span class=arithmatex>\(\mathcal{X}\)</span>) for tensors.</p> <p>We use <span class=arithmatex>\(\mathbb{R}\)</span> to denote the set containing all scalar real-valued numbers. By convention, we use calligraphic font (e.g., <span class=arithmatex>\(\mathcal{X}\)</span>) for sets. This is because we can think of a scalar as a set containing just one element. By extension, <span class=arithmatex>\(\mathbb{R}^n\)</span> denotes the set of all vectors consisting of <span class=arithmatex>\(n\)</span> real-valued scalars. In machine learning, we typically work with sets of scalars, vectors, matrices, or tensors of various sizes and shapes, so it is important to keep the distinctions among them straight.</p> <p>Scalars are implemented as tensors that contain only one element. Below, we assign two scalars and perform the familiar addition, multiplication, division, and exponentiation operations.</p> <div class="tabbed-set tabbed-alternate" data-tabs=31:2><input checked=checked id=231-scalars-pytorch name=__tabbed_31 type=radio><input id=231-scalars-tensorflow name=__tabbed_31 type=radio><div class=tabbed-labels><label for=231-scalars-pytorch>PYTORCH</label><label for=231-scalars-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-115-1><a id=__codelineno-115-1 name=__codelineno-115-1 href=#__codelineno-115-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-115-2><a id=__codelineno-115-2 name=__codelineno-115-2 href=#__codelineno-115-2></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>3.0</span><span class=p>)</span>
</span><span id=__span-115-3><a id=__codelineno-115-3 name=__codelineno-115-3 href=#__codelineno-115-3></a><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span><span class=p>)</span>
</span><span id=__span-115-4><a id=__codelineno-115-4 name=__codelineno-115-4 href=#__codelineno-115-4></a><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>/</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>**</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-116-1><a id=__codelineno-116-1 name=__codelineno-116-1 href=#__codelineno-116-1></a>(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-117-1><a id=__codelineno-117-1 name=__codelineno-117-1 href=#__codelineno-117-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-117-2><a id=__codelineno-117-2 name=__codelineno-117-2 href=#__codelineno-117-2></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=mf>3.0</span><span class=p>)</span>
</span><span id=__span-117-3><a id=__codelineno-117-3 name=__codelineno-117-3 href=#__codelineno-117-3></a><span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=mf>2.0</span><span class=p>)</span>
</span><span id=__span-117-4><a id=__codelineno-117-4 name=__codelineno-117-4 href=#__codelineno-117-4></a><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>/</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>**</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-118-1><a id=__codelineno-118-1 name=__codelineno-118-1 href=#__codelineno-118-1></a>(&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;,
</span><span id=__span-118-2><a id=__codelineno-118-2 name=__codelineno-118-2 href=#__codelineno-118-2></a> &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;,
</span><span id=__span-118-3><a id=__codelineno-118-3 name=__codelineno-118-3 href=#__codelineno-118-3></a> &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.5&gt;,
</span><span id=__span-118-4><a id=__codelineno-118-4 name=__codelineno-118-4 href=#__codelineno-118-4></a> &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.0&gt;)
</span></code></pre></div> </div> </div> </div> <h3 id=232-vectors>2.3.2 Vectors<a class=headerlink href=#232-vectors title="Permanent link">&para;</a></h3> <p>You can think of a vector as simply a list of scalar values. We call these values the <em>elements</em> (or <em>entries</em>) of the vector. When our vectors represent examples from our dataset, its elements correspond to features of our examples. For example, if we collected data on the height, weight, and age of a group of people, we could represent a single person as a vector <span class=arithmatex>\(\mathbf{x} = [x_1, x_2, x_3]\)</span> with elements corresponding to height, weight, and age, respectively. Throughout this book, we work with column vectors, which are represented as <em>n</em>-dimensional arrays with <em>n</em> rows and 1 column. Each element <span class=arithmatex>\(x_i\)</span> of a vector <span class=arithmatex>\(\mathbf{x}\)</span> is indicated by its index <span class=arithmatex>\(i\)</span>.</p> <p>In math notation, we will usually denote vectors as bold-faced, upright letters (e.g., <span class=arithmatex>\(\mathbf{x}\)</span>). In the source code of the book, we denote them as variables with arrow on top (e.g., <code>x</code>). In either case, we use lowercase letters with either no decoration (e.g., <span class=arithmatex>\(x\)</span>) or arrows (e.g., <span class=arithmatex>\(\vec{x}\)</span>) to refer to their elements.</p> <p>We can create a row vector <code>x</code> containing the first <span class=arithmatex>\(n\)</span> natural numbers with <code>arange</code>. Then we access any element by indexing into the tensor.</p> <div class="tabbed-set tabbed-alternate" data-tabs=32:2><input checked=checked id=232-vectors-pytorch name=__tabbed_32 type=radio><input id=232-vectors-tensorflow name=__tabbed_32 type=radio><div class=tabbed-labels><label for=232-vectors-pytorch>PYTORCH</label><label for=232-vectors-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-119-1><a id=__codelineno-119-1 name=__codelineno-119-1 href=#__codelineno-119-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>4</span><span class=p>)</span>
</span><span id=__span-119-2><a id=__codelineno-119-2 name=__codelineno-119-2 href=#__codelineno-119-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-120-1><a id=__codelineno-120-1 name=__codelineno-120-1 href=#__codelineno-120-1></a>tensor([0, 1, 2, 3])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-121-1><a id=__codelineno-121-1 name=__codelineno-121-1 href=#__codelineno-121-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>4</span><span class=p>)</span>
</span><span id=__span-121-2><a id=__codelineno-121-2 name=__codelineno-121-2 href=#__codelineno-121-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-122-1><a id=__codelineno-122-1 name=__codelineno-122-1 href=#__codelineno-122-1></a>&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>We can refer to any element of a vector by using a subscript. For example, we can refer to the <span class=arithmatex>\(i^\mathrm{th}\)</span> element of <span class=arithmatex>\(\mathbf{x}\)</span> by <span class=arithmatex>\(x_i\)</span>. Note that the element <span class=arithmatex>\(x_i\)</span> is a scalar, so we do not boldface the font when referring to it. Also, note that the element <span class=arithmatex>\(x_i\)</span> is a scalar, so we do not boldface the font when referring to it.</p> <div class="tabbed-set tabbed-alternate" data-tabs=33:2><input checked=checked id=232-vectors-pytorch_1 name=__tabbed_33 type=radio><input id=232-vectors-tensorflow_1 name=__tabbed_33 type=radio><div class=tabbed-labels><label for=232-vectors-pytorch_1>PYTORCH</label><label for=232-vectors-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-123-1><a id=__codelineno-123-1 name=__codelineno-123-1 href=#__codelineno-123-1></a><span class=n>x</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-124-1><a id=__codelineno-124-1 name=__codelineno-124-1 href=#__codelineno-124-1></a>tensor(3)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-125-1><a id=__codelineno-125-1 name=__codelineno-125-1 href=#__codelineno-125-1></a><span class=n>x</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-126-1><a id=__codelineno-126-1 name=__codelineno-126-1 href=#__codelineno-126-1></a>&lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt;
</span></code></pre></div> </div> </div> </div> <p>You can check the length of the vector with <code>len(x)</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=34:2><input checked=checked id=232-vectors-pytorch_2 name=__tabbed_34 type=radio><input id=232-vectors-tensorflow_2 name=__tabbed_34 type=radio><div class=tabbed-labels><label for=232-vectors-pytorch_2>PYTORCH</label><label for=232-vectors-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-127-1><a id=__codelineno-127-1 name=__codelineno-127-1 href=#__codelineno-127-1></a><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-128-1><a id=__codelineno-128-1 name=__codelineno-128-1 href=#__codelineno-128-1></a>4
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-129-1><a id=__codelineno-129-1 name=__codelineno-129-1 href=#__codelineno-129-1></a><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-130-1><a id=__codelineno-130-1 name=__codelineno-130-1 href=#__codelineno-130-1></a>4
</span></code></pre></div> </div> </div> </div> <p>You can check the shape of the vector with <code>x.shape</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=35:2><input checked=checked id=232-vectors-pytorch_3 name=__tabbed_35 type=radio><input id=232-vectors-tensorflow_3 name=__tabbed_35 type=radio><div class=tabbed-labels><label for=232-vectors-pytorch_3>PYTORCH</label><label for=232-vectors-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-131-1><a id=__codelineno-131-1 name=__codelineno-131-1 href=#__codelineno-131-1></a><span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-132-1><a id=__codelineno-132-1 name=__codelineno-132-1 href=#__codelineno-132-1></a>torch.Size([4])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-133-1><a id=__codelineno-133-1 name=__codelineno-133-1 href=#__codelineno-133-1></a><span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-134-1><a id=__codelineno-134-1 name=__codelineno-134-1 href=#__codelineno-134-1></a>TensorShape([4])
</span></code></pre></div> </div> </div> </div> <h3 id=233-matrices>2.3.3 Matrices<a class=headerlink href=#233-matrices title="Permanent link">&para;</a></h3> <p>Just as vectors generalize scalars from order 0 to order 1, matrices generalize vectors from order 1 to order 2. Matrices, which we will typically denote with bold-faced, capital letters (e.g., <span class=arithmatex>\(\mathbf{X}\)</span>), are represented in code as arrays with two axes. Visually, we can illustrate any matrix <span class=arithmatex>\(\mathbf{X} \in \mathbb{R}^{m \times n}\)</span> with a table, where each element <span class=arithmatex>\(x_{ij}\)</span> belongs to the <span class=arithmatex>\(i^{\mathrm{th}}\)</span> row and <span class=arithmatex>\(j^{\mathrm{th}}\)</span> column:</p> <div class=arithmatex>\[\mathbf{X} = \begin{bmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1n} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{m1} &amp; x_{m2} &amp; \cdots &amp; x_{mn} \end{bmatrix}.\]</div> <p>We can create a matrix <code>A</code> with <span class=arithmatex>\(m\)</span> rows and <span class=arithmatex>\(n\)</span> columns with <code>arange</code> and <code>reshape</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=36:2><input checked=checked id=233-matrices-pytorch name=__tabbed_36 type=radio><input id=233-matrices-tensorflow name=__tabbed_36 type=radio><div class=tabbed-labels><label for=233-matrices-pytorch>PYTORCH</label><label for=233-matrices-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-135-1><a id=__codelineno-135-1 name=__codelineno-135-1 href=#__codelineno-135-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-135-2><a id=__codelineno-135-2 name=__codelineno-135-2 href=#__codelineno-135-2></a><span class=n>A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-136-1><a id=__codelineno-136-1 name=__codelineno-136-1 href=#__codelineno-136-1></a>tensor([[ 0,  1,  2,  3],
</span><span id=__span-136-2><a id=__codelineno-136-2 name=__codelineno-136-2 href=#__codelineno-136-2></a>        [ 4,  5,  6,  7],
</span><span id=__span-136-3><a id=__codelineno-136-3 name=__codelineno-136-3 href=#__codelineno-136-3></a>        [ 8,  9, 10, 11],
</span><span id=__span-136-4><a id=__codelineno-136-4 name=__codelineno-136-4 href=#__codelineno-136-4></a>        [12, 13, 14, 15],
</span><span id=__span-136-5><a id=__codelineno-136-5 name=__codelineno-136-5 href=#__codelineno-136-5></a>        [16, 17, 18, 19]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-137-1><a id=__codelineno-137-1 name=__codelineno-137-1 href=#__codelineno-137-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-137-2><a id=__codelineno-137-2 name=__codelineno-137-2 href=#__codelineno-137-2></a><span class=n>A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-138-1><a id=__codelineno-138-1 name=__codelineno-138-1 href=#__codelineno-138-1></a>&lt;tf.Tensor: shape=(5, 4), dtype=int32, numpy=
</span><span id=__span-138-2><a id=__codelineno-138-2 name=__codelineno-138-2 href=#__codelineno-138-2></a>array([[ 0,  1,  2,  3],
</span><span id=__span-138-3><a id=__codelineno-138-3 name=__codelineno-138-3 href=#__codelineno-138-3></a>       [ 4,  5,  6,  7],
</span><span id=__span-138-4><a id=__codelineno-138-4 name=__codelineno-138-4 href=#__codelineno-138-4></a>       [ 8,  9, 10, 11],
</span><span id=__span-138-5><a id=__codelineno-138-5 name=__codelineno-138-5 href=#__codelineno-138-5></a>       [12, 13, 14, 15],
</span><span id=__span-138-6><a id=__codelineno-138-6 name=__codelineno-138-6 href=#__codelineno-138-6></a>       [16, 17, 18, 19]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>We can access the scalar element <span class=arithmatex>\(x_{ij}\)</span> of a matrix <span class=arithmatex>\(\mathbf{X}\)</span> by specifying the indices for the row (<span class=arithmatex>\(i\)</span>) and column (<span class=arithmatex>\(j\)</span>), such as <code>A[i, j]</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=37:2><input checked=checked id=233-matrices-pytorch_1 name=__tabbed_37 type=radio><input id=233-matrices-tensorflow_1 name=__tabbed_37 type=radio><div class=tabbed-labels><label for=233-matrices-pytorch_1>PYTORCH</label><label for=233-matrices-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-139-1><a id=__codelineno-139-1 name=__codelineno-139-1 href=#__codelineno-139-1></a><span class=n>A</span><span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-140-1><a id=__codelineno-140-1 name=__codelineno-140-1 href=#__codelineno-140-1></a>tensor(11)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-141-1><a id=__codelineno-141-1 name=__codelineno-141-1 href=#__codelineno-141-1></a><span class=n>A</span><span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-142-1><a id=__codelineno-142-1 name=__codelineno-142-1 href=#__codelineno-142-1></a>&lt;tf.Tensor: shape=(), dtype=int32, numpy=11&gt;
</span></code></pre></div> </div> </div> </div> <p>We can transpose the matrix through <code>A.T</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=38:2><input checked=checked id=233-matrices-pytorch_2 name=__tabbed_38 type=radio><input id=233-matrices-tensorflow_2 name=__tabbed_38 type=radio><div class=tabbed-labels><label for=233-matrices-pytorch_2>PYTORCH</label><label for=233-matrices-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-143-1><a id=__codelineno-143-1 name=__codelineno-143-1 href=#__codelineno-143-1></a><span class=n>A</span><span class=o>.</span><span class=n>T</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-144-1><a id=__codelineno-144-1 name=__codelineno-144-1 href=#__codelineno-144-1></a>tensor([[ 0,  4,  8, 12, 16],
</span><span id=__span-144-2><a id=__codelineno-144-2 name=__codelineno-144-2 href=#__codelineno-144-2></a>        [ 1,  5,  9, 13, 17],
</span><span id=__span-144-3><a id=__codelineno-144-3 name=__codelineno-144-3 href=#__codelineno-144-3></a>        [ 2,  6, 10, 14, 18],
</span><span id=__span-144-4><a id=__codelineno-144-4 name=__codelineno-144-4 href=#__codelineno-144-4></a>        [ 3,  7, 11, 15, 19]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-145-1><a id=__codelineno-145-1 name=__codelineno-145-1 href=#__codelineno-145-1></a><span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-146-1><a id=__codelineno-146-1 name=__codelineno-146-1 href=#__codelineno-146-1></a>&lt;tf.Tensor: shape=(4, 5), dtype=int32, numpy=
</span><span id=__span-146-2><a id=__codelineno-146-2 name=__codelineno-146-2 href=#__codelineno-146-2></a>array([[ 0,  4,  8, 12, 16],
</span><span id=__span-146-3><a id=__codelineno-146-3 name=__codelineno-146-3 href=#__codelineno-146-3></a>       [ 1,  5,  9, 13, 17],
</span><span id=__span-146-4><a id=__codelineno-146-4 name=__codelineno-146-4 href=#__codelineno-146-4></a>       [ 2,  6, 10, 14, 18],
</span><span id=__span-146-5><a id=__codelineno-146-5 name=__codelineno-146-5 href=#__codelineno-146-5></a>       [ 3,  7, 11, 15, 19]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=234-tensors>2.3.4 Tensors<a class=headerlink href=#234-tensors title="Permanent link">&para;</a></h3> <p>Just as vectors generalize scalars, and matrices generalize vectors, we can build data structures with even more axes. Tensors give us a generic way of describing <span class=arithmatex>\(n\)</span>-dimensional arrays with an arbitrary number of axes. Vectors, for example, are first-order tensors, and matrices are second-order tensors. Tensors will become more important when we start working with images, which arrive as 3D data structures. For now, we will skip the fancy notation and just think of tensors as multidimensional arrays. We can create a tensor <code>X</code> with 3 axes and sizes <span class=arithmatex>\(2 \times 3 \times 4\)</span>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=39:2><input checked=checked id=234-tensors-pytorch name=__tabbed_39 type=radio><input id=234-tensors-tensorflow name=__tabbed_39 type=radio><div class=tabbed-labels><label for=234-tensors-pytorch>PYTORCH</label><label for=234-tensors-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-147-1><a id=__codelineno-147-1 name=__codelineno-147-1 href=#__codelineno-147-1></a><span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>24</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-147-2><a id=__codelineno-147-2 name=__codelineno-147-2 href=#__codelineno-147-2></a><span class=n>X</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-148-1><a id=__codelineno-148-1 name=__codelineno-148-1 href=#__codelineno-148-1></a>tensor([[[ 0,  1,  2,  3],
</span><span id=__span-148-2><a id=__codelineno-148-2 name=__codelineno-148-2 href=#__codelineno-148-2></a>         [ 4,  5,  6,  7],
</span><span id=__span-148-3><a id=__codelineno-148-3 name=__codelineno-148-3 href=#__codelineno-148-3></a>         [ 8,  9, 10, 11]],
</span><span id=__span-148-4><a id=__codelineno-148-4 name=__codelineno-148-4 href=#__codelineno-148-4></a>
</span><span id=__span-148-5><a id=__codelineno-148-5 name=__codelineno-148-5 href=#__codelineno-148-5></a>        [[12, 13, 14, 15],
</span><span id=__span-148-6><a id=__codelineno-148-6 name=__codelineno-148-6 href=#__codelineno-148-6></a>         [16, 17, 18, 19],
</span><span id=__span-148-7><a id=__codelineno-148-7 name=__codelineno-148-7 href=#__codelineno-148-7></a>         [20, 21, 22, 23]]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-149-1><a id=__codelineno-149-1 name=__codelineno-149-1 href=#__codelineno-149-1></a><span class=n>X</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>24</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-149-2><a id=__codelineno-149-2 name=__codelineno-149-2 href=#__codelineno-149-2></a><span class=n>X</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-150-1><a id=__codelineno-150-1 name=__codelineno-150-1 href=#__codelineno-150-1></a>&lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=
</span><span id=__span-150-2><a id=__codelineno-150-2 name=__codelineno-150-2 href=#__codelineno-150-2></a>array([[[ 0,  1,  2,  3],
</span><span id=__span-150-3><a id=__codelineno-150-3 name=__codelineno-150-3 href=#__codelineno-150-3></a>        [ 4,  5,  6,  7],
</span><span id=__span-150-4><a id=__codelineno-150-4 name=__codelineno-150-4 href=#__codelineno-150-4></a>        [ 8,  9, 10, 11]],
</span><span id=__span-150-5><a id=__codelineno-150-5 name=__codelineno-150-5 href=#__codelineno-150-5></a>
</span><span id=__span-150-6><a id=__codelineno-150-6 name=__codelineno-150-6 href=#__codelineno-150-6></a>       [[12, 13, 14, 15],
</span><span id=__span-150-7><a id=__codelineno-150-7 name=__codelineno-150-7 href=#__codelineno-150-7></a>        [16, 17, 18, 19],
</span><span id=__span-150-8><a id=__codelineno-150-8 name=__codelineno-150-8 href=#__codelineno-150-8></a>        [20, 21, 22, 23]]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>Tensors can be even more useful than matrices. For instance, we can stack matrices to obtain a 3D tensor with a shape of <span class=arithmatex>\(3 \times 3 \times 4\)</span>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=40:2><input checked=checked id=234-tensors-pytorch_1 name=__tabbed_40 type=radio><input id=234-tensors-tensorflow_1 name=__tabbed_40 type=radio><div class=tabbed-labels><label for=234-tensors-pytorch_1>PYTORCH</label><label for=234-tensors-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-151-1><a id=__codelineno-151-1 name=__codelineno-151-1 href=#__codelineno-151-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-151-2><a id=__codelineno-151-2 name=__codelineno-151-2 href=#__codelineno-151-2></a><span class=n>B</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>  <span class=c1># Assign a copy of A to B by allocating new memory</span>
</span><span id=__span-151-3><a id=__codelineno-151-3 name=__codelineno-151-3 href=#__codelineno-151-3></a><span class=n>A</span><span class=p>,</span> <span class=n>A</span> <span class=o>+</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-152-1><a id=__codelineno-152-1 name=__codelineno-152-1 href=#__codelineno-152-1></a>(tensor([[ 0.,  1.,  2.,  3.],
</span><span id=__span-152-2><a id=__codelineno-152-2 name=__codelineno-152-2 href=#__codelineno-152-2></a>         [ 4.,  5.,  6.,  7.],
</span><span id=__span-152-3><a id=__codelineno-152-3 name=__codelineno-152-3 href=#__codelineno-152-3></a>         [ 8.,  9., 10., 11.],
</span><span id=__span-152-4><a id=__codelineno-152-4 name=__codelineno-152-4 href=#__codelineno-152-4></a>         [12., 13., 14., 15.],
</span><span id=__span-152-5><a id=__codelineno-152-5 name=__codelineno-152-5 href=#__codelineno-152-5></a>         [16., 17., 18., 19.]]),
</span><span id=__span-152-6><a id=__codelineno-152-6 name=__codelineno-152-6 href=#__codelineno-152-6></a> tensor([[ 0.,  2.,  4.,  6.],
</span><span id=__span-152-7><a id=__codelineno-152-7 name=__codelineno-152-7 href=#__codelineno-152-7></a>         [ 8., 10., 12., 14.],
</span><span id=__span-152-8><a id=__codelineno-152-8 name=__codelineno-152-8 href=#__codelineno-152-8></a>         [16., 18., 20., 22.],
</span><span id=__span-152-9><a id=__codelineno-152-9 name=__codelineno-152-9 href=#__codelineno-152-9></a>         [24., 26., 28., 30.],
</span><span id=__span-152-10><a id=__codelineno-152-10 name=__codelineno-152-10 href=#__codelineno-152-10></a>         [32., 34., 36., 38.]]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-153-1><a id=__codelineno-153-1 name=__codelineno-153-1 href=#__codelineno-153-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-153-2><a id=__codelineno-153-2 name=__codelineno-153-2 href=#__codelineno-153-2></a><span class=n>B</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>  <span class=c1># Assign a copy of A to B by allocating new memory</span>
</span><span id=__span-153-3><a id=__codelineno-153-3 name=__codelineno-153-3 href=#__codelineno-153-3></a><span class=n>A</span><span class=p>,</span> <span class=n>A</span> <span class=o>+</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-154-1><a id=__codelineno-154-1 name=__codelineno-154-1 href=#__codelineno-154-1></a>(&lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
</span><span id=__span-154-2><a id=__codelineno-154-2 name=__codelineno-154-2 href=#__codelineno-154-2></a>array([[ 0.,  1.,  2.,  3.],
</span><span id=__span-154-3><a id=__codelineno-154-3 name=__codelineno-154-3 href=#__codelineno-154-3></a>       [ 4.,  5.,  6.,  7.],
</span><span id=__span-154-4><a id=__codelineno-154-4 name=__codelineno-154-4 href=#__codelineno-154-4></a>       [ 8.,  9., 10., 11.],
</span><span id=__span-154-5><a id=__codelineno-154-5 name=__codelineno-154-5 href=#__codelineno-154-5></a>       [12., 13., 14., 15.],
</span><span id=__span-154-6><a id=__codelineno-154-6 name=__codelineno-154-6 href=#__codelineno-154-6></a>       [16., 17., 18., 19.]], dtype=float32)&gt;,
</span><span id=__span-154-7><a id=__codelineno-154-7 name=__codelineno-154-7 href=#__codelineno-154-7></a> &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
</span><span id=__span-154-8><a id=__codelineno-154-8 name=__codelineno-154-8 href=#__codelineno-154-8></a>array([[ 0.,  2.,  4.,  6.],
</span><span id=__span-154-9><a id=__codelineno-154-9 name=__codelineno-154-9 href=#__codelineno-154-9></a>       [ 8., 10., 12., 14.],
</span><span id=__span-154-10><a id=__codelineno-154-10 name=__codelineno-154-10 href=#__codelineno-154-10></a>       [16., 18., 20., 22.],
</span><span id=__span-154-11><a id=__codelineno-154-11 name=__codelineno-154-11 href=#__codelineno-154-11></a>       [24., 26., 28., 30.],
</span><span id=__span-154-12><a id=__codelineno-154-12 name=__codelineno-154-12 href=#__codelineno-154-12></a>       [32., 34., 36., 38.]], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <h3 id=235-basic-properties-of-tensor-arithmetic>2.3.5 Basic Properties of Tensor Arithmetic<a class=headerlink href=#235-basic-properties-of-tensor-arithmetic title="Permanent link">&para;</a></h3> <p>Scalars, vectors, matrices, and higher-order tensors all have some handy properties. For example, elementwise operations produce outputs that have the same shape as their operands.</p> <div class="tabbed-set tabbed-alternate" data-tabs=41:2><input checked=checked id=235-basic-properties-of-tensor-arithmetic-pytorch name=__tabbed_41 type=radio><input id=235-basic-properties-of-tensor-arithmetic-tensorflow name=__tabbed_41 type=radio><div class=tabbed-labels><label for=235-basic-properties-of-tensor-arithmetic-pytorch>PYTORCH</label><label for=235-basic-properties-of-tensor-arithmetic-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-155-1><a id=__codelineno-155-1 name=__codelineno-155-1 href=#__codelineno-155-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span><span id=__span-155-2><a id=__codelineno-155-2 name=__codelineno-155-2 href=#__codelineno-155-2></a><span class=n>B</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>  <span class=c1># Assign a copy of A to B by allocating new memory</span>
</span><span id=__span-155-3><a id=__codelineno-155-3 name=__codelineno-155-3 href=#__codelineno-155-3></a><span class=n>A</span><span class=p>,</span> <span class=n>A</span> <span class=o>+</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-156-1><a id=__codelineno-156-1 name=__codelineno-156-1 href=#__codelineno-156-1></a>(tensor([[0., 1., 2.],
</span><span id=__span-156-2><a id=__codelineno-156-2 name=__codelineno-156-2 href=#__codelineno-156-2></a>         [3., 4., 5.]]),
</span><span id=__span-156-3><a id=__codelineno-156-3 name=__codelineno-156-3 href=#__codelineno-156-3></a> tensor([[ 0.,  2.,  4.],
</span><span id=__span-156-4><a id=__codelineno-156-4 name=__codelineno-156-4 href=#__codelineno-156-4></a>         [ 6.,  8., 10.]]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-157-1><a id=__codelineno-157-1 name=__codelineno-157-1 href=#__codelineno-157-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>),</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-157-2><a id=__codelineno-157-2 name=__codelineno-157-2 href=#__codelineno-157-2></a><span class=n>B</span> <span class=o>=</span> <span class=n>A</span>  <span class=c1># No cloning of A to B by allocating new memory</span>
</span><span id=__span-157-3><a id=__codelineno-157-3 name=__codelineno-157-3 href=#__codelineno-157-3></a><span class=n>A</span><span class=p>,</span> <span class=n>A</span> <span class=o>+</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-158-1><a id=__codelineno-158-1 name=__codelineno-158-1 href=#__codelineno-158-1></a>(&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=
</span><span id=__span-158-2><a id=__codelineno-158-2 name=__codelineno-158-2 href=#__codelineno-158-2></a>array([[0., 1., 2.],
</span><span id=__span-158-3><a id=__codelineno-158-3 name=__codelineno-158-3 href=#__codelineno-158-3></a>       [3., 4., 5.]], dtype=float32)&gt;,
</span><span id=__span-158-4><a id=__codelineno-158-4 name=__codelineno-158-4 href=#__codelineno-158-4></a> &lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=
</span><span id=__span-158-5><a id=__codelineno-158-5 name=__codelineno-158-5 href=#__codelineno-158-5></a>array([[ 0.,  2.,  4.],
</span><span id=__span-158-6><a id=__codelineno-158-6 name=__codelineno-158-6 href=#__codelineno-158-6></a>       [ 6.,  8., 10.]], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <p>The elementwise product of two matrices is called their Hadamard product (denoted <span class=arithmatex>\(\odot\)</span>). We can spell out the entries of the Hadamard product of two matrices <span class=arithmatex>\(\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}\)</span>:</p> <div class=arithmatex>\[(\mathbf{A} \odot \mathbf{B})_{ij} = \begin{bmatrix} a_{11} b_{11} &amp; a_{12} b_{12} &amp; \cdots &amp; a_{1n} b_{1n} \\ a_{21} b_{21} &amp; a_{22} b_{22} &amp; \cdots &amp; a_{2n} b_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} b_{m1} &amp; a_{m2} b_{m2} &amp; \cdots &amp; a_{mn} b_{mn} \end{bmatrix}.\]</div> <div class="tabbed-set tabbed-alternate" data-tabs=42:2><input checked=checked id=235-basic-properties-of-tensor-arithmetic-pytorch_1 name=__tabbed_42 type=radio><input id=235-basic-properties-of-tensor-arithmetic-tensorflow_1 name=__tabbed_42 type=radio><div class=tabbed-labels><label for=235-basic-properties-of-tensor-arithmetic-pytorch_1>PYTORCH</label><label for=235-basic-properties-of-tensor-arithmetic-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-159-1><a id=__codelineno-159-1 name=__codelineno-159-1 href=#__codelineno-159-1></a><span class=n>A</span> <span class=o>*</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-160-1><a id=__codelineno-160-1 name=__codelineno-160-1 href=#__codelineno-160-1></a>tensor([[ 0.,  1.,  4.],
</span><span id=__span-160-2><a id=__codelineno-160-2 name=__codelineno-160-2 href=#__codelineno-160-2></a>        [ 9., 16., 25.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-161-1><a id=__codelineno-161-1 name=__codelineno-161-1 href=#__codelineno-161-1></a><span class=n>A</span> <span class=o>*</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-162-1><a id=__codelineno-162-1 name=__codelineno-162-1 href=#__codelineno-162-1></a>&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=
</span><span id=__span-162-2><a id=__codelineno-162-2 name=__codelineno-162-2 href=#__codelineno-162-2></a>array([[ 0.,  1.,  4.],
</span><span id=__span-162-3><a id=__codelineno-162-3 name=__codelineno-162-3 href=#__codelineno-162-3></a>       [ 9., 16., 25.]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=236-reduction>2.3.6 Reduction<a class=headerlink href=#236-reduction title="Permanent link">&para;</a></h3> <p>Often, we wish to calculate the sum of a tensor‚Äôs elements. To express the sum of the elements in a vector <span class=arithmatex>\(x\)</span> of length n, we use <span class=arithmatex>\(\sum_{i=1}^n x_i\)</span>. In code, we can call the method <code>sum</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=43:2><input checked=checked id=236-reduction-pytorch name=__tabbed_43 type=radio><input id=236-reduction-tensorflow name=__tabbed_43 type=radio><div class=tabbed-labels><label for=236-reduction-pytorch>PYTORCH</label><label for=236-reduction-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-163-1><a id=__codelineno-163-1 name=__codelineno-163-1 href=#__codelineno-163-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span><span id=__span-163-2><a id=__codelineno-163-2 name=__codelineno-163-2 href=#__codelineno-163-2></a><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-164-1><a id=__codelineno-164-1 name=__codelineno-164-1 href=#__codelineno-164-1></a>(tensor([0., 1., 2.]), tensor(3.))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-165-1><a id=__codelineno-165-1 name=__codelineno-165-1 href=#__codelineno-165-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span><span id=__span-165-2><a id=__codelineno-165-2 name=__codelineno-165-2 href=#__codelineno-165-2></a><span class=n>x</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-166-1><a id=__codelineno-166-1 name=__codelineno-166-1 href=#__codelineno-166-1></a>(&lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 2.], dtype=float32)&gt;,
</span><span id=__span-166-2><a id=__codelineno-166-2 name=__codelineno-166-2 href=#__codelineno-166-2></a> &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;)
</span></code></pre></div> </div> </div> </div> <p>To express sums over the elements of tensors of arbitrary shape, we simply sum over all its axes.</p> <div class="tabbed-set tabbed-alternate" data-tabs=44:2><input checked=checked id=236-reduction-pytorch_1 name=__tabbed_44 type=radio><input id=236-reduction-tensorflow_1 name=__tabbed_44 type=radio><div class=tabbed-labels><label for=236-reduction-pytorch_1>PYTORCH</label><label for=236-reduction-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-167-1><a id=__codelineno-167-1 name=__codelineno-167-1 href=#__codelineno-167-1></a><span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-168-1><a id=__codelineno-168-1 name=__codelineno-168-1 href=#__codelineno-168-1></a>(torch.Size([5, 4]), tensor(190.))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-169-1><a id=__codelineno-169-1 name=__codelineno-169-1 href=#__codelineno-169-1></a><span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-170-1><a id=__codelineno-170-1 name=__codelineno-170-1 href=#__codelineno-170-1></a>(TensorShape([5, 4]), &lt;tf.Tensor: shape=(), dtype=float32, numpy=190.0&gt;)
</span></code></pre></div> </div> </div> </div> <p>Specifying axis=1 will reduce the column dimension (axis 1) by summing up elements of all the columns.</p> <div class="tabbed-set tabbed-alternate" data-tabs=45:2><input checked=checked id=236-reduction-pytorch_2 name=__tabbed_45 type=radio><input id=236-reduction-tensorflow_2 name=__tabbed_45 type=radio><div class=tabbed-labels><label for=236-reduction-pytorch_2>PYTORCH</label><label for=236-reduction-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-171-1><a id=__codelineno-171-1 name=__codelineno-171-1 href=#__codelineno-171-1></a><span class=n>A_sum_axis1</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-171-2><a id=__codelineno-171-2 name=__codelineno-171-2 href=#__codelineno-171-2></a><span class=n>A_sum_axis1</span><span class=p>,</span> <span class=n>A_sum_axis1</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-172-1><a id=__codelineno-172-1 name=__codelineno-172-1 href=#__codelineno-172-1></a>(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-173-1><a id=__codelineno-173-1 name=__codelineno-173-1 href=#__codelineno-173-1></a><span class=n>A_sum_axis1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-173-2><a id=__codelineno-173-2 name=__codelineno-173-2 href=#__codelineno-173-2></a><span class=n>A_sum_axis1</span><span class=p>,</span> <span class=n>A_sum_axis1</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-174-1><a id=__codelineno-174-1 name=__codelineno-174-1 href=#__codelineno-174-1></a>(&lt;tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 6., 22., 38., 54., 70.], dtype=float32)&gt;,
</span><span id=__span-174-2><a id=__codelineno-174-2 name=__codelineno-174-2 href=#__codelineno-174-2></a> TensorShape([5]))
</span></code></pre></div> </div> </div> </div> <p>Reducing a matrix along both rows and columns via summation is equivalent to summing up all the elements of the matrix.</p> <div class="tabbed-set tabbed-alternate" data-tabs=46:2><input checked=checked id=236-reduction-pytorch_3 name=__tabbed_46 type=radio><input id=236-reduction-tensorflow_3 name=__tabbed_46 type=radio><div class=tabbed-labels><label for=236-reduction-pytorch_3>PYTORCH</label><label for=236-reduction-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-175-1><a id=__codelineno-175-1 name=__codelineno-175-1 href=#__codelineno-175-1></a><span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-176-1><a id=__codelineno-176-1 name=__codelineno-176-1 href=#__codelineno-176-1></a>tensor(190.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-177-1><a id=__codelineno-177-1 name=__codelineno-177-1 href=#__codelineno-177-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-178-1><a id=__codelineno-178-1 name=__codelineno-178-1 href=#__codelineno-178-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=190.0&gt;
</span></code></pre></div> </div> </div> </div> <p>A related quantity is the mean, also called the average. We calculate the mean by dividing the sum by the total number of elements. Because computing the mean is so common, it gets a dedicated library function that works analogously to sum.</p> <div class="tabbed-set tabbed-alternate" data-tabs=47:2><input checked=checked id=236-reduction-pytorch_4 name=__tabbed_47 type=radio><input id=236-reduction-tensorflow_4 name=__tabbed_47 type=radio><div class=tabbed-labels><label for=236-reduction-pytorch_4>PYTORCH</label><label for=236-reduction-tensorflow_4>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-179-1><a id=__codelineno-179-1 name=__codelineno-179-1 href=#__codelineno-179-1></a><span class=n>A</span><span class=o>.</span><span class=n>mean</span><span class=p>(),</span> <span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>/</span> <span class=n>A</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-180-1><a id=__codelineno-180-1 name=__codelineno-180-1 href=#__codelineno-180-1></a>(tensor(9.5000), tensor(9.5000))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-181-1><a id=__codelineno-181-1 name=__codelineno-181-1 href=#__codelineno-181-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>A</span><span class=p>),</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>)</span> <span class=o>/</span> <span class=n>tf</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=n>A</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-182-1><a id=__codelineno-182-1 name=__codelineno-182-1 href=#__codelineno-182-1></a>(&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.5&gt;,
</span><span id=__span-182-2><a id=__codelineno-182-2 name=__codelineno-182-2 href=#__codelineno-182-2></a> &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.5&gt;)
</span></code></pre></div> </div> </div> </div> <p>Likewise, the function for calculating the mean can also reduce a tensor along specific axes.</p> <div class="tabbed-set tabbed-alternate" data-tabs=48:2><input checked=checked id=236-reduction-pytorch_5 name=__tabbed_48 type=radio><input id=236-reduction-tensorflow_5 name=__tabbed_48 type=radio><div class=tabbed-labels><label for=236-reduction-pytorch_5>PYTORCH</label><label for=236-reduction-tensorflow_5>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-183-1><a id=__codelineno-183-1 name=__codelineno-183-1 href=#__codelineno-183-1></a><span class=n>A</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span> <span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=o>/</span> <span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-184-1><a id=__codelineno-184-1 name=__codelineno-184-1 href=#__codelineno-184-1></a>(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-185-1><a id=__codelineno-185-1 name=__codelineno-185-1 href=#__codelineno-185-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=o>/</span> <span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-186-1><a id=__codelineno-186-1 name=__codelineno-186-1 href=#__codelineno-186-1></a>(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)&gt;,
</span><span id=__span-186-2><a id=__codelineno-186-2 name=__codelineno-186-2 href=#__codelineno-186-2></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <h3 id=237-non-reduction-sum>2.3.7 Non-Reduction Sum<a class=headerlink href=#237-non-reduction-sum title="Permanent link">&para;</a></h3> <p>Sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean. This matters when we want to use the broadcast mechanism.</p> <div class="tabbed-set tabbed-alternate" data-tabs=49:2><input checked=checked id=237-non-reduction-sum-pytorch name=__tabbed_49 type=radio><input id=237-non-reduction-sum-tensorflow name=__tabbed_49 type=radio><div class=tabbed-labels><label for=237-non-reduction-sum-pytorch>PYTORCH</label><label for=237-non-reduction-sum-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-187-1><a id=__codelineno-187-1 name=__codelineno-187-1 href=#__codelineno-187-1></a><span class=n>sum_A</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-187-2><a id=__codelineno-187-2 name=__codelineno-187-2 href=#__codelineno-187-2></a><span class=n>sum_A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-188-1><a id=__codelineno-188-1 name=__codelineno-188-1 href=#__codelineno-188-1></a>tensor([[ 6.],
</span><span id=__span-188-2><a id=__codelineno-188-2 name=__codelineno-188-2 href=#__codelineno-188-2></a>        [22.],
</span><span id=__span-188-3><a id=__codelineno-188-3 name=__codelineno-188-3 href=#__codelineno-188-3></a>        [38.],
</span><span id=__span-188-4><a id=__codelineno-188-4 name=__codelineno-188-4 href=#__codelineno-188-4></a>        [54.],
</span><span id=__span-188-5><a id=__codelineno-188-5 name=__codelineno-188-5 href=#__codelineno-188-5></a>        [70.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-189-1><a id=__codelineno-189-1 name=__codelineno-189-1 href=#__codelineno-189-1></a><span class=n>sum_A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-189-2><a id=__codelineno-189-2 name=__codelineno-189-2 href=#__codelineno-189-2></a><span class=n>sum_A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-190-1><a id=__codelineno-190-1 name=__codelineno-190-1 href=#__codelineno-190-1></a>&lt;tf.Tensor: shape=(5, 1), dtype=int32, numpy=
</span><span id=__span-190-2><a id=__codelineno-190-2 name=__codelineno-190-2 href=#__codelineno-190-2></a>array([[ 6],
</span><span id=__span-190-3><a id=__codelineno-190-3 name=__codelineno-190-3 href=#__codelineno-190-3></a>       [22],
</span><span id=__span-190-4><a id=__codelineno-190-4 name=__codelineno-190-4 href=#__codelineno-190-4></a>       [38],
</span><span id=__span-190-5><a id=__codelineno-190-5 name=__codelineno-190-5 href=#__codelineno-190-5></a>       [54],
</span><span id=__span-190-6><a id=__codelineno-190-6 name=__codelineno-190-6 href=#__codelineno-190-6></a>       [70]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>For instance, since sum_A keeps its two axes after summing each row, we can divide A by sum_A with broadcasting to create a matrix where each row sums up to 1.</p> <div class="tabbed-set tabbed-alternate" data-tabs=50:2><input checked=checked id=237-non-reduction-sum-pytorch_1 name=__tabbed_50 type=radio><input id=237-non-reduction-sum-tensorflow_1 name=__tabbed_50 type=radio><div class=tabbed-labels><label for=237-non-reduction-sum-pytorch_1>PYTORCH</label><label for=237-non-reduction-sum-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-191-1><a id=__codelineno-191-1 name=__codelineno-191-1 href=#__codelineno-191-1></a><span class=n>A</span> <span class=o>/</span> <span class=n>sum_A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-192-1><a id=__codelineno-192-1 name=__codelineno-192-1 href=#__codelineno-192-1></a>tensor([[0.0000, 0.1667, 0.3333, 0.5000],
</span><span id=__span-192-2><a id=__codelineno-192-2 name=__codelineno-192-2 href=#__codelineno-192-2></a>        [0.1818, 0.2273, 0.2727, 0.3182],
</span><span id=__span-192-3><a id=__codelineno-192-3 name=__codelineno-192-3 href=#__codelineno-192-3></a>        [0.2105, 0.2368, 0.2632, 0.2895],
</span><span id=__span-192-4><a id=__codelineno-192-4 name=__codelineno-192-4 href=#__codelineno-192-4></a>        [0.2222, 0.2407, 0.2593, 0.2778],
</span><span id=__span-192-5><a id=__codelineno-192-5 name=__codelineno-192-5 href=#__codelineno-192-5></a>        [0.2286, 0.2429, 0.2571, 0.2714]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-193-1><a id=__codelineno-193-1 name=__codelineno-193-1 href=#__codelineno-193-1></a><span class=n>A</span> <span class=o>/</span> <span class=n>sum_A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-194-1><a id=__codelineno-194-1 name=__codelineno-194-1 href=#__codelineno-194-1></a>&lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
</span><span id=__span-194-2><a id=__codelineno-194-2 name=__codelineno-194-2 href=#__codelineno-194-2></a>array([[0.        , 0.16666667, 0.33333334, 0.5       ],
</span><span id=__span-194-3><a id=__codelineno-194-3 name=__codelineno-194-3 href=#__codelineno-194-3></a>       [0.18181819, 0.22727273, 0.27272728, 0.3181818 ],
</span><span id=__span-194-4><a id=__codelineno-194-4 name=__codelineno-194-4 href=#__codelineno-194-4></a>       [0.21052632, 0.23684211, 0.2631579 , 0.28947368],
</span><span id=__span-194-5><a id=__codelineno-194-5 name=__codelineno-194-5 href=#__codelineno-194-5></a>       [0.22222222, 0.24074075, 0.25925925, 0.2777778 ],
</span><span id=__span-194-6><a id=__codelineno-194-6 name=__codelineno-194-6 href=#__codelineno-194-6></a>       [0.22857143, 0.24285714, 0.25714287, 0.2714286 ]],
</span><span id=__span-194-7><a id=__codelineno-194-7 name=__codelineno-194-7 href=#__codelineno-194-7></a>      dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>If we want to calculate the cumulative sum of elements of A along some axis, say axis=0 (row by row), we can call the cumsum function. By design, this function does not reduce the input tensor along any axis.</p> <div class="tabbed-set tabbed-alternate" data-tabs=51:2><input checked=checked id=237-non-reduction-sum-pytorch_2 name=__tabbed_51 type=radio><input id=237-non-reduction-sum-tensorflow_2 name=__tabbed_51 type=radio><div class=tabbed-labels><label for=237-non-reduction-sum-pytorch_2>PYTORCH</label><label for=237-non-reduction-sum-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-195-1><a id=__codelineno-195-1 name=__codelineno-195-1 href=#__codelineno-195-1></a><span class=n>A</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-196-1><a id=__codelineno-196-1 name=__codelineno-196-1 href=#__codelineno-196-1></a>tensor([[ 0.,  1.,  2.,  3.],
</span><span id=__span-196-2><a id=__codelineno-196-2 name=__codelineno-196-2 href=#__codelineno-196-2></a>        [ 4.,  6.,  8., 10.],
</span><span id=__span-196-3><a id=__codelineno-196-3 name=__codelineno-196-3 href=#__codelineno-196-3></a>        [12., 15., 18., 21.],
</span><span id=__span-196-4><a id=__codelineno-196-4 name=__codelineno-196-4 href=#__codelineno-196-4></a>        [24., 28., 32., 36.],
</span><span id=__span-196-5><a id=__codelineno-196-5 name=__codelineno-196-5 href=#__codelineno-196-5></a>        [40., 45., 50., 55.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-197-1><a id=__codelineno-197-1 name=__codelineno-197-1 href=#__codelineno-197-1></a><span class=n>tf</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-198-1><a id=__codelineno-198-1 name=__codelineno-198-1 href=#__codelineno-198-1></a>&lt;tf.Tensor: shape=(5, 4), dtype=int32, numpy=
</span><span id=__span-198-2><a id=__codelineno-198-2 name=__codelineno-198-2 href=#__codelineno-198-2></a>array([[ 0,  1,  2,  3],
</span><span id=__span-198-3><a id=__codelineno-198-3 name=__codelineno-198-3 href=#__codelineno-198-3></a>       [ 4,  6,  8, 10],
</span><span id=__span-198-4><a id=__codelineno-198-4 name=__codelineno-198-4 href=#__codelineno-198-4></a>       [12, 15, 18, 21],
</span><span id=__span-198-5><a id=__codelineno-198-5 name=__codelineno-198-5 href=#__codelineno-198-5></a>       [24, 28, 32, 36],
</span><span id=__span-198-6><a id=__codelineno-198-6 name=__codelineno-198-6 href=#__codelineno-198-6></a>       [40, 45, 50, 55]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=238-dot-products>2.3.8 Dot Products<a class=headerlink href=#238-dot-products title="Permanent link">&para;</a></h3> <p>So far, we have only performed elementwise operations, sums, and averages. And if this was all we could do, linear algebra would not deserve its own section. Fortunately, this is where things get more interesting. One of the most fundamental operations is the dot product. Given two vectors <span class=arithmatex>\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^d\)</span>, their dot product <span class=arithmatex>\(\mathbf{x}^\top \mathbf{y}\)</span> (or <span class=arithmatex>\(\langle \mathbf{x}, \mathbf{y} \rangle\)</span>) is a sum over the products of the elements at the same position: <span class=arithmatex>\(\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^d x_i y_i\)</span>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=52:2><input checked=checked id=238-dot-products-pytorch name=__tabbed_52 type=radio><input id=238-dot-products-tensorflow name=__tabbed_52 type=radio><div class=tabbed-labels><label for=238-dot-products-pytorch>PYTORCH</label><label for=238-dot-products-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-199-1><a id=__codelineno-199-1 name=__codelineno-199-1 href=#__codelineno-199-1></a><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span><span id=__span-199-2><a id=__codelineno-199-2 name=__codelineno-199-2 href=#__codelineno-199-2></a><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-200-1><a id=__codelineno-200-1 name=__codelineno-200-1 href=#__codelineno-200-1></a>(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-201-1><a id=__codelineno-201-1 name=__codelineno-201-1 href=#__codelineno-201-1></a><span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span><span id=__span-201-2><a id=__codelineno-201-2 name=__codelineno-201-2 href=#__codelineno-201-2></a><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>tensordot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>axes</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-202-1><a id=__codelineno-202-1 name=__codelineno-202-1 href=#__codelineno-202-1></a>(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)&gt;,
</span><span id=__span-202-2><a id=__codelineno-202-2 name=__codelineno-202-2 href=#__codelineno-202-2></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)&gt;,
</span><span id=__span-202-3><a id=__codelineno-202-3 name=__codelineno-202-3 href=#__codelineno-202-3></a> &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;)
</span></code></pre></div> </div> </div> </div> <p>Equivalently, we can calculate the dot product of two vectors by performing an elementwise multiplication followed by a sum:</p> <div class=arithmatex>\[\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^d x_i y_i.\]</div> <div class="tabbed-set tabbed-alternate" data-tabs=53:2><input checked=checked id=238-dot-products-pytorch_1 name=__tabbed_53 type=radio><input id=238-dot-products-tensorflow_1 name=__tabbed_53 type=radio><div class=tabbed-labels><label for=238-dot-products-pytorch_1>PYTORCH</label><label for=238-dot-products-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-203-1><a id=__codelineno-203-1 name=__codelineno-203-1 href=#__codelineno-203-1></a><span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-204-1><a id=__codelineno-204-1 name=__codelineno-204-1 href=#__codelineno-204-1></a>tensor(6.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-205-1><a id=__codelineno-205-1 name=__codelineno-205-1 href=#__codelineno-205-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-206-1><a id=__codelineno-206-1 name=__codelineno-206-1 href=#__codelineno-206-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;
</span></code></pre></div> </div> </div> </div> <p>Dot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector <span class=arithmatex>\(\mathbf{x} \in \mathbb{R}^d\)</span> and a set of weights denoted by <span class=arithmatex>\(\mathbf{w} \in \mathbb{R}^d\)</span>, the weighted sum of the values in <span class=arithmatex>\(\mathbf{x}\)</span> according to the weights <span class=arithmatex>\(\mathbf{w}\)</span> could be expressed as the dot product <span class=arithmatex>\(\mathbf{x}^\top \mathbf{w}\)</span>. When the weights are non-negative and sum to one (i.e., <span class=arithmatex>\(\left(\sum_{i=1}^d {w_i} = 1\right)\)</span>), the dot product expresses a <em>weighted average</em>. We revisit this example in the section on softmax operations (see Section 2.5).</p> <h3 id=239-matrix-vector-products>2.3.9 Matrix-Vector Products<a class=headerlink href=#239-matrix-vector-products title="Permanent link">&para;</a></h3> <p>Now that we know how to calculate dot products, we can begin to understand the product between an <span class=arithmatex>\(m \times n\)</span> matrix <span class=arithmatex>\(\mathbf{A}\)</span> and an <span class=arithmatex>\(n\)</span>-dimensional vector <span class=arithmatex>\(x\)</span>. To start off, we visualize our matrix in terms of its row vectors:</p> <div class=arithmatex>\[\mathbf{A} = \begin{bmatrix} \mathbf{a}^\top_{1} \\ \mathbf{a}^\top_{2} \\ \vdots \\ \mathbf{a}^\top_{m} \end{bmatrix},\]</div> <p>where each <span class=arithmatex>\(\mathbf{a}^\top_{i} \in \mathbb{R}^n\)</span> is a row vector representing the <span class=arithmatex>\(i^\mathrm{th}\)</span> row of the matrix <span class=arithmatex>\(\mathbf{A}\)</span>. </p> <p>The maxtri-vector product <span class=arithmatex>\(\mathbf{A}\mathbf{x}\)</span> is simply a column vector of length <span class=arithmatex>\(m\)</span> whose <span class=arithmatex>\(i^\mathrm{th}\)</span> element is the dot product <span class=arithmatex>\(\mathbf{a}^\top_i \mathbf{x}\)</span>:</p> <div class=arithmatex>\[\mathbf{A} = \begin{bmatrix} \mathbf{a}^\top_{1} \\ \mathbf{a}^\top_{2} \\ \vdots \\ \mathbf{a}^\top_{m} \end{bmatrix} \quad \mathbf{A}\mathbf{x} = \begin{bmatrix} \mathbf{a}^\top_{1} \mathbf{x} \\ \mathbf{a}^\top_{2} \mathbf{x} \\ \vdots \\ \mathbf{a}^\top_{m} \mathbf{x} \end{bmatrix}.\]</div> <p>We can think of the matrix-vector product <span class=arithmatex>\(\mathbf{A}\mathbf{x}\)</span> as simply performing <span class=arithmatex>\(m\)</span> matrix-vector products and stitching the results together to form a vector of length <span class=arithmatex>\(m\)</span>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=54:2><input checked=checked id=239-matrix-vector-products-pytorch name=__tabbed_54 type=radio><input id=239-matrix-vector-products-tensorflow name=__tabbed_54 type=radio><div class=tabbed-labels><label for=239-matrix-vector-products-pytorch>PYTORCH</label><label for=239-matrix-vector-products-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-207-1><a id=__codelineno-207-1 name=__codelineno-207-1 href=#__codelineno-207-1></a><span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>mv</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-208-1><a id=__codelineno-208-1 name=__codelineno-208-1 href=#__codelineno-208-1></a>(torch.Size([5, 4]), torch.Size([4]), tensor([14., 38., 62., 86., 110.]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-209-1><a id=__codelineno-209-1 name=__codelineno-209-1 href=#__codelineno-209-1></a><span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>matvec</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-210-1><a id=__codelineno-210-1 name=__codelineno-210-1 href=#__codelineno-210-1></a>(TensorShape([5, 4]),
</span><span id=__span-210-2><a id=__codelineno-210-2 name=__codelineno-210-2 href=#__codelineno-210-2></a> TensorShape([4]),
</span><span id=__span-210-3><a id=__codelineno-210-3 name=__codelineno-210-3 href=#__codelineno-210-3></a> &lt;tf.Tensor: shape=(5,), dtype=int32, numpy=array([14, 38, 62, 86, 110], dtype=int32)&gt;)
</span></code></pre></div> </div> </div> </div> <h3 id=2310-matrix-matrix-multiplication>2.3.10 Matrix-Matrix Multiplication<a class=headerlink href=#2310-matrix-matrix-multiplication title="Permanent link">&para;</a></h3> <p>Once you have gotten the hang of dot products and matrix‚Äìvector products, then matrix‚Äìmatrix multiplication should be straightforward.</p> <p>Say that we have two matrices <span class=arithmatex>\(\mathbf{A} \in \mathbb{R}^{n \times k}\)</span> and <span class=arithmatex>\(\mathbf{B} \in \mathbb{R}^{k \times m}\)</span>:</p> <div class=arithmatex>\[\mathbf{A} = \begin{bmatrix} \mathbf{a_{11}} &amp; \mathbf{a_{12}} &amp; \cdots &amp; \mathbf{a_{1k}} \\ \mathbf{a_{21}} &amp; \mathbf{a_{22}} &amp; \cdots &amp; \mathbf{a_{2k}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{a_{n1}} &amp; \mathbf{a_{n2}} &amp; \cdots &amp; \mathbf{a_{nk}} \end{bmatrix}, \quad \mathbf{B} = \begin{bmatrix} \mathbf{b_{11}} &amp; \mathbf{b_{12}} &amp; \cdots &amp; \mathbf{b_{1m}} \\ \mathbf{b_{21}} &amp; \mathbf{b_{22}} &amp; \cdots &amp; \mathbf{b_{2m}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{b_{k1}} &amp; \mathbf{b_{k2}} &amp; \cdots &amp; \mathbf{b_{km}} \end{bmatrix}.\]</div> <p>Let us focus on computing the matrix product <span class=arithmatex>\(\mathbf{C} = \mathbf{A}\mathbf{B}\)</span>, where <span class=arithmatex>\(\mathbf{C} \in \mathbb{R}^{n \times m}\)</span>. To produce the matrix product <span class=arithmatex>\(\mathbf{C}\)</span>, we will carry out <span class=arithmatex>\(n\)</span> matrix-vector products and form the matrix <span class=arithmatex>\(\mathbf{C}\)</span> by concatenating the results together into a matrix with <span class=arithmatex>\(n\)</span> rows.</p> <div class=arithmatex>\[\mathbf{C} = \begin{bmatrix} \mathbf{a_{11}} \mathbf{B} &amp; \mathbf{a_{12}} \mathbf{B} &amp; \cdots &amp; \mathbf{a_{1k}} \mathbf{B} \\ \mathbf{a_{21}} \mathbf{B} &amp; \mathbf{a_{22}} \mathbf{B} &amp; \cdots &amp; \mathbf{a_{2k}} \mathbf{B} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{a_{n1}} \mathbf{B} &amp; \mathbf{a_{n2}} \mathbf{B} &amp; \cdots &amp; \mathbf{a_{nk}} \mathbf{B} \end{bmatrix}.\]</div> <p>Note that <span class=arithmatex>\(\mathbf{a_{ij}} \mathbf{B}\)</span> is a matrix-vector product. Here, we are writing <span class=arithmatex>\(\mathbf{a_{ij}}\)</span> to denote the row vector <span class=arithmatex>\(\mathbf{A}[i,:]\)</span> and <span class=arithmatex>\(\mathbf{a_{ij}} \mathbf{B}\)</span> still denotes a matrix-matrix product but one in which we multiply a row vector by a matrix.</p> <div class="tabbed-set tabbed-alternate" data-tabs=55:2><input checked=checked id=2310-matrix-matrix-multiplication-pytorch name=__tabbed_55 type=radio><input id=2310-matrix-matrix-multiplication-tensorflow name=__tabbed_55 type=radio><div class=tabbed-labels><label for=2310-matrix-matrix-multiplication-pytorch>PYTORCH</label><label for=2310-matrix-matrix-multiplication-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-211-1><a id=__codelineno-211-1 name=__codelineno-211-1 href=#__codelineno-211-1></a><span class=n>B</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-211-2><a id=__codelineno-211-2 name=__codelineno-211-2 href=#__codelineno-211-2></a><span class=n>torch</span><span class=o>.</span><span class=n>mm</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>),</span> <span class=n>A</span><span class=nd>@B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-212-1><a id=__codelineno-212-1 name=__codelineno-212-1 href=#__codelineno-212-1></a>(tensor([[ 3.,  3.,  3.,  3.],
</span><span id=__span-212-2><a id=__codelineno-212-2 name=__codelineno-212-2 href=#__codelineno-212-2></a>     [12., 12., 12., 12.]]),
</span><span id=__span-212-3><a id=__codelineno-212-3 name=__codelineno-212-3 href=#__codelineno-212-3></a>tensor([[ 3.,  3.,  3.,  3.],
</span><span id=__span-212-4><a id=__codelineno-212-4 name=__codelineno-212-4 href=#__codelineno-212-4></a>     [12., 12., 12., 12.]]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-213-1><a id=__codelineno-213-1 name=__codelineno-213-1 href=#__codelineno-213-1></a><span class=n>B</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span><span id=__span-213-2><a id=__codelineno-213-2 name=__codelineno-213-2 href=#__codelineno-213-2></a><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>),</span> <span class=n>A</span><span class=nd>@B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-214-1><a id=__codelineno-214-1 name=__codelineno-214-1 href=#__codelineno-214-1></a>(&lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=
</span><span id=__span-214-2><a id=__codelineno-214-2 name=__codelineno-214-2 href=#__codelineno-214-2></a>array([[ 3,  3,  3,  3],
</span><span id=__span-214-3><a id=__codelineno-214-3 name=__codelineno-214-3 href=#__codelineno-214-3></a>       [12, 12, 12, 12]], dtype=int32)&gt;,
</span><span id=__span-214-4><a id=__codelineno-214-4 name=__codelineno-214-4 href=#__codelineno-214-4></a> &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=
</span><span id=__span-214-5><a id=__codelineno-214-5 name=__codelineno-214-5 href=#__codelineno-214-5></a>array([[ 3,  3,  3,  3],
</span><span id=__span-214-6><a id=__codelineno-214-6 name=__codelineno-214-6 href=#__codelineno-214-6></a>       [12, 12, 12, 12]], dtype=int32)&gt;)
</span></code></pre></div> </div> </div> </div> <h3 id=2311-norms>2.3.11 Norms<a class=headerlink href=#2311-norms title="Permanent link">&para;</a></h3> <p>Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big it is. For instance, the <span class=arithmatex>\(l_2\)</span> norm measures the (Euclidean) length of a vector. Here, we are employing a notion of size that concerns the magnitude of a vector‚Äôs components (not its dimensionality).</p> <p>A norm is a function <span class=arithmatex>\(f\)</span> that maps a vector to a scalar, satisfying a handful of properties. Given any vector <span class=arithmatex>\(\mathbf{x}\)</span>, the first property says that if we scale all the elements of a vector by a constant factor <span class=arithmatex>\(\alpha\)</span>, its norm also scales by the <em>absolute value</em> of the same constant factor:</p> <div class=arithmatex>\[f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).\]</div> <p>The second property is the familiar triangle inequality:</p> <div class=arithmatex>\[f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).\]</div> <p>The third property simply says that the norm must be non-negative:</p> <div class=arithmatex>\[f(\mathbf{x}) \geq 0.\]</div> <p>That makes sense, as in most contexts the smallest <em>size</em> for anything is 0. The final property requires that the smallest norm is achieved and only achieved by a vector consisting of all zeros.</p> <div class=arithmatex>\[\forall i, [\mathbf{x}]_i = 0 \Leftrightarrow f(\mathbf{x})=0.\]</div> <p>You might notice that norms sound a lot like measures of distance. And with good reason! In fact, <em>the <span class=arithmatex>\(l_2\)</span> norm of a vector is the square root of the total squared distance of the vector</em>:</p> <div class=arithmatex>\[\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},\]</div> <p>where the subscript <span class=arithmatex>\(2\)</span> is often omitted in <span class=arithmatex>\(l_2\)</span> norms. In the above equation, the notation <span class=arithmatex>\(\mathbf{x}_i\)</span> refers to the scalar element of a vector <span class=arithmatex>\(\mathbf{x}\)</span>.</p> <p>In code, we can calculate the <span class=arithmatex>\(l_2\)</span> norm of a vector by calling <code>norm</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=56:2><input checked=checked id=2311-norms-pytorch name=__tabbed_56 type=radio><input id=2311-norms-tensorflow name=__tabbed_56 type=radio><div class=tabbed-labels><label for=2311-norms-pytorch>PYTORCH</label><label for=2311-norms-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-215-1><a id=__codelineno-215-1 name=__codelineno-215-1 href=#__codelineno-215-1></a><span class=n>u</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>3.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>4.0</span><span class=p>])</span>
</span><span id=__span-215-2><a id=__codelineno-215-2 name=__codelineno-215-2 href=#__codelineno-215-2></a><span class=n>torch</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>u</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-216-1><a id=__codelineno-216-1 name=__codelineno-216-1 href=#__codelineno-216-1></a>tensor(5.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-217-1><a id=__codelineno-217-1 name=__codelineno-217-1 href=#__codelineno-217-1></a><span class=n>u</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>3.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>4.0</span><span class=p>])</span>
</span><span id=__span-217-2><a id=__codelineno-217-2 name=__codelineno-217-2 href=#__codelineno-217-2></a><span class=n>tf</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>u</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-218-1><a id=__codelineno-218-1 name=__codelineno-218-1 href=#__codelineno-218-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;
</span></code></pre></div> </div> </div> </div> <p>In deep learning, we work more often with the squared <span class=arithmatex>\(l_2\)</span> norm. You might have noticed this from the formula for the objective function of linear regression, which involves a sum of squared terms. We can calculate this quantity via <code>torch.dot</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=57:2><input checked=checked id=2311-norms-pytorch_1 name=__tabbed_57 type=radio><input id=2311-norms-tensorflow_1 name=__tabbed_57 type=radio><div class=tabbed-labels><label for=2311-norms-pytorch_1>PYTORCH</label><label for=2311-norms-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-219-1><a id=__codelineno-219-1 name=__codelineno-219-1 href=#__codelineno-219-1></a><span class=n>torch</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>u</span><span class=p>,</span> <span class=n>u</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-220-1><a id=__codelineno-220-1 name=__codelineno-220-1 href=#__codelineno-220-1></a>tensor(25.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-221-1><a id=__codelineno-221-1 name=__codelineno-221-1 href=#__codelineno-221-1></a><span class=n>tf</span><span class=o>.</span><span class=n>tensordot</span><span class=p>(</span><span class=n>u</span><span class=p>,</span> <span class=n>u</span><span class=p>,</span> <span class=n>axes</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-222-1><a id=__codelineno-222-1 name=__codelineno-222-1 href=#__codelineno-222-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=25.0&gt;
</span></code></pre></div> </div> </div> </div> <p>The <span class=arithmatex>\(l_1\)</span> norm of a vector is calculated as the sum of the absolute values of the vector‚Äôs components:</p> <div class=arithmatex>\[\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.\]</div> <p>As compared with the <span class=arithmatex>\(l_2\)</span> norm, it is less influenced by outliers. To calculate the <span class=arithmatex>\(l_1\)</span> norm, we compose <code>abs</code> and <code>sum</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=58:2><input checked=checked id=2311-norms-pytorch_2 name=__tabbed_58 type=radio><input id=2311-norms-tensorflow_2 name=__tabbed_58 type=radio><div class=tabbed-labels><label for=2311-norms-pytorch_2>PYTORCH</label><label for=2311-norms-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-223-1><a id=__codelineno-223-1 name=__codelineno-223-1 href=#__codelineno-223-1></a><span class=n>torch</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>u</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-224-1><a id=__codelineno-224-1 name=__codelineno-224-1 href=#__codelineno-224-1></a>tensor(7.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-225-1><a id=__codelineno-225-1 name=__codelineno-225-1 href=#__codelineno-225-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>u</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-226-1><a id=__codelineno-226-1 name=__codelineno-226-1 href=#__codelineno-226-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=7.0&gt;
</span></code></pre></div> </div> </div> </div> <h2 id=24-calculus>2.4 Calculus<a class=headerlink href=#24-calculus title="Permanent link">&para;</a></h2> <h3 id=241-derivatives-and-differentiation>2.4.1. Derivatives and Differentiation<a class=headerlink href=#241-derivatives-and-differentiation title="Permanent link">&para;</a></h3> <p>Put simply, a derivative is the rate of change in a function with respect to changes in its arguments. Derivatives can tell us how rapidly a loss function would increase or decrease were we to increase or decrease each parameter by an infinitesimally small amount. Formally, for functions <span class=arithmatex>\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span>, the derivative of <span class=arithmatex>\(f\)</span> is defined as</p> <div class=arithmatex>\[f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.\]</div> <p>The notation for derivatives varies quite a bit. The following are all common ways of expressing the same thing:</p> <div class=arithmatex>\[\frac{df(x)}{dx} = f'(x) = D_x f(x) = D_x y = y' = \frac{dy}{dx}.\]</div> <p>The derivative of a function <span class=arithmatex>\(f\)</span> with respect to its argument <span class=arithmatex>\(x\)</span> is defined as</p> <div class=arithmatex>\[f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.\]</div> <p>In deep learning, our models are parameterized by sets of weights and biases. We often want to optimize these parameters to minimize some loss function, e.g., to improve our model‚Äôs accuracy on held-out data or to minimize the production cost. In order to do so, we will need to take derivatives of these loss functions with respect to our model parameters. Thus it is worth reviewing the basics of derivatives.</p> <h3 id=242-partial-derivatives>2.4.2. Partial Derivatives<a class=headerlink href=#242-partial-derivatives title="Permanent link">&para;</a></h3> <p>When our functions depend on more than one variable, we call them multivariate functions. Given a multivariate function <span class=arithmatex>\(f(\mathbf{x}) : \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, its partial derivative with respect to <span class=arithmatex>\(x_i\)</span> is</p> <div class=arithmatex>\[\frac{\partial}{\partial x_i} f(\mathbf{x}) = \lim_{h \rightarrow 0} \frac{f(\mathbf{x} + h \mathbf{e}_i) - f(\mathbf{x})}{h},\]</div> <p>where <span class=arithmatex>\(\mathbf{e}_i\)</span> is a vector of zeros with a one in the <span class=arithmatex>\(i^\mathrm{th}\)</span> coordinate. In multivariate functions, we can take derivatives with respect to any variable, treating all others as constants. This is called partial differentiation. To calculate a partial derivative, we simply differentiate as usual, following all the usual rules, but treating all the other variables as constants.</p> <h3 id=243-gradients>2.4.3. Gradients<a class=headerlink href=#243-gradients title="Permanent link">&para;</a></h3> <p>The gradient of a multivariate function <span class=arithmatex>\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> is the vector containing all of the partial derivatives, denoted <span class=arithmatex>\(\nabla_{\mathbf{x}} f(\mathbf{x})\)</span>:</p> <div class=arithmatex>\[\nabla_{\mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial}{\partial x_1} f(\mathbf{x}) \\ \frac{\partial}{\partial x_2} f(\mathbf{x}) \\ \vdots \\ \frac{\partial}{\partial x_n} f(\mathbf{x}) \end{bmatrix}.\]</div> <p>The gradient points in the direction of the greatest rate of increase of the function <span class=arithmatex>\(f\)</span>, and its magnitude is the slope of the function in that direction. The gradient of a function <span class=arithmatex>\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> gives us a vector <span class=arithmatex>\(\nabla_{\mathbf{x}} f(\mathbf{x})\)</span> whose entries are the partial derivatives of <span class=arithmatex>\(f\)</span> with respect to each of the input variables. This vector points in the direction of greatest increase of <span class=arithmatex>\(f\)</span> and its magnitude is the slope in that direction.</p> <h3 id=244-chain-rule>2.4.4. Chain Rule<a class=headerlink href=#244-chain-rule title="Permanent link">&para;</a></h3> <p>The chain rule allows us to differentiate compositions of functions. Given two functions <span class=arithmatex>\(f: \mathbb{R}^m \rightarrow \mathbb{R}^n\)</span> and <span class=arithmatex>\(g: \mathbb{R}^n \rightarrow \mathbb{R}^p\)</span>, the chain rule states that the gradient of the composition <span class=arithmatex>\(g \circ f\)</span> is given by</p> <div class=arithmatex>\[\nabla (\mathbf{g} \circ \mathbf{f})(\mathbf{x}) = \mathbf{J}_{\mathbf{f}}(\mathbf{x})^\top \mathbf{J}_{\mathbf{g}}(\mathbf{f}(\mathbf{x})),\]</div> <p>where <span class=arithmatex>\(\mathbf{J}_{\mathbf{f}}(\mathbf{x})\)</span> and <span class=arithmatex>\(\mathbf{J}_{\mathbf{g}}(\mathbf{f}(\mathbf{x}))\)</span> are the Jacobian matrices of <span class=arithmatex>\(f\)</span> and <span class=arithmatex>\(g\)</span>, respectively. In other words, the gradient of the function composition is the matrix product of the two Jacobian matrices.</p> <h3 id=245-discussion>2.4.5. Discussion<a class=headerlink href=#245-discussion title="Permanent link">&para;</a></h3> <p>While we have just scratched the surface of a deep topic, a number of concepts already come into focus: - First, the composition rules for differentiation can be applied routinely, enabling us to compute gradients automatically. This task requires no creativity and thus we can focus our cognitive powers elsewhere. - Second, computing the derivatives of vector-valued functions requires us to multiply matrices as we trace the dependency graph of variables from output to input. In particular, this graph is traversed in a forward direction when we evaluate a function and in a backwards direction when we compute gradients. Later chapters will formally introduce backpropagation, a computational procedure for applying the chain rule.</p> <p>From the viewpoint of optimization, gradients allow us to determine how to move the parameters of a model in order to lower the loss, and each step of the optimization algorithms used throughout this book will require calculating the gradient.</p> <h2 id=25-automatic-differentiation>2.5 Automatic Differentiation<a class=headerlink href=#25-automatic-differentiation title="Permanent link">&para;</a></h2> <h3 id=251-a-simple-example>2.5.1. A Simple Example<a class=headerlink href=#251-a-simple-example title="Permanent link">&para;</a></h3> <p>To get started we import the <code>autograd</code> package. Here, <code>autograd</code> stands for automatic differentiation.</p> <div class="tabbed-set tabbed-alternate" data-tabs=59:2><input checked=checked id=251-a-simple-example-pytorch name=__tabbed_59 type=radio><input id=251-a-simple-example-tensorflow name=__tabbed_59 type=radio><div class=tabbed-labels><label for=251-a-simple-example-pytorch>PYTORCH</label><label for=251-a-simple-example-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-227-1><a id=__codelineno-227-1 name=__codelineno-227-1 href=#__codelineno-227-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mf>4.0</span><span class=p>)</span>
</span><span id=__span-227-2><a id=__codelineno-227-2 name=__codelineno-227-2 href=#__codelineno-227-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-228-1><a id=__codelineno-228-1 name=__codelineno-228-1 href=#__codelineno-228-1></a>tensor([0., 1., 2., 3.], requires_grad=True)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-229-1><a id=__codelineno-229-1 name=__codelineno-229-1 href=#__codelineno-229-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mf>4.0</span><span class=p>)</span>
</span><span id=__span-229-2><a id=__codelineno-229-2 name=__codelineno-229-2 href=#__codelineno-229-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-230-1><a id=__codelineno-230-1 name=__codelineno-230-1 href=#__codelineno-230-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>Before we can compute gradients, we need to allocate memory for the gradients we wish to calculate.</p> <div class="tabbed-set tabbed-alternate" data-tabs=60:2><input checked=checked id=251-a-simple-example-pytorch_1 name=__tabbed_60 type=radio><input id=251-a-simple-example-tensorflow_1 name=__tabbed_60 type=radio><div class=tabbed-labels><label for=251-a-simple-example-pytorch_1>PYTORCH</label><label for=251-a-simple-example-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-231-1><a id=__codelineno-231-1 name=__codelineno-231-1 href=#__codelineno-231-1></a><span class=n>x</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>  <span class=c1># Same as `x = torch.arange(4.0, requires_grad=True)`</span>
</span><span id=__span-231-2><a id=__codelineno-231-2 name=__codelineno-231-2 href=#__codelineno-231-2></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span>  <span class=c1># The default value is None</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-232-1><a id=__codelineno-232-1 name=__codelineno-232-1 href=#__codelineno-232-1></a>tensor([0., 1., 2., 3.], requires_grad=True)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-233-1><a id=__codelineno-233-1 name=__codelineno-233-1 href=#__codelineno-233-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-233-2><a id=__codelineno-233-2 name=__codelineno-233-2 href=#__codelineno-233-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-234-1><a id=__codelineno-234-1 name=__codelineno-234-1 href=#__codelineno-234-1></a>&lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>Now let's compute the sum of the squares of <code>x</code>.</p> <div class="tabbed-set tabbed-alternate" data-tabs=61:2><input checked=checked id=251-a-simple-example-pytorch_2 name=__tabbed_61 type=radio><input id=251-a-simple-example-tensorflow_2 name=__tabbed_61 type=radio><div class=tabbed-labels><label for=251-a-simple-example-pytorch_2>PYTORCH</label><label for=251-a-simple-example-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-235-1><a id=__codelineno-235-1 name=__codelineno-235-1 href=#__codelineno-235-1></a><span class=n>y</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span><span id=__span-235-2><a id=__codelineno-235-2 name=__codelineno-235-2 href=#__codelineno-235-2></a><span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-236-1><a id=__codelineno-236-1 name=__codelineno-236-1 href=#__codelineno-236-1></a>tensor(28., grad_fn=&lt;MulBackward0&gt;)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-237-1><a id=__codelineno-237-1 name=__codelineno-237-1 href=#__codelineno-237-1></a><span class=n>y</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>tf</span><span class=o>.</span><span class=n>tensordot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>axes</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-237-2><a id=__codelineno-237-2 name=__codelineno-237-2 href=#__codelineno-237-2></a><span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-238-1><a id=__codelineno-238-1 name=__codelineno-238-1 href=#__codelineno-238-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=28.0&gt;
</span></code></pre></div> </div> </div> </div> <p>We can now take the gradient of y with respect to x by calling its backward method. Next, we can access the gradient via x‚Äôs grad attribute.</p> <div class="tabbed-set tabbed-alternate" data-tabs=62:2><input checked=checked id=251-a-simple-example-pytorch_3 name=__tabbed_62 type=radio><input id=251-a-simple-example-tensorflow_3 name=__tabbed_62 type=radio><div class=tabbed-labels><label for=251-a-simple-example-pytorch_3>PYTORCH</label><label for=251-a-simple-example-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-239-1><a id=__codelineno-239-1 name=__codelineno-239-1 href=#__codelineno-239-1></a><span class=n>y</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-239-2><a id=__codelineno-239-2 name=__codelineno-239-2 href=#__codelineno-239-2></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-240-1><a id=__codelineno-240-1 name=__codelineno-240-1 href=#__codelineno-240-1></a>tensor([ 0.,  4.,  8., 12.])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-241-1><a id=__codelineno-241-1 name=__codelineno-241-1 href=#__codelineno-241-1></a><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>t</span><span class=p>:</span>
</span><span id=__span-241-2><a id=__codelineno-241-2 name=__codelineno-241-2 href=#__codelineno-241-2></a>    <span class=n>y</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>tf</span><span class=o>.</span><span class=n>tensordot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>axes</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-241-3><a id=__codelineno-241-3 name=__codelineno-241-3 href=#__codelineno-241-3></a><span class=n>x_grad</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span><span id=__span-241-4><a id=__codelineno-241-4 name=__codelineno-241-4 href=#__codelineno-241-4></a><span class=n>x_grad</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-242-1><a id=__codelineno-242-1 name=__codelineno-242-1 href=#__codelineno-242-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=252-backward-for-non-scalar-variables>2.5.2. Backward for Non-Scalar Variables<a class=headerlink href=#252-backward-for-non-scalar-variables title="Permanent link">&para;</a></h3> <p>When y is a vector, the most natural representation of the derivative of y with respect to a vector x is a matrix called the Jacobian that contains the partial derivatives of each component of y with respect to each component of x. Likewise, for higher-order y and x, the result of differentiation could be an even higher-order tensor.</p> <p>While Jacobians do show up in some advanced machine learning techniques, more commonly we want to sum up the gradients of each component of y with respect to the full vector x, yielding a vector of the same shape as x. For example, we often have a vector representing the value of our loss function calculated separately for each example among a batch of training examples. Here, we just want to sum up the gradients computed individually for each example.</p> <div class="tabbed-set tabbed-alternate" data-tabs=63:2><input checked=checked id=252-backward-for-non-scalar-variables-pytorch name=__tabbed_63 type=radio><input id=252-backward-for-non-scalar-variables-tensorflow name=__tabbed_63 type=radio><div class=tabbed-labels><label for=252-backward-for-non-scalar-variables-pytorch>PYTORCH</label><label for=252-backward-for-non-scalar-variables-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-243-1><a id=__codelineno-243-1 name=__codelineno-243-1 href=#__codelineno-243-1></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span><span id=__span-243-2><a id=__codelineno-243-2 name=__codelineno-243-2 href=#__codelineno-243-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span><span id=__span-243-3><a id=__codelineno-243-3 name=__codelineno-243-3 href=#__codelineno-243-3></a><span class=n>y</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-243-4><a id=__codelineno-243-4 name=__codelineno-243-4 href=#__codelineno-243-4></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-244-1><a id=__codelineno-244-1 name=__codelineno-244-1 href=#__codelineno-244-1></a>tensor([1., 1., 1., 1.])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-245-1><a id=__codelineno-245-1 name=__codelineno-245-1 href=#__codelineno-245-1></a><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>t</span><span class=p>:</span>
</span><span id=__span-245-2><a id=__codelineno-245-2 name=__codelineno-245-2 href=#__codelineno-245-2></a>    <span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-245-3><a id=__codelineno-245-3 name=__codelineno-245-3 href=#__codelineno-245-3></a><span class=n>x_grad</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span><span id=__span-245-4><a id=__codelineno-245-4 name=__codelineno-245-4 href=#__codelineno-245-4></a><span class=n>x_grad</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-246-1><a id=__codelineno-246-1 name=__codelineno-246-1 href=#__codelineno-246-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=253-detaching-computation>2.5.3. Detaching Computation<a class=headerlink href=#253-detaching-computation title="Permanent link">&para;</a></h3> <p>Sometimes, we wish to move some calculations outside of the recorded computational graph. For example, say that y is a function of x, and that we wish to compute the gradient of y with respect to x, but that we plan to use the value of y, say y_val later on, without ever computing further derivatives of y with respect to x.</p> <div class="tabbed-set tabbed-alternate" data-tabs=64:4><input checked=checked id=253-detaching-computation-pytorch name=__tabbed_64 type=radio><input id=253-detaching-computation-tensorflow name=__tabbed_64 type=radio><input id=253-detaching-computation-pytorch_1 name=__tabbed_64 type=radio><input id=253-detaching-computation-tensorflow_1 name=__tabbed_64 type=radio><div class=tabbed-labels><label for=253-detaching-computation-pytorch>PYTORCH</label><label for=253-detaching-computation-tensorflow>TENSORFLOW</label><label for=253-detaching-computation-pytorch_1>PYTORCH</label><label for=253-detaching-computation-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-247-1><a id=__codelineno-247-1 name=__codelineno-247-1 href=#__codelineno-247-1></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span><span id=__span-247-2><a id=__codelineno-247-2 name=__codelineno-247-2 href=#__codelineno-247-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-247-3><a id=__codelineno-247-3 name=__codelineno-247-3 href=#__codelineno-247-3></a><span class=n>u</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span><span id=__span-247-4><a id=__codelineno-247-4 name=__codelineno-247-4 href=#__codelineno-247-4></a><span class=n>z</span> <span class=o>=</span> <span class=n>u</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-247-5><a id=__codelineno-247-5 name=__codelineno-247-5 href=#__codelineno-247-5></a>
</span><span id=__span-247-6><a id=__codelineno-247-6 name=__codelineno-247-6 href=#__codelineno-247-6></a><span class=n>z</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-247-7><a id=__codelineno-247-7 name=__codelineno-247-7 href=#__codelineno-247-7></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span> <span class=o>==</span> <span class=n>u</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-248-1><a id=__codelineno-248-1 name=__codelineno-248-1 href=#__codelineno-248-1></a>tensor([True, True, True, True])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-249-1><a id=__codelineno-249-1 name=__codelineno-249-1 href=#__codelineno-249-1></a><span class=c1># Set persistent=True to preserve the compute graph.</span>
</span><span id=__span-249-2><a id=__codelineno-249-2 name=__codelineno-249-2 href=#__codelineno-249-2></a><span class=c1># This lets us run t.gradient more than once</span>
</span><span id=__span-249-3><a id=__codelineno-249-3 name=__codelineno-249-3 href=#__codelineno-249-3></a><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>(</span><span class=n>persistent</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=k>as</span> <span class=n>t</span><span class=p>:</span>
</span><span id=__span-249-4><a id=__codelineno-249-4 name=__codelineno-249-4 href=#__codelineno-249-4></a>    <span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-249-5><a id=__codelineno-249-5 name=__codelineno-249-5 href=#__codelineno-249-5></a>    <span class=n>u</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>stop_gradient</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span><span id=__span-249-6><a id=__codelineno-249-6 name=__codelineno-249-6 href=#__codelineno-249-6></a>    <span class=n>z</span> <span class=o>=</span> <span class=n>u</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-249-7><a id=__codelineno-249-7 name=__codelineno-249-7 href=#__codelineno-249-7></a>
</span><span id=__span-249-8><a id=__codelineno-249-8 name=__codelineno-249-8 href=#__codelineno-249-8></a><span class=n>x_grad</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span><span id=__span-249-9><a id=__codelineno-249-9 name=__codelineno-249-9 href=#__codelineno-249-9></a><span class=n>x_grad</span> <span class=o>==</span> <span class=n>u</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-250-1><a id=__codelineno-250-1 name=__codelineno-250-1 href=#__codelineno-250-1></a>&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])&gt;
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-251-1><a id=__codelineno-251-1 name=__codelineno-251-1 href=#__codelineno-251-1></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span><span id=__span-251-2><a id=__codelineno-251-2 name=__codelineno-251-2 href=#__codelineno-251-2></a><span class=n>y</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-251-3><a id=__codelineno-251-3 name=__codelineno-251-3 href=#__codelineno-251-3></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span> <span class=o>==</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-252-1><a id=__codelineno-252-1 name=__codelineno-252-1 href=#__codelineno-252-1></a>tensor([True, True, True, True])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-253-1><a id=__codelineno-253-1 name=__codelineno-253-1 href=#__codelineno-253-1></a><span class=n>x_grad</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>y</span><span class=o>.</span><span class=n>sum</span><span class=p>(),</span> <span class=n>x</span><span class=p>)</span>
</span><span id=__span-253-2><a id=__codelineno-253-2 name=__codelineno-253-2 href=#__codelineno-253-2></a><span class=n>x_grad</span> <span class=o>==</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-254-1><a id=__codelineno-254-1 name=__codelineno-254-1 href=#__codelineno-254-1></a>&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=254-computing-the-gradient-of-python-control-flow>2.5.4. Computing the Gradient of Python Control Flow<a class=headerlink href=#254-computing-the-gradient-of-python-control-flow title="Permanent link">&para;</a></h3> <p>One benefit of using automatic differentiation is that even if the computational graph of the function contains Python‚Äôs control flow (such as conditional and loops), we can still calculate the gradient of the resulting variable. Consider the following program:</p> <div class="tabbed-set tabbed-alternate" data-tabs=65:2><input checked=checked id=254-computing-the-gradient-of-python-control-flow-pytorch name=__tabbed_65 type=radio><input id=254-computing-the-gradient-of-python-control-flow-tensorflow name=__tabbed_65 type=radio><div class=tabbed-labels><label for=254-computing-the-gradient-of-python-control-flow-pytorch>PYTORCH</label><label for=254-computing-the-gradient-of-python-control-flow-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-255-1><a id=__codelineno-255-1 name=__codelineno-255-1 href=#__codelineno-255-1></a><span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>a</span><span class=p>):</span>
</span><span id=__span-255-2><a id=__codelineno-255-2 name=__codelineno-255-2 href=#__codelineno-255-2></a>    <span class=n>b</span> <span class=o>=</span> <span class=n>a</span> <span class=o>*</span> <span class=mi>2</span>
</span><span id=__span-255-3><a id=__codelineno-255-3 name=__codelineno-255-3 href=#__codelineno-255-3></a>    <span class=k>while</span> <span class=n>b</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span> <span class=o>&lt;</span> <span class=mi>1000</span><span class=p>:</span>
</span><span id=__span-255-4><a id=__codelineno-255-4 name=__codelineno-255-4 href=#__codelineno-255-4></a>        <span class=n>b</span> <span class=o>=</span> <span class=n>b</span> <span class=o>*</span> <span class=mi>2</span>
</span><span id=__span-255-5><a id=__codelineno-255-5 name=__codelineno-255-5 href=#__codelineno-255-5></a>    <span class=k>if</span> <span class=n>b</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-255-6><a id=__codelineno-255-6 name=__codelineno-255-6 href=#__codelineno-255-6></a>        <span class=n>c</span> <span class=o>=</span> <span class=n>b</span>
</span><span id=__span-255-7><a id=__codelineno-255-7 name=__codelineno-255-7 href=#__codelineno-255-7></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-255-8><a id=__codelineno-255-8 name=__codelineno-255-8 href=#__codelineno-255-8></a>        <span class=n>c</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>b</span>
</span><span id=__span-255-9><a id=__codelineno-255-9 name=__codelineno-255-9 href=#__codelineno-255-9></a>    <span class=k>return</span> <span class=n>c</span>
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-256-1><a id=__codelineno-256-1 name=__codelineno-256-1 href=#__codelineno-256-1></a><span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>a</span><span class=p>):</span>
</span><span id=__span-256-2><a id=__codelineno-256-2 name=__codelineno-256-2 href=#__codelineno-256-2></a>    <span class=n>b</span> <span class=o>=</span> <span class=n>a</span> <span class=o>*</span> <span class=mi>2</span>
</span><span id=__span-256-3><a id=__codelineno-256-3 name=__codelineno-256-3 href=#__codelineno-256-3></a>    <span class=k>while</span> <span class=n>tf</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>b</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mi>1000</span><span class=p>:</span>
</span><span id=__span-256-4><a id=__codelineno-256-4 name=__codelineno-256-4 href=#__codelineno-256-4></a>        <span class=n>b</span> <span class=o>=</span> <span class=n>b</span> <span class=o>*</span> <span class=mi>2</span>
</span><span id=__span-256-5><a id=__codelineno-256-5 name=__codelineno-256-5 href=#__codelineno-256-5></a>    <span class=k>if</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>b</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-256-6><a id=__codelineno-256-6 name=__codelineno-256-6 href=#__codelineno-256-6></a>        <span class=n>c</span> <span class=o>=</span> <span class=n>b</span>
</span><span id=__span-256-7><a id=__codelineno-256-7 name=__codelineno-256-7 href=#__codelineno-256-7></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-256-8><a id=__codelineno-256-8 name=__codelineno-256-8 href=#__codelineno-256-8></a>        <span class=n>c</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>b</span>
</span><span id=__span-256-9><a id=__codelineno-256-9 name=__codelineno-256-9 href=#__codelineno-256-9></a>    <span class=k>return</span> <span class=n>c</span>
</span></code></pre></div> </div> </div> </div> <p>Let us compute the gradient.</p> <div class="tabbed-set tabbed-alternate" data-tabs=66:2><input checked=checked id=254-computing-the-gradient-of-python-control-flow-pytorch_1 name=__tabbed_66 type=radio><input id=254-computing-the-gradient-of-python-control-flow-tensorflow_1 name=__tabbed_66 type=radio><div class=tabbed-labels><label for=254-computing-the-gradient-of-python-control-flow-pytorch_1>PYTORCH</label><label for=254-computing-the-gradient-of-python-control-flow-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-257-1><a id=__codelineno-257-1 name=__codelineno-257-1 href=#__codelineno-257-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=p>(),</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-257-2><a id=__codelineno-257-2 name=__codelineno-257-2 href=#__codelineno-257-2></a><span class=n>d</span> <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span><span id=__span-257-3><a id=__codelineno-257-3 name=__codelineno-257-3 href=#__codelineno-257-3></a><span class=n>d</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-257-4><a id=__codelineno-257-4 name=__codelineno-257-4 href=#__codelineno-257-4></a><span class=n>a</span><span class=o>.</span><span class=n>grad</span> <span class=o>==</span> <span class=n>d</span> <span class=o>/</span> <span class=n>a</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-258-1><a id=__codelineno-258-1 name=__codelineno-258-1 href=#__codelineno-258-1></a>tensor(True)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-259-1><a id=__codelineno-259-1 name=__codelineno-259-1 href=#__codelineno-259-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>()))</span>
</span><span id=__span-259-2><a id=__codelineno-259-2 name=__codelineno-259-2 href=#__codelineno-259-2></a><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>t</span><span class=p>:</span>
</span><span id=__span-259-3><a id=__codelineno-259-3 name=__codelineno-259-3 href=#__codelineno-259-3></a>    <span class=n>d</span> <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span><span id=__span-259-4><a id=__codelineno-259-4 name=__codelineno-259-4 href=#__codelineno-259-4></a><span class=n>d_grad</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>d</span><span class=p>,</span> <span class=n>a</span><span class=p>)</span>
</span><span id=__span-259-5><a id=__codelineno-259-5 name=__codelineno-259-5 href=#__codelineno-259-5></a><span class=n>d_grad</span> <span class=o>==</span> <span class=n>d</span> <span class=o>/</span> <span class=n>a</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-260-1><a id=__codelineno-260-1 name=__codelineno-260-1 href=#__codelineno-260-1></a>&lt;tf.Tensor: shape=(), dtype=bool, numpy=True&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=255-discussion>2.5.5. Discussion<a class=headerlink href=#255-discussion title="Permanent link">&para;</a></h3> <p>You have now gotten a taste of the power of automatic differentiation. The development of libraries for calculating derivatives both automatically and efficiently has been a massive productivity booster for deep learning practitioners, liberating them so they can focus on less menial. Moreover, autograd lets us design massive models for which pen and paper gradient computations would be prohibitively time consuming. Interestingly, while we use autograd to optimize models (in a statistical sense) the optimization of autograd libraries themselves (in a computational sense) is a rich subject of vital interest to framework designers. Here, tools from compilers and graph manipulation are leveraged to compute results in the most expedient and memory-efficient manner.</p> <p>For now, try to remember these basics: (i) attach gradients to those variables with respect to which we desire derivatives; (ii) record the computation of the target value; (iii) execute the backpropagation function; (iv) access the resulting gradient.</p> <h2 id=26-probability-and-statistics>2.6 Probability and Statistics<a class=headerlink href=#26-probability-and-statistics title="Permanent link">&para;</a></h2> <p>One way or another, machine learning is all about uncertainty. In supervised learning, we want to predict something unknown (the target) given something known (the features). Depending on our objective, we might attempt to predict the most likely value of the target. Or we might predict the value with the smallest expected distance from the target. And sometimes we wish not only to predict a specific value but to quantify our uncertainty. For example, given some features describing a patient, we might want to know how likely they are to suffer a heart attack in the next year. In unsupervised learning, we often care about uncertainty. To determine whether a set of measurements are anomalous, it helps to know how likely one is to observe values in a population of interest. Furthermore, in reinforcement learning, we wish to develop agents that act intelligently in various environments. This requires reasoning about how an environment might be expected to change and what rewards one might expect to encounter in response to each of the available actions.</p> <p>Probability is the mathematical field concerned with reasoning under uncertainty. Given a probabilistic model of some process, we can reason about the likelihood of various events. The use of probabilities to describe the frequencies of repeatable events (like coin tosses) is fairly uncontroversial. In fact, <strong>frequentist</strong> scholars adhere to an interpretation of probability that applies only to such repeatable events. By contrast <strong>Bayesian</strong> scholars use the language of probability more broadly to formalize reasoning under uncertainty. Bayesian probability is characterized by two unique features: <strong>(i) assigning degrees of belief to non-repeatable events, e.g., what is the probability that a dam will collapse?; and (ii) subjectivity.</strong> While Bayesian probability provides unambiguous rules for how one should update their beliefs in light of new evidence, it allows for different individuals to start off with different prior beliefs. Statistics helps us to reason backwards, starting off with collection and organization of data and backing out to what inferences we might draw about the process that generated the data. Whenever we analyze a dataset, hunting for patterns that we hope might characterize a broader population, we are employing statistical thinking. Many courses, majors, theses, careers, departments, companies, and institutions have been devoted to the study of probability and statistics. While this section only scratches the surface, we will provide the foundation that you need to begin building models.</p> <h3 id=261-basics>2.6.1. Basics<a class=headerlink href=#261-basics title="Permanent link">&para;</a></h3> <h3 id=262-advanced-topics>2.6.2 Advanced Topics<a class=headerlink href=#262-advanced-topics title="Permanent link">&para;</a></h3> <h3 id=263-random-variables>2.6.3 Random Variables<a class=headerlink href=#263-random-variables title="Permanent link">&para;</a></h3> <h3 id=264-multile-random-variables>2.6.4 Multile Random Variables<a class=headerlink href=#264-multile-random-variables title="Permanent link">&para;</a></h3> <h3 id=265-expectation>2.6.5 Expectation<a class=headerlink href=#265-expectation title="Permanent link">&para;</a></h3> <h3 id=266-variance>2.6.6 Variance<a class=headerlink href=#266-variance title="Permanent link">&para;</a></h3> <h3 id=267-summary>2.6.7 Summary<a class=headerlink href=#267-summary title="Permanent link">&para;</a></h3> <h2 id=27-documentation>2.7 Documentation<a class=headerlink href=#27-documentation title="Permanent link">&para;</a></h2> <h3 id=271-functions-and-classes-in-a-module>2.7.1. Functions and Classes in a Module<a class=headerlink href=#271-functions-and-classes-in-a-module title="Permanent link">&para;</a></h3> <h3 id=272-specific-functions-and-classes>2.7.2. Specific Functions and Classes<a class=headerlink href=#272-specific-functions-and-classes title="Permanent link">&para;</a></h3> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5 9v12H1V9h4m4 12a2 2 0 0 1-2-2V9c0-.55.22-1.05.59-1.41L14.17 1l1.06 1.06c.27.27.44.64.44 1.05l-.03.32L14.69 8H21a2 2 0 0 1 2 2v2c0 .26-.05.5-.14.73l-3.02 7.05C19.54 20.5 18.83 21 18 21H9m0-2h9.03L21 12v-2h-8.79l1.13-5.32L9 9.03V19Z"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 15V3h4v12h-4M15 3a2 2 0 0 1 2 2v10c0 .55-.22 1.05-.59 1.41L9.83 23l-1.06-1.06c-.27-.27-.44-.64-.44-1.06l.03-.31.95-4.57H3a2 2 0 0 1-2-2v-2c0-.26.05-.5.14-.73l3.02-7.05C4.46 3.5 5.17 3 6 3h9m0 2H5.97L3 12v2h8.78l-1.13 5.32L15 14.97V5Z"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve this page by using our <a href=... target=_blank rel=noopener>feedback form</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../CH1-INTRO/ch1-intro/ class="md-footer__link md-footer__link--prev" aria-label="Previous: CH1-INTRO"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 320 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> CH1-INTRO </div> </div> </a> <a href=../../CH3-LNN/ch3-lnn/ class="md-footer__link md-footer__link--next" aria-label="Next: CH3-LNN üéØ"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> CH3-LNN üéØ </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 320 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2023 ~ now | üöÄ Shuaiwen Cui (Shaun) </div> </div> <div class=md-social> <a href=http://www.cuishuaiwen.com/ target=_blank rel=noopener title=www.cuishuaiwen.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1V472c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2 0-3.3-.1-1.4.1-2.8.1-4.2.1H392c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9.1-2.8v-69.6H32c-18 0-32-14-32-32.1 0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7l255.4 224.5c8 7 12 15 11 24z"/></svg> </a> <a href=https://github.com/Shuaiwen-Cui target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://www.linkedin.com/in/shaun-shuaiwen-cui/ target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> <a href=https://www.researchgate.net/profile/Shuaiwen-Cui target=_blank rel=noopener title=www.researchgate.net class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M0 32v448h448V32H0zm262.2 334.4c-6.6 3-33.2 6-50-14.2-9.2-10.6-25.3-33.3-42.2-63.6-8.9 0-14.7 0-21.4-.6v46.4c0 23.5 6 21.2 25.8 23.9v8.1c-6.9-.3-23.1-.8-35.6-.8-13.1 0-26.1.6-33.6.8v-8.1c15.5-2.9 22-1.3 22-23.9V225c0-22.6-6.4-21-22-23.9V193c25.8 1 53.1-.6 70.9-.6 31.7 0 55.9 14.4 55.9 45.6 0 21.1-16.7 42.2-39.2 47.5 13.6 24.2 30 45.6 42.2 58.9 7.2 7.8 17.2 14.7 27.2 14.7v7.3zm22.9-135c-23.3 0-32.2-15.7-32.2-32.2V167c0-12.2 8.8-30.4 34-30.4s30.4 17.9 30.4 17.9l-10.7 7.2s-5.5-12.5-19.7-12.5c-7.9 0-19.7 7.3-19.7 19.7v26.8c0 13.4 6.6 23.3 17.9 23.3 14.1 0 21.5-10.9 21.5-26.8h-17.9v-10.7h30.4c0 20.5 4.7 49.9-34 49.9zm-116.5 44.7c-9.4 0-13.6-.3-20-.8v-69.7c6.4-.6 15-.6 22.5-.6 23.3 0 37.2 12.2 37.2 34.5 0 21.9-15 36.6-39.7 36.6z"/></svg> </a> <a href=https://orcid.org/0000-0003-4447-6687 target=_blank rel=noopener title=orcid.org class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M294.75 188.19h-45.92V342h47.47c67.62 0 83.12-51.34 83.12-76.91 0-41.64-26.54-76.9-84.67-76.9zM256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm-80.79 360.76h-29.84v-207.5h29.84zm-14.92-231.14a19.57 19.57 0 1 1 19.57-19.57 19.64 19.64 0 0 1-19.57 19.57zM300 369h-81V161.26h80.6c76.73 0 110.44 54.83 110.44 103.85C410 318.39 368.38 369 300 369z"/></svg> </a> <a href=https://twitter.com/ShuaiwenC target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../../assets/javascripts/workers/search.f886a092.min.js", "tags": {"Default": "default-tag", "Hardware": "hardware-tag", "Software": "software-tag"}, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../../assets/javascripts/bundle.d7c377c4.min.js></script> <script src=/CODING/PYTHON/Book1_Python-For-Beginners_ÁºñÁ®ã‰∏çÈöæ.pdf></script> <script src=../../../../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../../javascripts/embed-pdf.js></script> <script src=../../../../assets/javascripts/custom.2340dcd7.min.js></script> </body> </html>