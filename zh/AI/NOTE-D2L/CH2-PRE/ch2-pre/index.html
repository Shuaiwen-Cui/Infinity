<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to Shaun's rabbit hole. This site serves as a personal knowledge base for me to record my thoughts and ideas. It is also a place for me to share my knowledge and experience with the world. I hope you find something useful here."><meta name=author content="Shuaiwen Cui"><link href=https://localhost:8000/zh/AI/NOTE-D2L/CH2-PRE/ch2-pre/ rel=canonical><link href=../../CH1-INTRO/ch1-intro/ rel=prev><link href=../../CH3-LNN/ch3-lnn/ rel=next><link rel=alternate type=application/rss+xml title="RSS 订阅" href=../../../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="已更新内容的 RSS 订阅" href=../../../../../feed_rss_updated.xml><link rel=icon href=../../../../../static/images/logo.png><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.2"><title>CH2-预备知识 - Eureka!</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.50c56a3b.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><style>.md-tag--default-tag{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M0 80v149.5c0 17 6.7 33.3 18.7 45.3l176 176c25 25 65.5 25 90.5 0l133.5-133.5c25-25 25-65.5 0-90.5l-176-176c-12-12-28.3-18.7-45.3-18.7H48C21.5 32 0 53.5 0 80zm112 32a32 32 0 1 1 0 64 32 32 0 1 1 0-64z"/></svg>');}.md-tag--hardware-tag{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M176 24c0-13.3-10.7-24-24-24s-24 10.7-24 24v40c-35.3 0-64 28.7-64 64H24c-13.3 0-24 10.7-24 24s10.7 24 24 24h40v56H24c-13.3 0-24 10.7-24 24s10.7 24 24 24h40v56H24c-13.3 0-24 10.7-24 24s10.7 24 24 24h40c0 35.3 28.7 64 64 64v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40h56v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40h56v40c0 13.3 10.7 24 24 24s24-10.7 24-24v-40c35.3 0 64-28.7 64-64h40c13.3 0 24-10.7 24-24s-10.7-24-24-24h-40v-56h40c13.3 0 24-10.7 24-24s-10.7-24-24-24h-40v-56h40c13.3 0 24-10.7 24-24s-10.7-24-24-24h-40c0-35.3-28.7-64-64-64V24c0-13.3-10.7-24-24-24s-24 10.7-24 24v40h-56V24c0-13.3-10.7-24-24-24s-24 10.7-24 24v40h-56V24zm-16 104h192c17.7 0 32 14.3 32 32v192c0 17.7-14.3 32-32 32H160c-17.7 0-32-14.3-32-32V160c0-17.7 14.3-32 32-32zm192 32H160v192h192V160z"/></svg>');}.md-tag--software-tag{--md-tag-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M64 96c0-35.3 28.7-64 64-64h384c35.3 0 64 28.7 64 64v256h-64V96H128v256H64V96zM0 403.2C0 392.6 8.6 384 19.2 384h601.6c10.6 0 19.2 8.6 19.2 19.2 0 42.4-34.4 76.8-76.8 76.8H76.8C34.4 480 0 445.6 0 403.2zM281 209l-31 31 31 31c9.4 9.4 9.4 24.6 0 33.9s-24.6 9.4-33.9 0l-48-48c-9.4-9.4-9.4-24.6 0-33.9l48-48c9.4-9.4 24.6-9.4 33.9 0s9.4 24.6 0 33.9zm112-34 48 48c9.4 9.4 9.4 24.6 0 33.9l-48 48c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l31-31-31-31c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0z"/></svg>');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config",""),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../../../../../assets/stylesheets/custom.00c04c01.min.css></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#ch02- class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label=不再显示此消息> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> For updates follow <strong>@ShuaiwenCui</strong> on <a href=https://www.linkedin.com/in/shaun-shuaiwen-cui/ rel=me> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </span> <strong>Linkedin</strong> </a> and <a href=https://twitter.com/ShuaiwenC> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../../../../ title=Eureka! class="md-header__button md-logo" aria-label=Eureka! data-md-component=logo> <img src=../../../../../static/images/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Eureka! </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> CH2-预备知识 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8v2m9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1 0 1.71-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5Z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <div class=md-header__option> <div class=md-select> <button class="md-header__button md-icon" aria-label=选择当前语言> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.52 17.52 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04M18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12m-2.62 7 1.62-4.33L19.12 17h-3.24Z"/></svg> </button> <div class=md-select__inner> <ul class=md-select__list> <li class=md-select__item> <a href=../../../../../AI/NOTE-D2L/CH2-PRE/ch2-pre/ hreflang=en class=md-select__link> English </a> </li> <li class=md-select__item> <a href=./ hreflang=zh class=md-select__link> 简体中文 </a> </li> </ul> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 320 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/Shuaiwen-Cui/Infinity.git/ title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> Shuaiwen-Cui/Infinity </div> </a> </div> </nav> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../../ class=md-tabs__link> 首页 </a> </li> <li class=md-tabs__item> <a href=../../../../MATH/math/ class=md-tabs__link> 数学 </a> </li> <li class=md-tabs__item> <a href=../../../../CS/CS/ class=md-tabs__link> 计算机科学 </a> </li> <li class=md-tabs__item> <a href=../../../../CODING/coding/ class=md-tabs__link> 编程 </a> </li> <li class=md-tabs__item> <a href=../../../../EMBEDDED-SYS/embedded-sys/ class=md-tabs__link> 嵌入式系统 </a> </li> <li class=md-tabs__item> <a href=../../../../DSP/dsp/ class=md-tabs__link> 数字信号处理 </a> </li> <li class=md-tabs__item> <a href=../../../../PERCEPTION/perception/ class=md-tabs__link> 感知 </a> </li> <li class=md-tabs__item> <a href=../../../../CONTROL/control/ class=md-tabs__link> 控制 </a> </li> <li class=md-tabs__item> <a href=../../../../ACTUATION/actuation/ class=md-tabs__link> 执行 </a> </li> <li class=md-tabs__item> <a href=../../../../IOT/iot/ class=md-tabs__link> 物联网 </a> </li> <li class=md-tabs__item> <a href=../../../../CLOUD/cloud/ class=md-tabs__link> 云 </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../BASICS/basics/ class=md-tabs__link> 人工智能 </a> </li> <li class=md-tabs__item> <a href=../../../../SHM/shm/ class=md-tabs__link> 结构健康监测 </a> </li> <li class=md-tabs__item> <a href=../../../../DEV/DEVENV/LATEX/latex/ class=md-tabs__link> 开发 </a> </li> <li class=md-tabs__item> <a href=../../../../RESEARCH/research/ class=md-tabs__link> 研究 </a> </li> <li class=md-tabs__item> <a href=../../../../PROJECT/project/ class=md-tabs__link> 项目 </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../ title=Eureka! class="md-nav__button md-logo" aria-label=Eureka! data-md-component=logo> <img src=../../../../../static/images/logo.png alt=logo> </a> Eureka! </label> <div class=md-nav__source> <a href=https://github.com/Shuaiwen-Cui/Infinity.git/ title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> Shuaiwen-Cui/Infinity </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../../../../ class="md-nav__link "> <span class=md-ellipsis> 首页 </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> 首页 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../HOME/about/ class=md-nav__link> <span class=md-ellipsis> 关于 </span> </a> </li> <li class=md-nav__item> <a href=../../../../HOME/sponsorship/ class=md-nav__link> <span class=md-ellipsis> 赞助 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> 数学 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> 数学 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../MATH/math/ class=md-nav__link> <span class=md-ellipsis> Math </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> 几何与图形学 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> 几何与图形学 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../MATH/GEOMETRY-GRAPHICS/COMPUTER-GRAPHICS/computer-graphics/ class=md-nav__link> <span class=md-ellipsis> 计算机图形学从零开始 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> 计算机科学 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> 计算机科学 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CS/CS/ class=md-nav__link> <span class=md-ellipsis> CS </span> </a> </li> <li class=md-nav__item> <a href=../../../../CS/OS/os/ class=md-nav__link> <span class=md-ellipsis> 操作系统 </span> </a> </li> <li class=md-nav__item> <a href=../../../../CS/DATA-BASE/data-base/ class=md-nav__link> <span class=md-ellipsis> 数据库 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> 编程 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> 编程 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CODING/coding/ class=md-nav__link> <span class=md-ellipsis> 编程 </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex> <span class=md-ellipsis> C/C++ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> C/C++ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C/c/ class=md-nav__link> <span class=md-ellipsis> C </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C-NOTES/c-notes/ class=md-nav__link> <span class=md-ellipsis> C笔记 </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/C-C%2B%2B/C%2B%2B/c%2B%2B/ class=md-nav__link> <span class=md-ellipsis> C++ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex> <span class=md-ellipsis> Python </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CODING/PYTHON/python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/PYTHON/ROADMAP/roadmap/ class=md-nav__link> <span class=md-ellipsis> 路线图 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex> <span class=md-ellipsis> HTML/CSS/JS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> HTML/CSS/JS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CODING/HTML-CSS-JS/HTML/html/ class=md-nav__link> <span class=md-ellipsis> 超文本标记语言 </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/HTML-CSS-JS/CSS/css/ class=md-nav__link> <span class=md-ellipsis> 级联样式表 </span> </a> </li> <li class=md-nav__item> <a href=../../../../CODING/HTML-CSS-JS/JS/js/ class=md-nav__link> <span class=md-ellipsis> JAVASCRIPT </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> 嵌入式系统 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> 嵌入式系统 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/embedded-sys/ class=md-nav__link> <span class=md-ellipsis> 嵌入式系统 </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex> <span class=md-ellipsis> 理论 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> 理论 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/THEORY/ELECTRONICS/electronics/ class=md-nav__link> <span class=md-ellipsis> 电子学 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/THEORY/CIRCUITS/circuits/ class=md-nav__link> <span class=md-ellipsis> 电路 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/THEORY/FREERTOS/freertos/ class=md-nav__link> <span class=md-ellipsis> 实时操作系统 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/THEORY/LINUX/linux/ class=md-nav__link> <span class=md-ellipsis> LINUX </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/SIMULATION/simulation/ class=md-nav__link> <span class=md-ellipsis> 仿真 </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex> <span class=md-ellipsis> 开发板 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> 开发板 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/ARDUINO/arduino/ class=md-nav__link> <span class=md-ellipsis> ARDUINO </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/STM32/stm32/ class=md-nav__link> <span class=md-ellipsis> STM32 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/ESP32/esp32/ class=md-nav__link> <span class=md-ellipsis> ESP32 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/RASPBERRYPI/raspberrypi/ class=md-nav__link> <span class=md-ellipsis> 树莓派 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/DEVKIT/JETSON/jetson/ class=md-nav__link> <span class=md-ellipsis> JETSON </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex> <span class=md-ellipsis> 集成开发环境 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> 集成开发环境 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/ide/ class=md-nav__link> <span class=md-ellipsis> 嵌入式系统集成开发环境 </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5_2> <label class=md-nav__link for=__nav_5_5_2 id=__nav_5_5_2_label tabindex=0> <span class=md-ellipsis> KEIL </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5_2> <span class="md-nav__icon md-icon"></span> KEIL </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/keil/ class=md-nav__link> <span class=md-ellipsis> Keil IDE </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5_2_2> <label class=md-nav__link for=__nav_5_5_2_2 id=__nav_5_5_2_2_label tabindex=0> <span class=md-ellipsis> 官方指南 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_5_5_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5_2_2> <span class="md-nav__icon md-icon"></span> 官方指南 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE/PREFACE/preface/ class=md-nav__link> <span class=md-ellipsis> 前言 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE/MDK-INTRODUCTION/mdk-introduction/ class=md-nav__link> <span class=md-ellipsis> MDK简介 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE/CMSIS/cmsis/ class=md-nav__link> <span class=md-ellipsis> CMSIS库 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE/SOFTWARE-COMPONENTS/software-components/ class=md-nav__link> <span class=md-ellipsis> 软件组件 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE/CREATE-APP/create-app/ class=md-nav__link> <span class=md-ellipsis> 创建应用 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE/DEBUG-APP/debug-app/ class=md-nav__link> <span class=md-ellipsis> 调试应用 </span> </a> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/KEIL/GUIDE/MDK-MIDDLEWARE/mdk-middleware/ class=md-nav__link> <span class=md-ellipsis> MDK中间件 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../EMBEDDED-SYS/IDE/PLATFORMIO/platformio/ class=md-nav__link> <span class=md-ellipsis> PLATFORMIO </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex> <span class=md-ellipsis> 数字信号处理 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> 数字信号处理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DSP/dsp/ class=md-nav__link> <span class=md-ellipsis> 数字信号处理 </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2> <label class=md-nav__link for=__nav_6_2 id=__nav_6_2_label tabindex> <span class=md-ellipsis> “DSP相关内容”笔记 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2> <span class="md-nav__icon md-icon"></span> “DSP相关内容”笔记 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2_1> <label class=md-nav__link for=__nav_6_2_1 id=__nav_6_2_1_label tabindex=0> <span class=md-ellipsis> 时频分析 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_6_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2_1> <span class="md-nav__icon md-icon"></span> 时频分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH01/tf-ana-ch01/ class=md-nav__link> <span class=md-ellipsis> CH01 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH02/tf-ana-ch02/ class=md-nav__link> <span class=md-ellipsis> CH02 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH03/tf-ana-ch03/ class=md-nav__link> <span class=md-ellipsis> CH03 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH04/tf-ana-ch04/ class=md-nav__link> <span class=md-ellipsis> CH04 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH05/tf-ana-ch05/ class=md-nav__link> <span class=md-ellipsis> CH05 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DSP/NOTE-DSP-STUFF/TIME-FREQ-ANA/CH06/tf-ana-ch06/ class=md-nav__link> <span class=md-ellipsis> CH06 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex> <span class=md-ellipsis> 感知 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> 感知 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../PERCEPTION/perception/ class=md-nav__link> <span class=md-ellipsis> 感知 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_8> <label class=md-nav__link for=__nav_8 id=__nav_8_label tabindex> <span class=md-ellipsis> 控制 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_8_label aria-expanded=false> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> 控制 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CONTROL/control/ class=md-nav__link> <span class=md-ellipsis> 控制 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9> <label class=md-nav__link for=__nav_9 id=__nav_9_label tabindex> <span class=md-ellipsis> 执行 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_9_label aria-expanded=false> <label class=md-nav__title for=__nav_9> <span class="md-nav__icon md-icon"></span> 执行 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/actuation/ class=md-nav__link> <span class=md-ellipsis> 执行 </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_2> <label class=md-nav__link for=__nav_9_2 id=__nav_9_2_label tabindex> <span class=md-ellipsis> 机器人操作系统 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_9_2_label aria-expanded=false> <label class=md-nav__title for=__nav_9_2> <span class="md-nav__icon md-icon"></span> 机器人操作系统 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/ROS/ros/ class=md-nav__link> <span class=md-ellipsis> 机器人操作系统 (ROS) </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_2_2> <label class=md-nav__link for=__nav_9_2_2 id=__nav_9_2_2_label tabindex=0> <span class=md-ellipsis> 官方指南 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_9_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_9_2_2> <span class="md-nav__icon md-icon"></span> 官方指南 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/ROS/OFFICIAL-GUIDE/INSTALLATION/installation/ class=md-nav__link> <span class=md-ellipsis> 安装 </span> </a> </li> <li class=md-nav__item> <a href=../../../../ACTUATION/ROS/OFFICIAL-GUIDE/DISTRIBUTION/distribution/ class=md-nav__link> <span class=md-ellipsis> 发行版 </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_2_2_3> <label class=md-nav__link for=__nav_9_2_2_3 id=__nav_9_2_2_3_label tabindex=0> <span class=md-ellipsis> 教程 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_9_2_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_9_2_2_3> <span class="md-nav__icon md-icon"></span> 教程 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/ROS/OFFICIAL-GUIDE/TUTORIALS/CLI-TOOLS/cli-tools/ class=md-nav__link> <span class=md-ellipsis> 命令行工具 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_3> <label class=md-nav__link for=__nav_9_3 id=__nav_9_3_label tabindex> <span class=md-ellipsis> 动手学ROS </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_9_3_label aria-expanded=false> <label class=md-nav__title for=__nav_9_3> <span class="md-nav__icon md-icon"></span> 动手学ROS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../ACTUATION/HANDSON-ROS/handson-ros/ class=md-nav__link> <span class=md-ellipsis> 动手学ROS </span> </a> </li> <li class=md-nav__item> <a href=../../../../ACTUATION/HANDSON-ROS/P1-GET-STARTED/p1-get-started/ class=md-nav__link> <span class=md-ellipsis> P1-入门 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_10> <label class=md-nav__link for=__nav_10 id=__nav_10_label tabindex> <span class=md-ellipsis> 物联网 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_10_label aria-expanded=false> <label class=md-nav__title for=__nav_10> <span class="md-nav__icon md-icon"></span> 物联网 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../IOT/iot/ class=md-nav__link> <span class=md-ellipsis> 物联网 </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_10_2> <label class=md-nav__link for=__nav_10_2 id=__nav_10_2_label tabindex> <span class=md-ellipsis> 无线传感器网络 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_10_2_label aria-expanded=false> <label class=md-nav__title for=__nav_10_2> <span class="md-nav__icon md-icon"></span> 无线传感器网络 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../IOT/WSN/wsn/ class=md-nav__link> <span class=md-ellipsis> 无线传感网络 (WSN) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_10_3> <label class=md-nav__link for=__nav_10_3 id=__nav_10_3_label tabindex> <span class=md-ellipsis> 消息队列遥测传输协议 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_10_3_label aria-expanded=false> <label class=md-nav__title for=__nav_10_3> <span class="md-nav__icon md-icon"></span> 消息队列遥测传输协议 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../IOT/MQTT/mqtt/ class=md-nav__link> <span class=md-ellipsis> 消息队列遥测传输 (MQTT) </span> </a> </li> <li class=md-nav__item> <a href=../../../../IOT/MQTT/BROKER/broker/ class=md-nav__link> <span class=md-ellipsis> 代理 </span> </a> </li> <li class=md-nav__item> <a href=../../../../IOT/MQTT/CLIENT/client/ class=md-nav__link> <span class=md-ellipsis> 客户端 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_11> <label class=md-nav__link for=__nav_11 id=__nav_11_label tabindex> <span class=md-ellipsis> 云 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_11_label aria-expanded=false> <label class=md-nav__title for=__nav_11> <span class="md-nav__icon md-icon"></span> 云 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CLOUD/cloud/ class=md-nav__link> <span class=md-ellipsis> 云 </span> </a> </li> <li class=md-nav__item> <a href=../../../../CLOUD/CLOUD-TECH/cloud-tech/ class=md-nav__link> <span class=md-ellipsis> 云技术 </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_11_3> <label class=md-nav__link for=__nav_11_3 id=__nav_11_3_label tabindex> <span class=md-ellipsis> 上手实践 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_11_3_label aria-expanded=false> <label class=md-nav__title for=__nav_11_3> <span class="md-nav__icon md-icon"></span> 上手实践 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../CLOUD/HANDS-ON/001-HAVE-A-SERVER/have-a-server/ class=md-nav__link> <span class=md-ellipsis> 拥有一台服务器 </span> </a> </li> <li class=md-nav__item> <a href=../../../../CLOUD/HANDS-ON/002-SERVER-CONFIG/server-config/ class=md-nav__link> <span class=md-ellipsis> 服务器配置 </span> </a> </li> <li class=md-nav__item> <a href=../../../../CLOUD/HANDS-ON/003-DOMAIN-NAME/domain-name/ class=md-nav__link> <span class=md-ellipsis> 获得一个域名 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_12 checked> <label class=md-nav__link for=__nav_12 id=__nav_12_label tabindex> <span class=md-ellipsis> 人工智能 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_12_label aria-expanded=true> <label class=md-nav__title for=__nav_12> <span class="md-nav__icon md-icon"></span> 人工智能 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../BASICS/basics/ class=md-nav__link> <span class=md-ellipsis> 基础 </span> </a> </li> <li class=md-nav__item> <a href=../../../FRAMEWORKS/frameworks/ class=md-nav__link> <span class=md-ellipsis> 框架 </span> </a> </li> <li class=md-nav__item> <a href=../../../MLP/mlp/ class=md-nav__link> <span class=md-ellipsis> 多层感知机 </span> </a> </li> <li class=md-nav__item> <a href=../../../CNN/cnn/ class=md-nav__link> <span class=md-ellipsis> 卷积神经网络 </span> </a> </li> <li class=md-nav__item> <a href=../../../RNN/rnn/ class=md-nav__link> <span class=md-ellipsis> 循环神经网络 </span> </a> </li> <li class=md-nav__item> <a href=../../../TRANSFORMER/transformer/ class=md-nav__link> <span class=md-ellipsis> “变形金刚” </span> </a> </li> <li class=md-nav__item> <a href=../../../GAN/gan/ class=md-nav__link> <span class=md-ellipsis> 生成对抗网络 </span> </a> </li> <li class=md-nav__item> <a href=../../../RL/rl/ class=md-nav__link> <span class=md-ellipsis> 强化学习 </span> </a> </li> <li class=md-nav__item> <a href=../../../FL/fl/ class=md-nav__link> <span class=md-ellipsis> 联邦学习 </span> </a> </li> <li class=md-nav__item> <a href=../../../CV/cv/ class=md-nav__link> <span class=md-ellipsis> 计算机视觉 </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_12_11 checked> <label class=md-nav__link for=__nav_12_11 id=__nav_12_11_label tabindex> <span class=md-ellipsis> 动手学深度学习 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_12_11_label aria-expanded=true> <label class=md-nav__title for=__nav_12_11> <span class="md-nav__icon md-icon"></span> 动手学深度学习 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CH1-INTRO/ch1-intro/ class=md-nav__link> <span class=md-ellipsis> CH1-引言 </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> CH2-预备知识 </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> CH2-预备知识 </span> </a> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#21 class=md-nav__link> <span class=md-ellipsis> 2.1 数据操作 </span> </a> <nav class=md-nav aria-label="2.1 数据操作"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#211 class=md-nav__link> <span class=md-ellipsis> 2.1.1 入门 </span> </a> </li> <li class=md-nav__item> <a href=#212 class=md-nav__link> <span class=md-ellipsis> 2.1.2 运算符 </span> </a> </li> <li class=md-nav__item> <a href=#213 class=md-nav__link> <span class=md-ellipsis> 2.1.3 广播机制 </span> </a> </li> <li class=md-nav__item> <a href=#214 class=md-nav__link> <span class=md-ellipsis> 2.1.4 索引和切片 </span> </a> </li> <li class=md-nav__item> <a href=#215 class=md-nav__link> <span class=md-ellipsis> 2.1.5 节省内存 </span> </a> </li> <li class=md-nav__item> <a href=#216-python class=md-nav__link> <span class=md-ellipsis> 2.1.6 转换为其他 Python 对象 </span> </a> </li> <li class=md-nav__item> <a href=#217 class=md-nav__link> <span class=md-ellipsis> 2.1.7 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#22 class=md-nav__link> <span class=md-ellipsis> 2.2 数据预处理 </span> </a> <nav class=md-nav aria-label="2.2 数据预处理"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#221 class=md-nav__link> <span class=md-ellipsis> 2.2.1 读取数据集 </span> </a> </li> <li class=md-nav__item> <a href=#222 class=md-nav__link> <span class=md-ellipsis> 2.2.2 处理缺失值 </span> </a> </li> <li class=md-nav__item> <a href=#223 class=md-nav__link> <span class=md-ellipsis> 2.2.3 转换为张量格式 </span> </a> </li> <li class=md-nav__item> <a href=#224 class=md-nav__link> <span class=md-ellipsis> 2.2.4 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#23 class=md-nav__link> <span class=md-ellipsis> 2.3 线性代数 </span> </a> <nav class=md-nav aria-label="2.3 线性代数"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#231 class=md-nav__link> <span class=md-ellipsis> 2.3.1 标量 </span> </a> </li> <li class=md-nav__item> <a href=#232 class=md-nav__link> <span class=md-ellipsis> 2.3.2 向量 </span> </a> <nav class=md-nav aria-label="2.3.2 向量"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#2321 class=md-nav__link> <span class=md-ellipsis> 2.3.2.1 长度、维度和形状 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#233 class=md-nav__link> <span class=md-ellipsis> 2.3.3 矩阵 </span> </a> </li> <li class=md-nav__item> <a href=#234 class=md-nav__link> <span class=md-ellipsis> 2.3.4 张量 </span> </a> </li> <li class=md-nav__item> <a href=#235 class=md-nav__link> <span class=md-ellipsis> 2.3.5 张量算法的基本性质 </span> </a> </li> <li class=md-nav__item> <a href=#236 class=md-nav__link> <span class=md-ellipsis> 2.3.6 降维 </span> </a> <nav class=md-nav aria-label="2.3.6 降维"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#2361 class=md-nav__link> <span class=md-ellipsis> 2.3.6.1 非降维求和 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#237 class=md-nav__link> <span class=md-ellipsis> 2.3.7 点积 </span> </a> </li> <li class=md-nav__item> <a href=#238- class=md-nav__link> <span class=md-ellipsis> 2.3.8 矩阵-向量积 </span> </a> </li> <li class=md-nav__item> <a href=#239- class=md-nav__link> <span class=md-ellipsis> 2.3.9 矩阵-矩阵乘法 </span> </a> </li> <li class=md-nav__item> <a href=#2310 class=md-nav__link> <span class=md-ellipsis> 2.3.10 范数 </span> </a> <nav class=md-nav aria-label="2.3.10 范数"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#23101 class=md-nav__link> <span class=md-ellipsis> 2.3.10.1 范数和目标 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2311 class=md-nav__link> <span class=md-ellipsis> 2.3.11 关于线性代数的更多信息¶ </span> </a> </li> <li class=md-nav__item> <a href=#2312 class=md-nav__link> <span class=md-ellipsis> 2.3.12 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#24 class=md-nav__link> <span class=md-ellipsis> 2.4 微积分 </span> </a> <nav class=md-nav aria-label="2.4 微积分"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#241 class=md-nav__link> <span class=md-ellipsis> 2.4.1 导数 </span> </a> </li> <li class=md-nav__item> <a href=#242 class=md-nav__link> <span class=md-ellipsis> 2.4.2 偏导数 </span> </a> </li> <li class=md-nav__item> <a href=#243 class=md-nav__link> <span class=md-ellipsis> 2.4.3 梯度 </span> </a> </li> <li class=md-nav__item> <a href=#244 class=md-nav__link> <span class=md-ellipsis> 2.4.4 链式法则 </span> </a> </li> <li class=md-nav__item> <a href=#245 class=md-nav__link> <span class=md-ellipsis> 2.4.5 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#25 class=md-nav__link> <span class=md-ellipsis> 2.5 自动微分 </span> </a> <nav class=md-nav aria-label="2.5 自动微分"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#251 class=md-nav__link> <span class=md-ellipsis> 2.5.1 一个简单的例子 </span> </a> </li> <li class=md-nav__item> <a href=#252 class=md-nav__link> <span class=md-ellipsis> 2.5.2 非标量变量的反向传播 </span> </a> </li> <li class=md-nav__item> <a href=#253 class=md-nav__link> <span class=md-ellipsis> 2.5.3 分离计算 </span> </a> </li> <li class=md-nav__item> <a href=#254-python class=md-nav__link> <span class=md-ellipsis> 2.5.4 Python控制流的梯度计算 </span> </a> </li> <li class=md-nav__item> <a href=#255 class=md-nav__link> <span class=md-ellipsis> 2.5.5 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#26 class=md-nav__link> <span class=md-ellipsis> 2.6 概率与统计 </span> </a> <nav class=md-nav aria-label="2.6 概率与统计"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#261 class=md-nav__link> <span class=md-ellipsis> 2.6.1 基本概率论 </span> </a> <nav class=md-nav aria-label="2.6.1 基本概率论"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#2611 class=md-nav__link> <span class=md-ellipsis> 2.6.1.1 概率论公理 </span> </a> </li> <li class=md-nav__item> <a href=#2612 class=md-nav__link> <span class=md-ellipsis> 2.6.1.2 随机变量 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#262 class=md-nav__link> <span class=md-ellipsis> 2.6.2 处理多个随机变量 </span> </a> <nav class=md-nav aria-label="2.6.2 处理多个随机变量"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#2621 class=md-nav__link> <span class=md-ellipsis> 2.6.2.1 联合概率 </span> </a> </li> <li class=md-nav__item> <a href=#2622 class=md-nav__link> <span class=md-ellipsis> 2.6.2.2 条件概率 </span> </a> </li> <li class=md-nav__item> <a href=#2623 class=md-nav__link> <span class=md-ellipsis> 2.6.2.3 贝叶斯定理 </span> </a> </li> <li class=md-nav__item> <a href=#2624 class=md-nav__link> <span class=md-ellipsis> 2.6.2.4 边际化 （边际概率，边际分布） </span> </a> </li> <li class=md-nav__item> <a href=#2625 class=md-nav__link> <span class=md-ellipsis> 2.6.2.5 独立性 </span> </a> </li> <li class=md-nav__item> <a href=#2626 class=md-nav__link> <span class=md-ellipsis> 2.6.2.6 应用 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#263 class=md-nav__link> <span class=md-ellipsis> 2.6.3 期望和方差 </span> </a> </li> <li class=md-nav__item> <a href=#264 class=md-nav__link> <span class=md-ellipsis> 2.6.4 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#27 class=md-nav__link> <span class=md-ellipsis> 2.7 文档 </span> </a> <nav class=md-nav aria-label="2.7 文档"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#271 class=md-nav__link> <span class=md-ellipsis> 2.7.1 查找模块中的所有函数和类 </span> </a> </li> <li class=md-nav__item> <a href=#272 class=md-nav__link> <span class=md-ellipsis> 2.7.2 查找特定函数和类的所有成员 </span> </a> </li> <li class=md-nav__item> <a href=#273 class=md-nav__link> <span class=md-ellipsis> 2.7.3 小结 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../CH3-LNN/ch3-lnn/ class=md-nav__link> <span class=md-ellipsis> CH3-线性神经网络 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_13> <label class=md-nav__link for=__nav_13 id=__nav_13_label tabindex> <span class=md-ellipsis> 结构健康监测 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_13_label aria-expanded=false> <label class=md-nav__title for=__nav_13> <span class="md-nav__icon md-icon"></span> 结构健康监测 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../SHM/shm/ class=md-nav__link> <span class=md-ellipsis> 结构健康监测 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_14> <label class=md-nav__link for=__nav_14 id=__nav_14_label tabindex> <span class=md-ellipsis> 开发 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_14_label aria-expanded=false> <label class=md-nav__title for=__nav_14> <span class="md-nav__icon md-icon"></span> 开发 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_14_1> <label class=md-nav__link for=__nav_14_1 id=__nav_14_1_label tabindex> <span class=md-ellipsis> 开发环境 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_14_1_label aria-expanded=false> <label class=md-nav__title for=__nav_14_1> <span class="md-nav__icon md-icon"></span> 开发环境 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DEV/DEVENV/LATEX/latex/ class=md-nav__link> <span class=md-ellipsis> LATEX </span> </a> </li> <li class=md-nav__item> <a href=../../../../DEV/DEVENV/REMOTE-CONTROL/remote-control/ class=md-nav__link> <span class=md-ellipsis> 远程控制 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DEV/DEVENV/CLOUD-SYNC/cloud-sync/ class=md-nav__link> <span class=md-ellipsis> 云同步 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../DEV/FRONT-END/front-end/ class=md-nav__link> <span class=md-ellipsis> 前端 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DEV/BACK-END/back-end/ class=md-nav__link> <span class=md-ellipsis> 后端 </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_14_4> <label class=md-nav__link for=__nav_14_4 id=__nav_14_4_label tabindex> <span class=md-ellipsis> 可视化 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_14_4_label aria-expanded=false> <label class=md-nav__title for=__nav_14_4> <span class="md-nav__icon md-icon"></span> 可视化 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../DEV/VISUALIZATION/WEB-BASED/web-based/ class=md-nav__link> <span class=md-ellipsis> WEB端 </span> </a> </li> <li class=md-nav__item> <a href=../../../../DEV/VISUALIZATION/CLIENT-BASED/client-based/ class=md-nav__link> <span class=md-ellipsis> 客户端 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_15> <label class=md-nav__link for=__nav_15 id=__nav_15_label tabindex> <span class=md-ellipsis> 研究 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_15_label aria-expanded=false> <label class=md-nav__title for=__nav_15> <span class="md-nav__icon md-icon"></span> 研究 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../RESEARCH/research/ class=md-nav__link> <span class=md-ellipsis> 研究 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_16> <label class=md-nav__link for=__nav_16 id=__nav_16_label tabindex> <span class=md-ellipsis> 项目 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_16_label aria-expanded=false> <label class=md-nav__title for=__nav_16> <span class="md-nav__icon md-icon"></span> 项目 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../PROJECT/project/ class=md-nav__link> <span class=md-ellipsis> 项目 </span> </a> </li> <li class=md-nav__item> <a href=../../../../PROJECT/TECH-BLOG/mkdocs_and_material/ class=md-nav__link> <span class=md-ellipsis> 技术博客 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#21 class=md-nav__link> <span class=md-ellipsis> 2.1 数据操作 </span> </a> <nav class=md-nav aria-label="2.1 数据操作"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#211 class=md-nav__link> <span class=md-ellipsis> 2.1.1 入门 </span> </a> </li> <li class=md-nav__item> <a href=#212 class=md-nav__link> <span class=md-ellipsis> 2.1.2 运算符 </span> </a> </li> <li class=md-nav__item> <a href=#213 class=md-nav__link> <span class=md-ellipsis> 2.1.3 广播机制 </span> </a> </li> <li class=md-nav__item> <a href=#214 class=md-nav__link> <span class=md-ellipsis> 2.1.4 索引和切片 </span> </a> </li> <li class=md-nav__item> <a href=#215 class=md-nav__link> <span class=md-ellipsis> 2.1.5 节省内存 </span> </a> </li> <li class=md-nav__item> <a href=#216-python class=md-nav__link> <span class=md-ellipsis> 2.1.6 转换为其他 Python 对象 </span> </a> </li> <li class=md-nav__item> <a href=#217 class=md-nav__link> <span class=md-ellipsis> 2.1.7 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#22 class=md-nav__link> <span class=md-ellipsis> 2.2 数据预处理 </span> </a> <nav class=md-nav aria-label="2.2 数据预处理"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#221 class=md-nav__link> <span class=md-ellipsis> 2.2.1 读取数据集 </span> </a> </li> <li class=md-nav__item> <a href=#222 class=md-nav__link> <span class=md-ellipsis> 2.2.2 处理缺失值 </span> </a> </li> <li class=md-nav__item> <a href=#223 class=md-nav__link> <span class=md-ellipsis> 2.2.3 转换为张量格式 </span> </a> </li> <li class=md-nav__item> <a href=#224 class=md-nav__link> <span class=md-ellipsis> 2.2.4 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#23 class=md-nav__link> <span class=md-ellipsis> 2.3 线性代数 </span> </a> <nav class=md-nav aria-label="2.3 线性代数"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#231 class=md-nav__link> <span class=md-ellipsis> 2.3.1 标量 </span> </a> </li> <li class=md-nav__item> <a href=#232 class=md-nav__link> <span class=md-ellipsis> 2.3.2 向量 </span> </a> <nav class=md-nav aria-label="2.3.2 向量"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#2321 class=md-nav__link> <span class=md-ellipsis> 2.3.2.1 长度、维度和形状 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#233 class=md-nav__link> <span class=md-ellipsis> 2.3.3 矩阵 </span> </a> </li> <li class=md-nav__item> <a href=#234 class=md-nav__link> <span class=md-ellipsis> 2.3.4 张量 </span> </a> </li> <li class=md-nav__item> <a href=#235 class=md-nav__link> <span class=md-ellipsis> 2.3.5 张量算法的基本性质 </span> </a> </li> <li class=md-nav__item> <a href=#236 class=md-nav__link> <span class=md-ellipsis> 2.3.6 降维 </span> </a> <nav class=md-nav aria-label="2.3.6 降维"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#2361 class=md-nav__link> <span class=md-ellipsis> 2.3.6.1 非降维求和 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#237 class=md-nav__link> <span class=md-ellipsis> 2.3.7 点积 </span> </a> </li> <li class=md-nav__item> <a href=#238- class=md-nav__link> <span class=md-ellipsis> 2.3.8 矩阵-向量积 </span> </a> </li> <li class=md-nav__item> <a href=#239- class=md-nav__link> <span class=md-ellipsis> 2.3.9 矩阵-矩阵乘法 </span> </a> </li> <li class=md-nav__item> <a href=#2310 class=md-nav__link> <span class=md-ellipsis> 2.3.10 范数 </span> </a> <nav class=md-nav aria-label="2.3.10 范数"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#23101 class=md-nav__link> <span class=md-ellipsis> 2.3.10.1 范数和目标 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2311 class=md-nav__link> <span class=md-ellipsis> 2.3.11 关于线性代数的更多信息¶ </span> </a> </li> <li class=md-nav__item> <a href=#2312 class=md-nav__link> <span class=md-ellipsis> 2.3.12 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#24 class=md-nav__link> <span class=md-ellipsis> 2.4 微积分 </span> </a> <nav class=md-nav aria-label="2.4 微积分"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#241 class=md-nav__link> <span class=md-ellipsis> 2.4.1 导数 </span> </a> </li> <li class=md-nav__item> <a href=#242 class=md-nav__link> <span class=md-ellipsis> 2.4.2 偏导数 </span> </a> </li> <li class=md-nav__item> <a href=#243 class=md-nav__link> <span class=md-ellipsis> 2.4.3 梯度 </span> </a> </li> <li class=md-nav__item> <a href=#244 class=md-nav__link> <span class=md-ellipsis> 2.4.4 链式法则 </span> </a> </li> <li class=md-nav__item> <a href=#245 class=md-nav__link> <span class=md-ellipsis> 2.4.5 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#25 class=md-nav__link> <span class=md-ellipsis> 2.5 自动微分 </span> </a> <nav class=md-nav aria-label="2.5 自动微分"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#251 class=md-nav__link> <span class=md-ellipsis> 2.5.1 一个简单的例子 </span> </a> </li> <li class=md-nav__item> <a href=#252 class=md-nav__link> <span class=md-ellipsis> 2.5.2 非标量变量的反向传播 </span> </a> </li> <li class=md-nav__item> <a href=#253 class=md-nav__link> <span class=md-ellipsis> 2.5.3 分离计算 </span> </a> </li> <li class=md-nav__item> <a href=#254-python class=md-nav__link> <span class=md-ellipsis> 2.5.4 Python控制流的梯度计算 </span> </a> </li> <li class=md-nav__item> <a href=#255 class=md-nav__link> <span class=md-ellipsis> 2.5.5 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#26 class=md-nav__link> <span class=md-ellipsis> 2.6 概率与统计 </span> </a> <nav class=md-nav aria-label="2.6 概率与统计"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#261 class=md-nav__link> <span class=md-ellipsis> 2.6.1 基本概率论 </span> </a> <nav class=md-nav aria-label="2.6.1 基本概率论"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#2611 class=md-nav__link> <span class=md-ellipsis> 2.6.1.1 概率论公理 </span> </a> </li> <li class=md-nav__item> <a href=#2612 class=md-nav__link> <span class=md-ellipsis> 2.6.1.2 随机变量 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#262 class=md-nav__link> <span class=md-ellipsis> 2.6.2 处理多个随机变量 </span> </a> <nav class=md-nav aria-label="2.6.2 处理多个随机变量"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#2621 class=md-nav__link> <span class=md-ellipsis> 2.6.2.1 联合概率 </span> </a> </li> <li class=md-nav__item> <a href=#2622 class=md-nav__link> <span class=md-ellipsis> 2.6.2.2 条件概率 </span> </a> </li> <li class=md-nav__item> <a href=#2623 class=md-nav__link> <span class=md-ellipsis> 2.6.2.3 贝叶斯定理 </span> </a> </li> <li class=md-nav__item> <a href=#2624 class=md-nav__link> <span class=md-ellipsis> 2.6.2.4 边际化 （边际概率，边际分布） </span> </a> </li> <li class=md-nav__item> <a href=#2625 class=md-nav__link> <span class=md-ellipsis> 2.6.2.5 独立性 </span> </a> </li> <li class=md-nav__item> <a href=#2626 class=md-nav__link> <span class=md-ellipsis> 2.6.2.6 应用 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#263 class=md-nav__link> <span class=md-ellipsis> 2.6.3 期望和方差 </span> </a> </li> <li class=md-nav__item> <a href=#264 class=md-nav__link> <span class=md-ellipsis> 2.6.4 小结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#27 class=md-nav__link> <span class=md-ellipsis> 2.7 文档 </span> </a> <nav class=md-nav aria-label="2.7 文档"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#271 class=md-nav__link> <span class=md-ellipsis> 2.7.1 查找模块中的所有函数和类 </span> </a> </li> <li class=md-nav__item> <a href=#272 class=md-nav__link> <span class=md-ellipsis> 2.7.2 查找特定函数和类的所有成员 </span> </a> </li> <li class=md-nav__item> <a href=#273 class=md-nav__link> <span class=md-ellipsis> 2.7.3 小结 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/Shuaiwen-Cui/Infinity.git/edit/master/docs/AI/NOTE-D2L/CH2-PRE/ch2-pre.zh.md title=编辑此页 class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg> </a> <a href=https://github.com/Shuaiwen-Cui/Infinity.git/raw/master/docs/AI/NOTE-D2L/CH2-PRE/ch2-pre.zh.md title=查看本页的源代码 class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg> </a> <h1 id=ch02->CH02 - 预备知识<a class=headerlink href=#ch02- title="Permanent link">&para;</a></h1> <p>要学习深度学习，首先需要先掌握一些基本技能。 所有机器学习方法都涉及从数据中提取信息。 因此，我们先学习一些关于数据的实用技能，包括存储、操作和预处理数据。</p> <p>机器学习通常需要处理大型数据集。 我们可以将某些数据集视为一个表，其中表的行对应样本，列对应属性。 线性代数为人们提供了一些用来处理表格数据的方法。 我们不会太深究细节，而是将重点放在矩阵运算的基本原理及其实现上。</p> <p>深度学习是关于优化的学习。 对于一个带有参数的模型，我们想要找到其中能拟合数据的最好模型。 在算法的每个步骤中，决定以何种方式调整参数需要一点微积分知识。 本章将简要介绍这些知识。 幸运的是，autograd包会自动计算微分，本章也将介绍它。</p> <p>机器学习还涉及如何做出预测：给定观察到的信息，某些未知属性可能的值是多少？ 要在不确定的情况下进行严格的推断，我们需要借用概率语言。</p> <p>最后，官方文档提供了本书之外的大量描述和示例。 在本章的结尾，我们将展示如何在官方文档中查找所需信息。</p> <h2 id=21>2.1 数据操作<a class=headerlink href=#21 title="Permanent link">&para;</a></h2> <h3 id=211>2.1.1 入门<a class=headerlink href=#211 title="Permanent link">&para;</a></h3> <p>导入<code>torch</code>或<code>tensorflow</code>库</p> <div class="tabbed-set tabbed-alternate" data-tabs=1:2><input checked=checked id=211-pytorch name=__tabbed_1 type=radio><input id=211-tensorflow name=__tabbed_1 type=radio><div class=tabbed-labels><label for=211-pytorch>PYTORCH</label><label for=211-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></code></pre></div> </div> </div> </div> <p>张量是一个数组，它可以具有任意数量的维度。 例如，向量是一维张量，矩阵是二维张量。 张量中的每个元素都具有相同的数据类型，且张量的形状由每个维度的大小组成。</p> <div class="tabbed-set tabbed-alternate" data-tabs=2:2><input checked=checked id=211-pytorch_1 name=__tabbed_2 type=radio><input id=211-tensorflow_1 name=__tabbed_2 type=radio><div class=tabbed-labels><label for=211-pytorch_1>PYTORCH</label><label for=211-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>如果想要查看张量的形状，可以访问其<code>shape</code>属性。</p> <div class="tabbed-set tabbed-alternate" data-tabs=3:2><input checked=checked id=211-pytorch_2 name=__tabbed_3 type=radio><input id=211-tensorflow_2 name=__tabbed_3 type=radio><div class=tabbed-labels><label for=211-pytorch_2>PYTORCH</label><label for=211-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>torch.Size([12])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>TensorShape([12])
</span></code></pre></div> </div> </div> </div> <p>如果想要查看张量中元素的总数，可以调用<code>numel</code>函数或者<code>size</code>属性。</p> <div class="tabbed-set tabbed-alternate" data-tabs=4:2><input checked=checked id=211-pytorch_3 name=__tabbed_4 type=radio><input id=211-tensorflow_3 name=__tabbed_4 type=radio><div class=tabbed-labels><label for=211-pytorch_3>PYTORCH</label><label for=211-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=n>x</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a>12
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=n>tf</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a>12
</span></code></pre></div> </div> </div> </div> <p>如果想要改变张量形状，而不改变元素数量和元素值，可以调用<code>reshape</code>函数。 例如，我们可以将张量<code>x</code>从形状为(12,)的一维张量转换为形状为(3,4)的二维张量。</p> <div class="tabbed-set tabbed-alternate" data-tabs=5:2><input checked=checked id=211-pytorch_4 name=__tabbed_5 type=radio><input id=211-tensorflow_4 name=__tabbed_5 type=radio><div class=tabbed-labels><label for=211-pytorch_4>PYTORCH</label><label for=211-tensorflow_4>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a>tensor([[ 0,  1,  2,  3],
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a>        [ 4,  5,  6,  7],
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a>        [ 8,  9, 10, 11]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a>array([[ 0,  1,  2,  3],
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a>       [ 4,  5,  6,  7],
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a>       [ 8,  9, 10, 11]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>如果我们只知道想要张量具有多少列，而不知道想要张量具有多少行，我们可以用-1指定，让系统自动推断出此值。 在上面的例子中，由于张量<code>x</code>包含12个元素，因此我们可以用<code>x.reshape(-1, 4)</code>或<code>x.reshape(3, -1)</code>来重塑张量<code>x</code>为形状为(3,4)或(4,3)的张量。</p> <div class="tabbed-set tabbed-alternate" data-tabs=6:2><input checked=checked id=211-pytorch_5 name=__tabbed_6 type=radio><input id=211-tensorflow_5 name=__tabbed_6 type=radio><div class=tabbed-labels><label for=211-pytorch_5>PYTORCH</label><label for=211-tensorflow_5>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a>tensor([[ 0,  1,  2,  3],
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a>        [ 4,  5,  6,  7],
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a>        [ 8,  9, 10, 11]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a>array([[ 0,  1,  2,  3],
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>       [ 4,  5,  6,  7],
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a>       [ 8,  9, 10, 11]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>有时，我们希望使用全0、全1、其他常量或者从特定分布中随机采样的数字张量来初始化张量。 我们可以创建一个形状为(2,3,4)的张量，其中所有元素都设置为0。</p> <div class="tabbed-set tabbed-alternate" data-tabs=7:2><input checked=checked id=211-pytorch_6 name=__tabbed_7 type=radio><input id=211-tensorflow_6 name=__tabbed_7 type=radio><div class=tabbed-labels><label for=211-pytorch_6>PYTORCH</label><label for=211-tensorflow_6>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a>tensor([[[0., 0., 0., 0.],
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a>         [0., 0., 0., 0.],
</span><span id=__span-23-3><a id=__codelineno-23-3 name=__codelineno-23-3 href=#__codelineno-23-3></a>         [0., 0., 0., 0.]],
</span><span id=__span-23-4><a id=__codelineno-23-4 name=__codelineno-23-4 href=#__codelineno-23-4></a>
</span><span id=__span-23-5><a id=__codelineno-23-5 name=__codelineno-23-5 href=#__codelineno-23-5></a>        [[0., 0., 0., 0.],
</span><span id=__span-23-6><a id=__codelineno-23-6 name=__codelineno-23-6 href=#__codelineno-23-6></a>         [0., 0., 0., 0.],
</span><span id=__span-23-7><a id=__codelineno-23-7 name=__codelineno-23-7 href=#__codelineno-23-7></a>         [0., 0., 0., 0.]]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=n>tf</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a>&lt;tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a>array([[[0., 0., 0., 0.],
</span><span id=__span-25-3><a id=__codelineno-25-3 name=__codelineno-25-3 href=#__codelineno-25-3></a>        [0., 0., 0., 0.],
</span><span id=__span-25-4><a id=__codelineno-25-4 name=__codelineno-25-4 href=#__codelineno-25-4></a>        [0., 0., 0., 0.]],
</span><span id=__span-25-5><a id=__codelineno-25-5 name=__codelineno-25-5 href=#__codelineno-25-5></a>
</span><span id=__span-25-6><a id=__codelineno-25-6 name=__codelineno-25-6 href=#__codelineno-25-6></a>       [[0., 0., 0., 0.],
</span><span id=__span-25-7><a id=__codelineno-25-7 name=__codelineno-25-7 href=#__codelineno-25-7></a>        [0., 0., 0., 0.],
</span><span id=__span-25-8><a id=__codelineno-25-8 name=__codelineno-25-8 href=#__codelineno-25-8></a>        [0., 0., 0., 0.]]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>同样，我们可以创建一个形状为(2,3,4)的张量，其中所有元素都设置为1。</p> <div class="tabbed-set tabbed-alternate" data-tabs=8:2><input checked=checked id=211-pytorch_7 name=__tabbed_8 type=radio><input id=211-tensorflow_7 name=__tabbed_8 type=radio><div class=tabbed-labels><label for=211-pytorch_7>PYTORCH</label><label for=211-tensorflow_7>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a>tensor([[[1., 1., 1., 1.],
</span><span id=__span-27-2><a id=__codelineno-27-2 name=__codelineno-27-2 href=#__codelineno-27-2></a>         [1., 1., 1., 1.],
</span><span id=__span-27-3><a id=__codelineno-27-3 name=__codelineno-27-3 href=#__codelineno-27-3></a>         [1., 1., 1., 1.]],
</span><span id=__span-27-4><a id=__codelineno-27-4 name=__codelineno-27-4 href=#__codelineno-27-4></a>
</span><span id=__span-27-5><a id=__codelineno-27-5 name=__codelineno-27-5 href=#__codelineno-27-5></a>        [[1., 1., 1., 1.],
</span><span id=__span-27-6><a id=__codelineno-27-6 name=__codelineno-27-6 href=#__codelineno-27-6></a>         [1., 1., 1., 1.],
</span><span id=__span-27-7><a id=__codelineno-27-7 name=__codelineno-27-7 href=#__codelineno-27-7></a>         [1., 1., 1., 1.]]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a><span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a>&lt;tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=
</span><span id=__span-29-2><a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a>array([[[1., 1., 1., 1.],
</span><span id=__span-29-3><a id=__codelineno-29-3 name=__codelineno-29-3 href=#__codelineno-29-3></a>        [1., 1., 1., 1.],
</span><span id=__span-29-4><a id=__codelineno-29-4 name=__codelineno-29-4 href=#__codelineno-29-4></a>        [1., 1., 1., 1.]],
</span><span id=__span-29-5><a id=__codelineno-29-5 name=__codelineno-29-5 href=#__codelineno-29-5></a>
</span><span id=__span-29-6><a id=__codelineno-29-6 name=__codelineno-29-6 href=#__codelineno-29-6></a>       [[1., 1., 1., 1.],
</span><span id=__span-29-7><a id=__codelineno-29-7 name=__codelineno-29-7 href=#__codelineno-29-7></a>        [1., 1., 1., 1.],
</span><span id=__span-29-8><a id=__codelineno-29-8 name=__codelineno-29-8 href=#__codelineno-29-8></a>        [1., 1., 1., 1.]]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>我们也可以通过Python的列表（或嵌套列表）指定需要创建的张量中每个元素的值。</p> <div class="tabbed-set tabbed-alternate" data-tabs=9:2><input checked=checked id=211-pytorch_8 name=__tabbed_9 type=radio><input id=211-tensorflow_8 name=__tabbed_9 type=radio><div class=tabbed-labels><label for=211-pytorch_8>PYTORCH</label><label for=211-tensorflow_8>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a>tensor([[2, 1, 4, 3],
</span><span id=__span-31-2><a id=__codelineno-31-2 name=__codelineno-31-2 href=#__codelineno-31-2></a>        [1, 2, 3, 4],
</span><span id=__span-31-3><a id=__codelineno-31-3 name=__codelineno-31-3 href=#__codelineno-31-3></a>        [4, 3, 2, 1]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1 href=#__codelineno-32-1></a><span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1 href=#__codelineno-33-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=
</span><span id=__span-33-2><a id=__codelineno-33-2 name=__codelineno-33-2 href=#__codelineno-33-2></a>array([[2, 1, 4, 3],
</span><span id=__span-33-3><a id=__codelineno-33-3 name=__codelineno-33-3 href=#__codelineno-33-3></a>       [1, 2, 3, 4],
</span><span id=__span-33-4><a id=__codelineno-33-4 name=__codelineno-33-4 href=#__codelineno-33-4></a>       [4, 3, 2, 1]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>通常，我们希望从某个概率分布中随机采样一些值，例如在深度学习中经常使用的正态分布。 在本书中，我们不需要深入了解概率和统计知识。 有关这些主题的优秀书籍包括概率导论 [<a class="magiclink magiclink-github magiclink-mention" href=https://github.com/blitzstein2014introduction title="GitHub User: blitzstein2014introduction">@blitzstein2014introduction</a>] 和统计学习方法 [<a class="magiclink magiclink-github magiclink-mention" href=https://github.com/lihang2012statistical title="GitHub User: lihang2012statistical">@lihang2012statistical</a>]。 为了生成具有形状为(3,4)的张量。 其中的每个元素都将随机采样于均值为0、标准差为1的正态分布。</p> <div class="tabbed-set tabbed-alternate" data-tabs=10:2><input checked=checked id=211-pytorch_9 name=__tabbed_10 type=radio><input id=211-tensorflow_9 name=__tabbed_10 type=radio><div class=tabbed-labels><label for=211-pytorch_9>PYTORCH</label><label for=211-tensorflow_9>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1 href=#__codelineno-34-1></a><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1 href=#__codelineno-35-1></a>tensor([[ 0.1835,  0.7694, -0.4696,  0.5421],
</span><span id=__span-35-2><a id=__codelineno-35-2 name=__codelineno-35-2 href=#__codelineno-35-2></a>        [-0.4637, -0.0328,  0.2369, -0.4107],
</span><span id=__span-35-3><a id=__codelineno-35-3 name=__codelineno-35-3 href=#__codelineno-35-3></a>        [-0.9880, -0.5176, -0.1713, -0.2025]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1 href=#__codelineno-36-1></a><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1 href=#__codelineno-37-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
</span><span id=__span-37-2><a id=__codelineno-37-2 name=__codelineno-37-2 href=#__codelineno-37-2></a>array([[ 0.1835,  0.7694, -0.4696,  0.5421],
</span><span id=__span-37-3><a id=__codelineno-37-3 name=__codelineno-37-3 href=#__codelineno-37-3></a>       [-0.4637, -0.0328,  0.2369, -0.4107],
</span><span id=__span-37-4><a id=__codelineno-37-4 name=__codelineno-37-4 href=#__codelineno-37-4></a>       [-0.9880, -0.5176, -0.1713, -0.2025]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=212>2.1.2 运算符<a class=headerlink href=#212 title="Permanent link">&para;</a></h3> <p>张量支持大量的运算符（操作符）。 例如，我们可以对两个张量按元素相加。 给定张量<code>X</code>和<code>Y</code>，我们可以使用<code>X + Y</code>来实现按元素加法。 对于按元素乘法、按元素除法和按元素指数运算，我们分别可以使用<code>X * Y</code>、<code>X / Y</code>和<code>X**Y</code>。 符号<code>**</code>代表按元素乘方运算。对于任意具有相同形状的张量， 常见的标准算术运算符（+、-、<em>、/和*</em>）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。 在下面的例子中，我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。</p> <div class="tabbed-set tabbed-alternate" data-tabs=11:2><input checked=checked id=212-pytorch name=__tabbed_11 type=radio><input id=212-tensorflow name=__tabbed_11 type=radio><div class=tabbed-labels><label for=212-pytorch>PYTORCH</label><label for=212-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-38-1><a id=__codelineno-38-1 name=__codelineno-38-1 href=#__codelineno-38-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>])</span>
</span><span id=__span-38-2><a id=__codelineno-38-2 name=__codelineno-38-2 href=#__codelineno-38-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span><span id=__span-38-3><a id=__codelineno-38-3 name=__codelineno-38-3 href=#__codelineno-38-3></a><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>/</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>**</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-39-1><a id=__codelineno-39-1 name=__codelineno-39-1 href=#__codelineno-39-1></a>(tensor([ 3.,  4.,  6., 10.]),
</span><span id=__span-39-2><a id=__codelineno-39-2 name=__codelineno-39-2 href=#__codelineno-39-2></a> tensor([-1.,  0.,  2.,  6.]),
</span><span id=__span-39-3><a id=__codelineno-39-3 name=__codelineno-39-3 href=#__codelineno-39-3></a> tensor([ 2.,  4.,  8., 16.]),
</span><span id=__span-39-4><a id=__codelineno-39-4 name=__codelineno-39-4 href=#__codelineno-39-4></a> tensor([0.5000, 1.0000, 2.0000, 4.0000]),
</span><span id=__span-39-5><a id=__codelineno-39-5 name=__codelineno-39-5 href=#__codelineno-39-5></a> tensor([ 1.,  4., 16., 64.]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-40-1><a id=__codelineno-40-1 name=__codelineno-40-1 href=#__codelineno-40-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>1.0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>])</span>
</span><span id=__span-40-2><a id=__codelineno-40-2 name=__codelineno-40-2 href=#__codelineno-40-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span><span id=__span-40-3><a id=__codelineno-40-3 name=__codelineno-40-3 href=#__codelineno-40-3></a><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>/</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>**</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-41-1><a id=__codelineno-41-1 name=__codelineno-41-1 href=#__codelineno-41-1></a>(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.,  4.,  6., 10.], dtype=float32)&gt;,
</span><span id=__span-41-2><a id=__codelineno-41-2 name=__codelineno-41-2 href=#__codelineno-41-2></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.,  0.,  2.,  6.], dtype=float32)&gt;,
</span><span id=__span-41-3><a id=__codelineno-41-3 name=__codelineno-41-3 href=#__codelineno-41-3></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.,  4.,  8., 16.], dtype=float32)&gt;,
</span><span id=__span-41-4><a id=__codelineno-41-4 name=__codelineno-41-4 href=#__codelineno-41-4></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.5, 1. , 2. , 4. ], dtype=float32)&gt;,
</span><span id=__span-41-5><a id=__codelineno-41-5 name=__codelineno-41-5 href=#__codelineno-41-5></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.,  4., 16., 64.], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <p>“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。</p> <div class="tabbed-set tabbed-alternate" data-tabs=12:2><input checked=checked id=212-pytorch_1 name=__tabbed_12 type=radio><input id=212-tensorflow_1 name=__tabbed_12 type=radio><div class=tabbed-labels><label for=212-pytorch_1>PYTORCH</label><label for=212-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-42-1><a id=__codelineno-42-1 name=__codelineno-42-1 href=#__codelineno-42-1></a><span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-43-1><a id=__codelineno-43-1 name=__codelineno-43-1 href=#__codelineno-43-1></a>tensor([  2.7183,   7.3891,  54.5981, 298.8674])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-44-1><a id=__codelineno-44-1 name=__codelineno-44-1 href=#__codelineno-44-1></a><span class=n>tf</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-45-1><a id=__codelineno-45-1 name=__codelineno-45-1 href=#__codelineno-45-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([  2.7183,   7.3891,  54.5981, 298.8674], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>我们也可以把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。</p> <div class="tabbed-set tabbed-alternate" data-tabs=13:2><input checked=checked id=212-pytorch_2 name=__tabbed_13 type=radio><input id=212-tensorflow_2 name=__tabbed_13 type=radio><div class=tabbed-labels><label for=212-pytorch_2>PYTORCH</label><label for=212-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-46-1><a id=__codelineno-46-1 name=__codelineno-46-1 href=#__codelineno-46-1></a><span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span><span id=__span-46-2><a id=__codelineno-46-2 name=__codelineno-46-2 href=#__codelineno-46-2></a><span class=n>Y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>2.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span><span id=__span-46-3><a id=__codelineno-46-3 name=__codelineno-46-3 href=#__codelineno-46-3></a><span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-47-1><a id=__codelineno-47-1 name=__codelineno-47-1 href=#__codelineno-47-1></a>(tensor([[ 0.,  1.,  2.,  3.],
</span><span id=__span-47-2><a id=__codelineno-47-2 name=__codelineno-47-2 href=#__codelineno-47-2></a>         [ 4.,  5.,  6.,  7.],
</span><span id=__span-47-3><a id=__codelineno-47-3 name=__codelineno-47-3 href=#__codelineno-47-3></a>         [ 8.,  9., 10., 11.],
</span><span id=__span-47-4><a id=__codelineno-47-4 name=__codelineno-47-4 href=#__codelineno-47-4></a>         [ 2.,  1.,  4.,  3.],
</span><span id=__span-47-5><a id=__codelineno-47-5 name=__codelineno-47-5 href=#__codelineno-47-5></a>         [ 1.,  2.,  3.,  4.],
</span><span id=__span-47-6><a id=__codelineno-47-6 name=__codelineno-47-6 href=#__codelineno-47-6></a>         [ 4.,  3.,  2.,  1.]]),
</span><span id=__span-47-7><a id=__codelineno-47-7 name=__codelineno-47-7 href=#__codelineno-47-7></a> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
</span><span id=__span-47-8><a id=__codelineno-47-8 name=__codelineno-47-8 href=#__codelineno-47-8></a>         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
</span><span id=__span-47-9><a id=__codelineno-47-9 name=__codelineno-47-9 href=#__codelineno-47-9></a>         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-48-1><a id=__codelineno-48-1 name=__codelineno-48-1 href=#__codelineno-48-1></a><span class=n>X</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>),</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span><span id=__span-48-2><a id=__codelineno-48-2 name=__codelineno-48-2 href=#__codelineno-48-2></a><span class=n>Y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mf>2.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span><span id=__span-48-3><a id=__codelineno-48-3 name=__codelineno-48-3 href=#__codelineno-48-3></a><span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span> <span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-49-1><a id=__codelineno-49-1 name=__codelineno-49-1 href=#__codelineno-49-1></a>(&lt;tf.Tensor: shape=(6, 4), dtype=float32, numpy=
</span><span id=__span-49-2><a id=__codelineno-49-2 name=__codelineno-49-2 href=#__codelineno-49-2></a>array([[ 0.,  1.,  2.,  3.],
</span><span id=__span-49-3><a id=__codelineno-49-3 name=__codelineno-49-3 href=#__codelineno-49-3></a>       [ 4.,  5.,  6.,  7.],
</span><span id=__span-49-4><a id=__codelineno-49-4 name=__codelineno-49-4 href=#__codelineno-49-4></a>       [ 8.,  9., 10., 11.],
</span><span id=__span-49-5><a id=__codelineno-49-5 name=__codelineno-49-5 href=#__codelineno-49-5></a>       [ 2.,  1.,  4.,  3.],
</span><span id=__span-49-6><a id=__codelineno-49-6 name=__codelineno-49-6 href=#__codelineno-49-6></a>       [ 1.,  2.,  3.,  4.],
</span><span id=__span-49-7><a id=__codelineno-49-7 name=__codelineno-49-7 href=#__codelineno-49-7></a>       [ 4.,  3.,  2.,  1.]], dtype=float32)&gt;,
</span><span id=__span-49-8><a id=__codelineno-49-8 name=__codelineno-49-8 href=#__codelineno-49-8></a> &lt;tf.Tensor: shape=(3, 8), dtype=float32, numpy=
</span><span id=__span-49-9><a id=__codelineno-49-9 name=__codelineno-49-9 href=#__codelineno-49-9></a>array([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
</span><span id=__span-49-10><a id=__codelineno-49-10 name=__codelineno-49-10 href=#__codelineno-49-10></a>       [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
</span><span id=__span-49-11><a id=__codelineno-49-11 name=__codelineno-49-11 href=#__codelineno-49-11></a>       [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <p>有时，我们想通过逻辑运算符构建二元张量。</p> <div class="tabbed-set tabbed-alternate" data-tabs=14:2><input checked=checked id=212-pytorch_3 name=__tabbed_14 type=radio><input id=212-tensorflow_3 name=__tabbed_14 type=radio><div class=tabbed-labels><label for=212-pytorch_3>PYTORCH</label><label for=212-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-50-1><a id=__codelineno-50-1 name=__codelineno-50-1 href=#__codelineno-50-1></a><span class=n>X</span> <span class=o>==</span> <span class=n>Y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-51-1><a id=__codelineno-51-1 name=__codelineno-51-1 href=#__codelineno-51-1></a>tensor([[False,  True, False,  True],
</span><span id=__span-51-2><a id=__codelineno-51-2 name=__codelineno-51-2 href=#__codelineno-51-2></a>        [False, False, False, False],
</span><span id=__span-51-3><a id=__codelineno-51-3 name=__codelineno-51-3 href=#__codelineno-51-3></a>        [False, False, False, False]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-52-1><a id=__codelineno-52-1 name=__codelineno-52-1 href=#__codelineno-52-1></a><span class=n>X</span> <span class=o>==</span> <span class=n>Y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-53-1><a id=__codelineno-53-1 name=__codelineno-53-1 href=#__codelineno-53-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy=
</span><span id=__span-53-2><a id=__codelineno-53-2 name=__codelineno-53-2 href=#__codelineno-53-2></a>array([[False,  True, False,  True],
</span><span id=__span-53-3><a id=__codelineno-53-3 name=__codelineno-53-3 href=#__codelineno-53-3></a>       [False, False, False, False],
</span><span id=__span-53-4><a id=__codelineno-53-4 name=__codelineno-53-4 href=#__codelineno-53-4></a>       [False, False, False, False]])&gt;
</span></code></pre></div> </div> </div> </div> <p>对张量中的所有元素进行求和会产生一个只有一个元素的张量。</p> <div class="tabbed-set tabbed-alternate" data-tabs=15:2><input checked=checked id=212-pytorch_4 name=__tabbed_15 type=radio><input id=212-tensorflow_4 name=__tabbed_15 type=radio><div class=tabbed-labels><label for=212-pytorch_4>PYTORCH</label><label for=212-tensorflow_4>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-54-1><a id=__codelineno-54-1 name=__codelineno-54-1 href=#__codelineno-54-1></a><span class=n>X</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-55-1><a id=__codelineno-55-1 name=__codelineno-55-1 href=#__codelineno-55-1></a>tensor(66.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-56-1><a id=__codelineno-56-1 name=__codelineno-56-1 href=#__codelineno-56-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-57-1><a id=__codelineno-57-1 name=__codelineno-57-1 href=#__codelineno-57-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=66.0&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=213>2.1.3 广播机制<a class=headerlink href=#213 title="Permanent link">&para;</a></h3> <p>在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，我们还可以在不同形状的张量上执行按元素操作。 当两个张量的形状不同时，我们将通过广播机制（broadcasting mechanism）来执行按元素操作。 这个机制的工作方式如下： 首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。</p> <p>在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例所示。</p> <div class="tabbed-set tabbed-alternate" data-tabs=16:2><input checked=checked id=213-pytorch name=__tabbed_16 type=radio><input id=213-tensorflow name=__tabbed_16 type=radio><div class=tabbed-labels><label for=213-pytorch>PYTORCH</label><label for=213-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-58-1><a id=__codelineno-58-1 name=__codelineno-58-1 href=#__codelineno-58-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span><span id=__span-58-2><a id=__codelineno-58-2 name=__codelineno-58-2 href=#__codelineno-58-2></a><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span><span id=__span-58-3><a id=__codelineno-58-3 name=__codelineno-58-3 href=#__codelineno-58-3></a><span class=n>a</span><span class=p>,</span> <span class=n>b</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-59-1><a id=__codelineno-59-1 name=__codelineno-59-1 href=#__codelineno-59-1></a>(tensor([[0],
</span><span id=__span-59-2><a id=__codelineno-59-2 name=__codelineno-59-2 href=#__codelineno-59-2></a>         [1],
</span><span id=__span-59-3><a id=__codelineno-59-3 name=__codelineno-59-3 href=#__codelineno-59-3></a>         [2]]),
</span><span id=__span-59-4><a id=__codelineno-59-4 name=__codelineno-59-4 href=#__codelineno-59-4></a> tensor([[0, 1]]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-60-1><a id=__codelineno-60-1 name=__codelineno-60-1 href=#__codelineno-60-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>3</span><span class=p>),</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span><span id=__span-60-2><a id=__codelineno-60-2 name=__codelineno-60-2 href=#__codelineno-60-2></a><span class=n>b</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>2</span><span class=p>),</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span><span id=__span-60-3><a id=__codelineno-60-3 name=__codelineno-60-3 href=#__codelineno-60-3></a><span class=n>a</span><span class=p>,</span> <span class=n>b</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-61-1><a id=__codelineno-61-1 name=__codelineno-61-1 href=#__codelineno-61-1></a>(&lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=
</span><span id=__span-61-2><a id=__codelineno-61-2 name=__codelineno-61-2 href=#__codelineno-61-2></a>array([[0],
</span><span id=__span-61-3><a id=__codelineno-61-3 name=__codelineno-61-3 href=#__codelineno-61-3></a>       [1],
</span><span id=__span-61-4><a id=__codelineno-61-4 name=__codelineno-61-4 href=#__codelineno-61-4></a>       [2]], dtype=int32)&gt;,
</span><span id=__span-61-5><a id=__codelineno-61-5 name=__codelineno-61-5 href=#__codelineno-61-5></a> &lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)&gt;)
</span></code></pre></div> </div> </div> </div> <p>由于<code>a</code>和<code>b</code>分别是3行1列和1行2列的矩阵，如果我们想要将它们相加，那么<code>a</code>中第一列的3个元素被广播（复制）到了第二列，而<code>b</code>中第一行的2个元素被广播（复制）到了第二行和第三行。 如此，我们就可以对两个3行2列的矩阵按元素相加。</p> <div class="tabbed-set tabbed-alternate" data-tabs=17:2><input checked=checked id=213-pytorch_1 name=__tabbed_17 type=radio><input id=213-tensorflow_1 name=__tabbed_17 type=radio><div class=tabbed-labels><label for=213-pytorch_1>PYTORCH</label><label for=213-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-62-1><a id=__codelineno-62-1 name=__codelineno-62-1 href=#__codelineno-62-1></a><span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-63-1><a id=__codelineno-63-1 name=__codelineno-63-1 href=#__codelineno-63-1></a>tensor([[0, 1],
</span><span id=__span-63-2><a id=__codelineno-63-2 name=__codelineno-63-2 href=#__codelineno-63-2></a>        [1, 2],
</span><span id=__span-63-3><a id=__codelineno-63-3 name=__codelineno-63-3 href=#__codelineno-63-3></a>        [2, 3]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-64-1><a id=__codelineno-64-1 name=__codelineno-64-1 href=#__codelineno-64-1></a><span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-65-1><a id=__codelineno-65-1 name=__codelineno-65-1 href=#__codelineno-65-1></a>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
</span><span id=__span-65-2><a id=__codelineno-65-2 name=__codelineno-65-2 href=#__codelineno-65-2></a>array([[0, 1],
</span><span id=__span-65-3><a id=__codelineno-65-3 name=__codelineno-65-3 href=#__codelineno-65-3></a>       [1, 2],
</span><span id=__span-65-4><a id=__codelineno-65-4 name=__codelineno-65-4 href=#__codelineno-65-4></a>       [2, 3]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=214>2.1.4 索引和切片<a class=headerlink href=#214 title="Permanent link">&para;</a></h3> <p>就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。 与任何Python数组一样，第一个元素的索引是0，而不是1。 与标准Python列表一样，我们可以通过使用方括号（[]）来访问和分配张量中的元素。</p> <div class="tabbed-set tabbed-alternate" data-tabs=18:2><input checked=checked id=214-pytorch name=__tabbed_18 type=radio><input id=214-tensorflow name=__tabbed_18 type=radio><div class=tabbed-labels><label for=214-pytorch>PYTORCH</label><label for=214-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-66-1><a id=__codelineno-66-1 name=__codelineno-66-1 href=#__codelineno-66-1></a><span class=n>X</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-67-1><a id=__codelineno-67-1 name=__codelineno-67-1 href=#__codelineno-67-1></a>(tensor([ 8.,  9., 10., 11.]), tensor([[ 4.,  5.,  6.,  7.],
</span><span id=__span-67-2><a id=__codelineno-67-2 name=__codelineno-67-2 href=#__codelineno-67-2></a>         [ 8.,  9., 10., 11.]]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-68-1><a id=__codelineno-68-1 name=__codelineno-68-1 href=#__codelineno-68-1></a><span class=n>X</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-69-1><a id=__codelineno-69-1 name=__codelineno-69-1 href=#__codelineno-69-1></a>(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)&gt;,
</span><span id=__span-69-2><a id=__codelineno-69-2 name=__codelineno-69-2 href=#__codelineno-69-2></a> &lt;tf.Tensor: shape=(2, 4), dtype=float32, numpy=
</span><span id=__span-69-3><a id=__codelineno-69-3 name=__codelineno-69-3 href=#__codelineno-69-3></a>array([[ 4.,  5.,  6.,  7.],
</span><span id=__span-69-4><a id=__codelineno-69-4 name=__codelineno-69-4 href=#__codelineno-69-4></a>       [ 8.,  9., 10., 11.]], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <p>除读取外，我们还可以通过指定索引来将元素写入矩阵。</p> <div class="tabbed-set tabbed-alternate" data-tabs=19:2><input checked=checked id=214-pytorch_1 name=__tabbed_19 type=radio><input id=214-tensorflow_1 name=__tabbed_19 type=radio><div class=tabbed-labels><label for=214-pytorch_1>PYTORCH</label><label for=214-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-70-1><a id=__codelineno-70-1 name=__codelineno-70-1 href=#__codelineno-70-1></a><span class=n>X</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=mi>9</span>
</span><span id=__span-70-2><a id=__codelineno-70-2 name=__codelineno-70-2 href=#__codelineno-70-2></a><span class=n>X</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-71-1><a id=__codelineno-71-1 name=__codelineno-71-1 href=#__codelineno-71-1></a>tensor([[ 0.,  1.,  2.,  3.],
</span><span id=__span-71-2><a id=__codelineno-71-2 name=__codelineno-71-2 href=#__codelineno-71-2></a>        [ 4.,  5.,  9.,  7.],
</span><span id=__span-71-3><a id=__codelineno-71-3 name=__codelineno-71-3 href=#__codelineno-71-3></a>        [ 8.,  9., 10., 11.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-72-1><a id=__codelineno-72-1 name=__codelineno-72-1 href=#__codelineno-72-1></a><span class=n>X</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=mi>9</span><span class=p>)</span>
</span><span id=__span-72-2><a id=__codelineno-72-2 name=__codelineno-72-2 href=#__codelineno-72-2></a><span class=n>X</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-73-1><a id=__codelineno-73-1 name=__codelineno-73-1 href=#__codelineno-73-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
</span><span id=__span-73-2><a id=__codelineno-73-2 name=__codelineno-73-2 href=#__codelineno-73-2></a>array([[ 0.,  1.,  2.,  3.],
</span><span id=__span-73-3><a id=__codelineno-73-3 name=__codelineno-73-3 href=#__codelineno-73-3></a>       [ 4.,  5.,  9.,  7.],
</span><span id=__span-73-4><a id=__codelineno-73-4 name=__codelineno-73-4 href=#__codelineno-73-4></a>       [ 8.,  9., 10., 11.]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，<code>[0:2, :]</code>访问第1行和第2行，其中“:”代表沿轴0（行）的所有元素。 虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。</p> <div class="tabbed-set tabbed-alternate" data-tabs=20:2><input checked=checked id=214-pytorch_2 name=__tabbed_20 type=radio><input id=214-tensorflow_2 name=__tabbed_20 type=radio><div class=tabbed-labels><label for=214-pytorch_2>PYTORCH</label><label for=214-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-74-1><a id=__codelineno-74-1 name=__codelineno-74-1 href=#__codelineno-74-1></a><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=p>:]</span> <span class=o>=</span> <span class=mi>12</span>
</span><span id=__span-74-2><a id=__codelineno-74-2 name=__codelineno-74-2 href=#__codelineno-74-2></a><span class=n>X</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-75-1><a id=__codelineno-75-1 name=__codelineno-75-1 href=#__codelineno-75-1></a>tensor([[12., 12., 12., 12.],
</span><span id=__span-75-2><a id=__codelineno-75-2 name=__codelineno-75-2 href=#__codelineno-75-2></a>        [12., 12., 12., 12.],
</span><span id=__span-75-3><a id=__codelineno-75-3 name=__codelineno-75-3 href=#__codelineno-75-3></a>        [ 8.,  9., 10., 11.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-76-1><a id=__codelineno-76-1 name=__codelineno-76-1 href=#__codelineno-76-1></a><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=p>:]</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>
</span><span id=__span-76-2><a id=__codelineno-76-2 name=__codelineno-76-2 href=#__codelineno-76-2></a><span class=n>X</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-77-1><a id=__codelineno-77-1 name=__codelineno-77-1 href=#__codelineno-77-1></a>&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
</span><span id=__span-77-2><a id=__codelineno-77-2 name=__codelineno-77-2 href=#__codelineno-77-2></a>array([[12., 12., 12., 12.],
</span><span id=__span-77-3><a id=__codelineno-77-3 name=__codelineno-77-3 href=#__codelineno-77-3></a>       [12., 12., 12., 12.],
</span><span id=__span-77-4><a id=__codelineno-77-4 name=__codelineno-77-4 href=#__codelineno-77-4></a>       [ 8.,  9., 10., 11.]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=215>2.1.5 节省内存<a class=headerlink href=#215 title="Permanent link">&para;</a></h3> <p>运行一些操作可能会导致为新结果分配内存。 例如，如果我们用<code>Y = X + Y</code>，我们会将<code>Y</code>指向新创建的内存，而不是将<code>Y</code>指向<code>X</code>的原始位置。 我们可以使用Python的<code>id</code>函数来验证这一点，这个函数给我们提供了内存中引用对象的确切地址。 运行<code>Y = Y + X</code>后，我们会发现<code>id(Y)</code>指向另一个位置。 这是因为Python首先计算<code>Y + X</code>，为结果分配新的内存，然后使<code>Y</code>指向内存中的这个新位置。</p> <div class="tabbed-set tabbed-alternate" data-tabs=21:2><input checked=checked id=215-pytorch name=__tabbed_21 type=radio><input id=215-tensorflow name=__tabbed_21 type=radio><div class=tabbed-labels><label for=215-pytorch>PYTORCH</label><label for=215-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-78-1><a id=__codelineno-78-1 name=__codelineno-78-1 href=#__codelineno-78-1></a><span class=n>before</span> <span class=o>=</span> <span class=nb>id</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span>
</span><span id=__span-78-2><a id=__codelineno-78-2 name=__codelineno-78-2 href=#__codelineno-78-2></a><span class=n>Y</span> <span class=o>=</span> <span class=n>Y</span> <span class=o>+</span> <span class=n>X</span>
</span><span id=__span-78-3><a id=__codelineno-78-3 name=__codelineno-78-3 href=#__codelineno-78-3></a><span class=nb>id</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span> <span class=o>==</span> <span class=n>before</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-79-1><a id=__codelineno-79-1 name=__codelineno-79-1 href=#__codelineno-79-1></a>False
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-80-1><a id=__codelineno-80-1 name=__codelineno-80-1 href=#__codelineno-80-1></a><span class=n>before</span> <span class=o>=</span> <span class=nb>id</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span>
</span><span id=__span-80-2><a id=__codelineno-80-2 name=__codelineno-80-2 href=#__codelineno-80-2></a><span class=n>Y</span> <span class=o>=</span> <span class=n>Y</span> <span class=o>+</span> <span class=n>X</span>
</span><span id=__span-80-3><a id=__codelineno-80-3 name=__codelineno-80-3 href=#__codelineno-80-3></a><span class=nb>id</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span> <span class=o>==</span> <span class=n>before</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-81-1><a id=__codelineno-81-1 name=__codelineno-81-1 href=#__codelineno-81-1></a>False
</span></code></pre></div> </div> </div> </div> <p>这可能是不可取的，原因有两个：</p> <p>首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；</p> <p>如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。</p> <p>幸运的是，(<strong>执行原地操作</strong>)的方法很简单。我们可以使用切片表示法将操作的结果分配给先前分配给结果的数组，例如<code>Y[:] = &lt;expression&gt;</code>。为了说明这一点，我们首先创建一个新的矩阵<code>Z</code>，其形状与另一个<code>Y</code>相同，使用<code>zeros_like</code>来分配一个全0的块。</p> <div class="tabbed-set tabbed-alternate" data-tabs=22:2><input checked=checked id=215-pytorch_1 name=__tabbed_22 type=radio><input id=215-tensorflow_1 name=__tabbed_22 type=radio><div class=tabbed-labels><label for=215-pytorch_1>PYTORCH</label><label for=215-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-82-1><a id=__codelineno-82-1 name=__codelineno-82-1 href=#__codelineno-82-1></a><span class=n>Z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span>
</span><span id=__span-82-2><a id=__codelineno-82-2 name=__codelineno-82-2 href=#__codelineno-82-2></a><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;id(Z):&#39;</span><span class=p>,</span> <span class=nb>id</span><span class=p>(</span><span class=n>Z</span><span class=p>))</span>
</span><span id=__span-82-3><a id=__codelineno-82-3 name=__codelineno-82-3 href=#__codelineno-82-3></a><span class=n>Z</span><span class=p>[:]</span> <span class=o>=</span> <span class=n>X</span> <span class=o>+</span> <span class=n>Y</span>
</span><span id=__span-82-4><a id=__codelineno-82-4 name=__codelineno-82-4 href=#__codelineno-82-4></a><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;id(Z):&#39;</span><span class=p>,</span> <span class=nb>id</span><span class=p>(</span><span class=n>Z</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-83-1><a id=__codelineno-83-1 name=__codelineno-83-1 href=#__codelineno-83-1></a>id(Z): 140703086993984
</span><span id=__span-83-2><a id=__codelineno-83-2 name=__codelineno-83-2 href=#__codelineno-83-2></a>id(Z): 140703086993984
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-84-1><a id=__codelineno-84-1 name=__codelineno-84-1 href=#__codelineno-84-1></a><span class=n>Z</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span>
</span><span id=__span-84-2><a id=__codelineno-84-2 name=__codelineno-84-2 href=#__codelineno-84-2></a><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;id(Z):&#39;</span><span class=p>,</span> <span class=nb>id</span><span class=p>(</span><span class=n>Z</span><span class=p>))</span>
</span><span id=__span-84-3><a id=__codelineno-84-3 name=__codelineno-84-3 href=#__codelineno-84-3></a><span class=n>Z</span><span class=p>[:]</span> <span class=o>=</span> <span class=n>X</span> <span class=o>+</span> <span class=n>Y</span>
</span><span id=__span-84-4><a id=__codelineno-84-4 name=__codelineno-84-4 href=#__codelineno-84-4></a><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;id(Z):&#39;</span><span class=p>,</span> <span class=nb>id</span><span class=p>(</span><span class=n>Z</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-85-1><a id=__codelineno-85-1 name=__codelineno-85-1 href=#__codelineno-85-1></a>id(Z): 140703086993984
</span><span id=__span-85-2><a id=__codelineno-85-2 name=__codelineno-85-2 href=#__codelineno-85-2></a>id(Z): 140703086993984
</span></code></pre></div> </div> </div> </div> <p>现在，我们用<code>X + Y</code>替换<code>Y</code>。我们使用<code>[:]</code>来指定我们想要用分配给<code>X + Y</code>的结果来更新<code>Y</code>中的值。</p> <div class="tabbed-set tabbed-alternate" data-tabs=23:2><input checked=checked id=215-pytorch_2 name=__tabbed_23 type=radio><input id=215-tensorflow_2 name=__tabbed_23 type=radio><div class=tabbed-labels><label for=215-pytorch_2>PYTORCH</label><label for=215-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-86-1><a id=__codelineno-86-1 name=__codelineno-86-1 href=#__codelineno-86-1></a><span class=n>before</span> <span class=o>=</span> <span class=nb>id</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span>
</span><span id=__span-86-2><a id=__codelineno-86-2 name=__codelineno-86-2 href=#__codelineno-86-2></a><span class=n>Y</span><span class=p>[:]</span> <span class=o>=</span> <span class=n>X</span> <span class=o>+</span> <span class=n>Y</span>
</span><span id=__span-86-3><a id=__codelineno-86-3 name=__codelineno-86-3 href=#__codelineno-86-3></a><span class=nb>id</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span> <span class=o>==</span> <span class=n>before</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-87-1><a id=__codelineno-87-1 name=__codelineno-87-1 href=#__codelineno-87-1></a>True
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-88-1><a id=__codelineno-88-1 name=__codelineno-88-1 href=#__codelineno-88-1></a><span class=n>before</span> <span class=o>=</span> <span class=nb>id</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span>
</span><span id=__span-88-2><a id=__codelineno-88-2 name=__codelineno-88-2 href=#__codelineno-88-2></a><span class=n>Y</span><span class=p>[:]</span> <span class=o>=</span> <span class=n>X</span> <span class=o>+</span> <span class=n>Y</span>
</span><span id=__span-88-3><a id=__codelineno-88-3 name=__codelineno-88-3 href=#__codelineno-88-3></a><span class=nb>id</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span> <span class=o>==</span> <span class=n>before</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-89-1><a id=__codelineno-89-1 name=__codelineno-89-1 href=#__codelineno-89-1></a>True
</span></code></pre></div> </div> </div> </div> <h3 id=216-python>2.1.6 转换为其他 Python 对象<a class=headerlink href=#216-python title="Permanent link">&para;</a></h3> <p>将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 转换后的结果不共享内存。 这个小的不便实际上是非常重要的：当在CPU或GPU上执行操作的时候， 如果Python的NumPy包也希望使用相同的内存块执行其他操作，人们不希望停下计算来等它。</p> <div class="tabbed-set tabbed-alternate" data-tabs=24:2><input checked=checked id=216-python-pytorch name=__tabbed_24 type=radio><input id=216-python-tensorflow name=__tabbed_24 type=radio><div class=tabbed-labels><label for=216-python-pytorch>PYTORCH</label><label for=216-python-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-90-1><a id=__codelineno-90-1 name=__codelineno-90-1 href=#__codelineno-90-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span><span id=__span-90-2><a id=__codelineno-90-2 name=__codelineno-90-2 href=#__codelineno-90-2></a><span class=n>B</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span><span id=__span-90-3><a id=__codelineno-90-3 name=__codelineno-90-3 href=#__codelineno-90-3></a><span class=nb>type</span><span class=p>(</span><span class=n>A</span><span class=p>),</span> <span class=nb>type</span><span class=p>(</span><span class=n>B</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-91-1><a id=__codelineno-91-1 name=__codelineno-91-1 href=#__codelineno-91-1></a>(numpy.ndarray, torch.Tensor)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-92-1><a id=__codelineno-92-1 name=__codelineno-92-1 href=#__codelineno-92-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span><span id=__span-92-2><a id=__codelineno-92-2 name=__codelineno-92-2 href=#__codelineno-92-2></a><span class=n>B</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>convert_to_tensor</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span><span id=__span-92-3><a id=__codelineno-92-3 name=__codelineno-92-3 href=#__codelineno-92-3></a><span class=nb>type</span><span class=p>(</span><span class=n>A</span><span class=p>),</span> <span class=nb>type</span><span class=p>(</span><span class=n>B</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-93-1><a id=__codelineno-93-1 name=__codelineno-93-1 href=#__codelineno-93-1></a>(numpy.ndarray, tensorflow.python.framework.ops.EagerTensor)
</span></code></pre></div> </div> </div> </div> <p>要(<strong>将大小为1的张量转换为Python标量</strong>)，我们可以调用<code>item</code>函数或Python的内置函数。</p> <div class="tabbed-set tabbed-alternate" data-tabs=25:2><input checked=checked id=216-python-pytorch_1 name=__tabbed_25 type=radio><input id=216-python-tensorflow_1 name=__tabbed_25 type=radio><div class=tabbed-labels><label for=216-python-pytorch_1>PYTORCH</label><label for=216-python-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-94-1><a id=__codelineno-94-1 name=__codelineno-94-1 href=#__codelineno-94-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>3.5</span><span class=p>])</span>
</span><span id=__span-94-2><a id=__codelineno-94-2 name=__codelineno-94-2 href=#__codelineno-94-2></a><span class=n>a</span><span class=p>,</span> <span class=n>a</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=nb>float</span><span class=p>(</span><span class=n>a</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-95-1><a id=__codelineno-95-1 name=__codelineno-95-1 href=#__codelineno-95-1></a>(tensor([3.5000]), 3.5, 3.5, 3)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-96-1><a id=__codelineno-96-1 name=__codelineno-96-1 href=#__codelineno-96-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>3.5</span><span class=p>])</span>
</span><span id=__span-96-2><a id=__codelineno-96-2 name=__codelineno-96-2 href=#__codelineno-96-2></a><span class=n>a</span><span class=p>,</span> <span class=n>a</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=nb>float</span><span class=p>(</span><span class=n>a</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-97-1><a id=__codelineno-97-1 name=__codelineno-97-1 href=#__codelineno-97-1></a>(&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.5], dtype=float32)&gt;,
</span><span id=__span-97-2><a id=__codelineno-97-2 name=__codelineno-97-2 href=#__codelineno-97-2></a> array([3.5], dtype=float32),
</span><span id=__span-97-3><a id=__codelineno-97-3 name=__codelineno-97-3 href=#__codelineno-97-3></a> 3.5,
</span><span id=__span-97-4><a id=__codelineno-97-4 name=__codelineno-97-4 href=#__codelineno-97-4></a> 3)
</span></code></pre></div> </div> </div> </div> <h3 id=217>2.1.7 小结<a class=headerlink href=#217 title="Permanent link">&para;</a></h3> <p>深度学习存储和操作数据的主要接口是张量（n维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。</p> <h2 id=22>2.2 数据预处理<a class=headerlink href=#22 title="Permanent link">&para;</a></h2> <p>为了能用深度学习来解决现实世界的问题，我们经常从预处理原始数据开始， 而不是从那些准备好的张量格式数据开始。 在Python中常用的数据分析工具中，我们通常使用pandas软件包。 像庞大的Python生态系统中的许多其他扩展包一样，pandas可以与张量兼容。 本节我们将简要介绍使用pandas预处理原始数据，并将原始数据转换为张量格式的步骤。 后面的章节将介绍更多的数据预处理技术。</p> <h3 id=221>2.2.1 读取数据集<a class=headerlink href=#221 title="Permanent link">&para;</a></h3> <p>举一个例子，我们首先创建一个人工数据集，并存储在CSV（逗号分隔值）文件 ../data/house_tiny.csv中。 以其他格式存储的数据也可以通过类似的方式进行处理。 下面我们将数据集按行写入CSV文件中。</p> <p><div class="language-python highlight"><pre><span></span><code><span id=__span-98-1><a id=__codelineno-98-1 name=__codelineno-98-1 href=#__codelineno-98-1></a><span class=kn>import</span> <span class=nn>os</span>
</span><span id=__span-98-2><a id=__codelineno-98-2 name=__codelineno-98-2 href=#__codelineno-98-2></a>
</span><span id=__span-98-3><a id=__codelineno-98-3 name=__codelineno-98-3 href=#__codelineno-98-3></a><span class=n>os</span><span class=o>.</span><span class=n>makedirs</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s1>&#39;..&#39;</span><span class=p>,</span> <span class=s1>&#39;data&#39;</span><span class=p>),</span> <span class=n>exist_ok</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-98-4><a id=__codelineno-98-4 name=__codelineno-98-4 href=#__codelineno-98-4></a><span class=n>data_file</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s1>&#39;..&#39;</span><span class=p>,</span> <span class=s1>&#39;data&#39;</span><span class=p>,</span> <span class=s1>&#39;house_tiny.csv&#39;</span><span class=p>)</span>
</span><span id=__span-98-5><a id=__codelineno-98-5 name=__codelineno-98-5 href=#__codelineno-98-5></a><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>data_file</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span><span id=__span-98-6><a id=__codelineno-98-6 name=__codelineno-98-6 href=#__codelineno-98-6></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;NumRooms,Alley,Price</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>  <span class=c1># 列名</span>
</span><span id=__span-98-7><a id=__codelineno-98-7 name=__codelineno-98-7 href=#__codelineno-98-7></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;NA,Pave,127500</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>  <span class=c1># 每行表示一个数据样本</span>
</span><span id=__span-98-8><a id=__codelineno-98-8 name=__codelineno-98-8 href=#__codelineno-98-8></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;2,NA,106000</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span><span id=__span-98-9><a id=__codelineno-98-9 name=__codelineno-98-9 href=#__codelineno-98-9></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;4,NA,178100</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span><span id=__span-98-10><a id=__codelineno-98-10 name=__codelineno-98-10 href=#__codelineno-98-10></a>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;NA,NA,140000</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span></code></pre></div> 要从创建的CSV文件中加载原始数据集，我们导入pandas包并调用read_csv函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）。</p> <div class="tabbed-set tabbed-alternate" data-tabs=26:2><input checked=checked id=221-pytorch name=__tabbed_26 type=radio><input id=221-tensorflow name=__tabbed_26 type=radio><div class=tabbed-labels><label for=221-pytorch>PYTORCH</label><label for=221-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-99-1><a id=__codelineno-99-1 name=__codelineno-99-1 href=#__codelineno-99-1></a><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=n>pandas</span>
</span><span id=__span-99-2><a id=__codelineno-99-2 name=__codelineno-99-2 href=#__codelineno-99-2></a><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span><span id=__span-99-3><a id=__codelineno-99-3 name=__codelineno-99-3 href=#__codelineno-99-3></a><span class=n>data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>data_file</span><span class=p>)</span>
</span><span id=__span-99-4><a id=__codelineno-99-4 name=__codelineno-99-4 href=#__codelineno-99-4></a><span class=n>data</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-100-1><a id=__codelineno-100-1 name=__codelineno-100-1 href=#__codelineno-100-1></a>   NumRooms Alley   Price
</span><span id=__span-100-2><a id=__codelineno-100-2 name=__codelineno-100-2 href=#__codelineno-100-2></a>0       NaN  Pave  127500
</span><span id=__span-100-3><a id=__codelineno-100-3 name=__codelineno-100-3 href=#__codelineno-100-3></a>1       2.0   NaN  106000
</span><span id=__span-100-4><a id=__codelineno-100-4 name=__codelineno-100-4 href=#__codelineno-100-4></a>2       4.0   NaN  178100
</span><span id=__span-100-5><a id=__codelineno-100-5 name=__codelineno-100-5 href=#__codelineno-100-5></a>3       NaN   NaN  140000
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-101-1><a id=__codelineno-101-1 name=__codelineno-101-1 href=#__codelineno-101-1></a><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=n>pandas</span>
</span><span id=__span-101-2><a id=__codelineno-101-2 name=__codelineno-101-2 href=#__codelineno-101-2></a><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span><span id=__span-101-3><a id=__codelineno-101-3 name=__codelineno-101-3 href=#__codelineno-101-3></a><span class=n>data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>data_file</span><span class=p>)</span>
</span><span id=__span-101-4><a id=__codelineno-101-4 name=__codelineno-101-4 href=#__codelineno-101-4></a><span class=n>data</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-102-1><a id=__codelineno-102-1 name=__codelineno-102-1 href=#__codelineno-102-1></a>   NumRooms Alley   Price
</span><span id=__span-102-2><a id=__codelineno-102-2 name=__codelineno-102-2 href=#__codelineno-102-2></a>0       NaN  Pave  127500
</span><span id=__span-102-3><a id=__codelineno-102-3 name=__codelineno-102-3 href=#__codelineno-102-3></a>1       2.0   NaN  106000
</span><span id=__span-102-4><a id=__codelineno-102-4 name=__codelineno-102-4 href=#__codelineno-102-4></a>2       4.0   NaN  178100
</span><span id=__span-102-5><a id=__codelineno-102-5 name=__codelineno-102-5 href=#__codelineno-102-5></a>3       NaN   NaN  140000
</span></code></pre></div> </div> </div> </div> <h3 id=222>2.2.2 处理缺失值<a class=headerlink href=#222 title="Permanent link">&para;</a></h3> <p>注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括插值法和删除法， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 在这里，我们将考虑插值法。</p> <p>通过位置索引iloc，我们将data分成inputs和outputs， 其中前者为data的前两列，而后者为data的最后一列。 对于inputs中缺少的数值，我们用同一列的均值替换“NaN”项。</p> <div class="tabbed-set tabbed-alternate" data-tabs=27:2><input checked=checked id=222-pytorch name=__tabbed_27 type=radio><input id=222-tensorflow name=__tabbed_27 type=radio><div class=tabbed-labels><label for=222-pytorch>PYTORCH</label><label for=222-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-103-1><a id=__codelineno-103-1 name=__codelineno-103-1 href=#__codelineno-103-1></a><span class=n>inputs</span><span class=p>,</span> <span class=n>outputs</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>:</span><span class=mi>2</span><span class=p>],</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>
</span><span id=__span-103-2><a id=__codelineno-103-2 name=__codelineno-103-2 href=#__codelineno-103-2></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>fillna</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
</span><span id=__span-103-3><a id=__codelineno-103-3 name=__codelineno-103-3 href=#__codelineno-103-3></a><span class=n>inputs</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-104-1><a id=__codelineno-104-1 name=__codelineno-104-1 href=#__codelineno-104-1></a>   NumRooms Alley
</span><span id=__span-104-2><a id=__codelineno-104-2 name=__codelineno-104-2 href=#__codelineno-104-2></a>0       3.0  Pave
</span><span id=__span-104-3><a id=__codelineno-104-3 name=__codelineno-104-3 href=#__codelineno-104-3></a>1       2.0   NaN
</span><span id=__span-104-4><a id=__codelineno-104-4 name=__codelineno-104-4 href=#__codelineno-104-4></a>2       4.0   NaN
</span><span id=__span-104-5><a id=__codelineno-104-5 name=__codelineno-104-5 href=#__codelineno-104-5></a>3       3.0   NaN
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-105-1><a id=__codelineno-105-1 name=__codelineno-105-1 href=#__codelineno-105-1></a><span class=n>inputs</span><span class=p>,</span> <span class=n>outputs</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>:</span><span class=mi>2</span><span class=p>],</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>
</span><span id=__span-105-2><a id=__codelineno-105-2 name=__codelineno-105-2 href=#__codelineno-105-2></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>fillna</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
</span><span id=__span-105-3><a id=__codelineno-105-3 name=__codelineno-105-3 href=#__codelineno-105-3></a><span class=n>inputs</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-106-1><a id=__codelineno-106-1 name=__codelineno-106-1 href=#__codelineno-106-1></a>   NumRooms Alley
</span><span id=__span-106-2><a id=__codelineno-106-2 name=__codelineno-106-2 href=#__codelineno-106-2></a>0       3.0  Pave
</span><span id=__span-106-3><a id=__codelineno-106-3 name=__codelineno-106-3 href=#__codelineno-106-3></a>1       2.0   NaN
</span><span id=__span-106-4><a id=__codelineno-106-4 name=__codelineno-106-4 href=#__codelineno-106-4></a>2       4.0   NaN
</span><span id=__span-106-5><a id=__codelineno-106-5 name=__codelineno-106-5 href=#__codelineno-106-5></a>3       3.0   NaN
</span></code></pre></div> </div> </div> </div> <p>对于inputs中的类别值或离散值，我们将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， pandas可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。</p> <div class="tabbed-set tabbed-alternate" data-tabs=28:2><input checked=checked id=222-pytorch_1 name=__tabbed_28 type=radio><input id=222-tensorflow_1 name=__tabbed_28 type=radio><div class=tabbed-labels><label for=222-pytorch_1>PYTORCH</label><label for=222-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-107-1><a id=__codelineno-107-1 name=__codelineno-107-1 href=#__codelineno-107-1></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>dummy_na</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-107-2><a id=__codelineno-107-2 name=__codelineno-107-2 href=#__codelineno-107-2></a><span class=n>inputs</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-108-1><a id=__codelineno-108-1 name=__codelineno-108-1 href=#__codelineno-108-1></a>   NumRooms  Alley_Pave  Alley_nan
</span><span id=__span-108-2><a id=__codelineno-108-2 name=__codelineno-108-2 href=#__codelineno-108-2></a>0       3.0           1          0
</span><span id=__span-108-3><a id=__codelineno-108-3 name=__codelineno-108-3 href=#__codelineno-108-3></a>1       2.0           0          1
</span><span id=__span-108-4><a id=__codelineno-108-4 name=__codelineno-108-4 href=#__codelineno-108-4></a>2       4.0           0          1
</span><span id=__span-108-5><a id=__codelineno-108-5 name=__codelineno-108-5 href=#__codelineno-108-5></a>3       3.0           0          1
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-109-1><a id=__codelineno-109-1 name=__codelineno-109-1 href=#__codelineno-109-1></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>dummy_na</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-109-2><a id=__codelineno-109-2 name=__codelineno-109-2 href=#__codelineno-109-2></a><span class=n>inputs</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-110-1><a id=__codelineno-110-1 name=__codelineno-110-1 href=#__codelineno-110-1></a>   NumRooms  Alley_Pave  Alley_nan
</span><span id=__span-110-2><a id=__codelineno-110-2 name=__codelineno-110-2 href=#__codelineno-110-2></a>0       3.0           1          0
</span><span id=__span-110-3><a id=__codelineno-110-3 name=__codelineno-110-3 href=#__codelineno-110-3></a>1       2.0           0          1
</span><span id=__span-110-4><a id=__codelineno-110-4 name=__codelineno-110-4 href=#__codelineno-110-4></a>2       4.0           0          1
</span><span id=__span-110-5><a id=__codelineno-110-5 name=__codelineno-110-5 href=#__codelineno-110-5></a>3       3.0           0          1
</span></code></pre></div> </div> </div> </div> <h3 id=223>2.2.3 转换为张量格式<a class=headerlink href=#223 title="Permanent link">&para;</a></h3> <p>现在inputs和outputs中的所有条目都是数值类型，它们可以转换为张量格式。 当数据采用张量格式后，我们可以(<strong>通过在 :numref:<code>sec_ndarray</code> 中引入的张量逻辑来(</strong>(<strong>访问任意元素。</strong>)**)</p> <div class="tabbed-set tabbed-alternate" data-tabs=29:2><input checked=checked id=223-pytorch name=__tabbed_29 type=radio><input id=223-tensorflow name=__tabbed_29 type=radio><div class=tabbed-labels><label for=223-pytorch>PYTORCH</label><label for=223-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-111-1><a id=__codelineno-111-1 name=__codelineno-111-1 href=#__codelineno-111-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-111-2><a id=__codelineno-111-2 name=__codelineno-111-2 href=#__codelineno-111-2></a><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>values</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>values</span><span class=p>)</span>
</span><span id=__span-111-3><a id=__codelineno-111-3 name=__codelineno-111-3 href=#__codelineno-111-3></a><span class=n>X</span><span class=p>,</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-112-1><a id=__codelineno-112-1 name=__codelineno-112-1 href=#__codelineno-112-1></a>(tensor([[3., 1., 0.],
</span><span id=__span-112-2><a id=__codelineno-112-2 name=__codelineno-112-2 href=#__codelineno-112-2></a>         [2., 0., 1.],
</span><span id=__span-112-3><a id=__codelineno-112-3 name=__codelineno-112-3 href=#__codelineno-112-3></a>         [4., 0., 1.],
</span><span id=__span-112-4><a id=__codelineno-112-4 name=__codelineno-112-4 href=#__codelineno-112-4></a>         [3., 0., 1.]], dtype=torch.float64),
</span><span id=__span-112-5><a id=__codelineno-112-5 name=__codelineno-112-5 href=#__codelineno-112-5></a> tensor([127500, 106000, 178100, 140000]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-113-1><a id=__codelineno-113-1 name=__codelineno-113-1 href=#__codelineno-113-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-113-2><a id=__codelineno-113-2 name=__codelineno-113-2 href=#__codelineno-113-2></a><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>values</span><span class=p>),</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>values</span><span class=p>)</span>
</span><span id=__span-113-3><a id=__codelineno-113-3 name=__codelineno-113-3 href=#__codelineno-113-3></a><span class=n>X</span><span class=p>,</span> <span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-114-1><a id=__codelineno-114-1 name=__codelineno-114-1 href=#__codelineno-114-1></a>(&lt;tf.Tensor: shape=(4, 3), dtype=float64, numpy=
</span><span id=__span-114-2><a id=__codelineno-114-2 name=__codelineno-114-2 href=#__codelineno-114-2></a>array([[3., 1., 0.],
</span><span id=__span-114-3><a id=__codelineno-114-3 name=__codelineno-114-3 href=#__codelineno-114-3></a>       [2., 0., 1.],
</span><span id=__span-114-4><a id=__codelineno-114-4 name=__codelineno-114-4 href=#__codelineno-114-4></a>       [4., 0., 1.],
</span><span id=__span-114-5><a id=__codelineno-114-5 name=__codelineno-114-5 href=#__codelineno-114-5></a>       [3., 0., 1.]])&gt;,
</span><span id=__span-114-6><a id=__codelineno-114-6 name=__codelineno-114-6 href=#__codelineno-114-6></a> &lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([127500, 106000, 178100, 140000])&gt;)
</span></code></pre></div> </div> </div> </div> <h3 id=224>2.2.4 小结<a class=headerlink href=#224 title="Permanent link">&para;</a></h3> <ul> <li> <p>pandas软件包是Python中常用的数据分析工具中，pandas可以与张量兼容。</p> </li> <li> <p>用pandas处理缺失的数据时，我们可根据情况选择用插值法和删除法。</p> </li> </ul> <h2 id=23>2.3 线性代数<a class=headerlink href=#23 title="Permanent link">&para;</a></h2> <p>在介绍完如何存储和操作数据后，接下来将简要地回顾一下部分基本线性代数内容。 这些内容有助于读者了解和实现本书中介绍的大多数模型。 本节将介绍线性代数中的基本数学对象、算术和运算，并用数学符号和相应的代码实现来表示它们。</p> <h3 id=231>2.3.1 标量<a class=headerlink href=#231 title="Permanent link">&para;</a></h3> <p>标量由只有一个元素的张量表示。 下面的代码将实例化两个标量，并执行一些熟悉的算术运算，即加法、乘法、除法和指数。</p> <div class="tabbed-set tabbed-alternate" data-tabs=30:2><input checked=checked id=231-pytorch name=__tabbed_30 type=radio><input id=231-tensorflow name=__tabbed_30 type=radio><div class=tabbed-labels><label for=231-pytorch>PYTORCH</label><label for=231-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-115-1><a id=__codelineno-115-1 name=__codelineno-115-1 href=#__codelineno-115-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-115-2><a id=__codelineno-115-2 name=__codelineno-115-2 href=#__codelineno-115-2></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>3.0</span><span class=p>)</span>
</span><span id=__span-115-3><a id=__codelineno-115-3 name=__codelineno-115-3 href=#__codelineno-115-3></a><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span><span class=p>)</span>
</span><span id=__span-115-4><a id=__codelineno-115-4 name=__codelineno-115-4 href=#__codelineno-115-4></a>
</span><span id=__span-115-5><a id=__codelineno-115-5 name=__codelineno-115-5 href=#__codelineno-115-5></a><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>/</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=o>**</span><span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-116-1><a id=__codelineno-116-1 name=__codelineno-116-1 href=#__codelineno-116-1></a>(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-117-1><a id=__codelineno-117-1 name=__codelineno-117-1 href=#__codelineno-117-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-117-2><a id=__codelineno-117-2 name=__codelineno-117-2 href=#__codelineno-117-2></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=mf>3.0</span><span class=p>)</span>
</span><span id=__span-117-3><a id=__codelineno-117-3 name=__codelineno-117-3 href=#__codelineno-117-3></a><span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=mf>2.0</span><span class=p>)</span>
</span><span id=__span-117-4><a id=__codelineno-117-4 name=__codelineno-117-4 href=#__codelineno-117-4></a>
</span><span id=__span-117-5><a id=__codelineno-117-5 name=__codelineno-117-5 href=#__codelineno-117-5></a><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>/</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=o>**</span><span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-118-1><a id=__codelineno-118-1 name=__codelineno-118-1 href=#__codelineno-118-1></a>(&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)&gt;,
</span><span id=__span-118-2><a id=__codelineno-118-2 name=__codelineno-118-2 href=#__codelineno-118-2></a> &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)&gt;,
</span><span id=__span-118-3><a id=__codelineno-118-3 name=__codelineno-118-3 href=#__codelineno-118-3></a> &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.5], dtype=float32)&gt;,
</span><span id=__span-118-4><a id=__codelineno-118-4 name=__codelineno-118-4 href=#__codelineno-118-4></a> &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([9.], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <h3 id=232>2.3.2 向量<a class=headerlink href=#232 title="Permanent link">&para;</a></h3> <p>向量可以被视为标量值组成的列表。 这些标量值被称为向量的元素（element）或分量（component）。 当向量表示数据集中的样本时，它们的值具有一定的现实意义。在数学表示法中，我们通常用粗体、小写字母表示向量。 </p> <div class="tabbed-set tabbed-alternate" data-tabs=31:2><input checked=checked id=232-pytorch name=__tabbed_31 type=radio><input id=232-tensorflow name=__tabbed_31 type=radio><div class=tabbed-labels><label for=232-pytorch>PYTORCH</label><label for=232-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-119-1><a id=__codelineno-119-1 name=__codelineno-119-1 href=#__codelineno-119-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-119-2><a id=__codelineno-119-2 name=__codelineno-119-2 href=#__codelineno-119-2></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>4</span><span class=p>)</span>
</span><span id=__span-119-3><a id=__codelineno-119-3 name=__codelineno-119-3 href=#__codelineno-119-3></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-120-1><a id=__codelineno-120-1 name=__codelineno-120-1 href=#__codelineno-120-1></a>tensor([0, 1, 2, 3])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-121-1><a id=__codelineno-121-1 name=__codelineno-121-1 href=#__codelineno-121-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-121-2><a id=__codelineno-121-2 name=__codelineno-121-2 href=#__codelineno-121-2></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>4</span><span class=p>)</span>
</span><span id=__span-121-3><a id=__codelineno-121-3 name=__codelineno-121-3 href=#__codelineno-121-3></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-122-1><a id=__codelineno-122-1 name=__codelineno-122-1 href=#__codelineno-122-1></a>&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>我们可以使用下标来引用向量的任一元素.</p> <div class="tabbed-set tabbed-alternate" data-tabs=32:2><input checked=checked id=232-pytorch_1 name=__tabbed_32 type=radio><input id=232-tensorflow_1 name=__tabbed_32 type=radio><div class=tabbed-labels><label for=232-pytorch_1>PYTORCH</label><label for=232-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-123-1><a id=__codelineno-123-1 name=__codelineno-123-1 href=#__codelineno-123-1></a><span class=n>x</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-124-1><a id=__codelineno-124-1 name=__codelineno-124-1 href=#__codelineno-124-1></a>tensor(3)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-125-1><a id=__codelineno-125-1 name=__codelineno-125-1 href=#__codelineno-125-1></a><span class=n>x</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-126-1><a id=__codelineno-126-1 name=__codelineno-126-1 href=#__codelineno-126-1></a>&lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt;
</span></code></pre></div> </div> </div> </div> <h4 id=2321>2.3.2.1 长度、维度和形状<a class=headerlink href=#2321 title="Permanent link">&para;</a></h4> <p>向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。 向量的长度通常称为维度。</p> <p>与普通的Python数组一样，我们可以通过调用Python的内置len()函数来访问张量的长度。</p> <div class="tabbed-set tabbed-alternate" data-tabs=33:2><input checked=checked id=2321-pytorch name=__tabbed_33 type=radio><input id=2321-tensorflow name=__tabbed_33 type=radio><div class=tabbed-labels><label for=2321-pytorch>PYTORCH</label><label for=2321-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-127-1><a id=__codelineno-127-1 name=__codelineno-127-1 href=#__codelineno-127-1></a><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-128-1><a id=__codelineno-128-1 name=__codelineno-128-1 href=#__codelineno-128-1></a>4
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-129-1><a id=__codelineno-129-1 name=__codelineno-129-1 href=#__codelineno-129-1></a><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-130-1><a id=__codelineno-130-1 name=__codelineno-130-1 href=#__codelineno-130-1></a>4
</span></code></pre></div> </div> </div> </div> <p>当用张量表示一个向量（只有一个轴）时，我们也可以通过.shape属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。 对于只有一个轴的张量，形状只有一个元素。</p> <div class="tabbed-set tabbed-alternate" data-tabs=34:2><input checked=checked id=2321-pytorch_1 name=__tabbed_34 type=radio><input id=2321-tensorflow_1 name=__tabbed_34 type=radio><div class=tabbed-labels><label for=2321-pytorch_1>PYTORCH</label><label for=2321-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-131-1><a id=__codelineno-131-1 name=__codelineno-131-1 href=#__codelineno-131-1></a><span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-132-1><a id=__codelineno-132-1 name=__codelineno-132-1 href=#__codelineno-132-1></a>torch.Size([4])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-133-1><a id=__codelineno-133-1 name=__codelineno-133-1 href=#__codelineno-133-1></a><span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-134-1><a id=__codelineno-134-1 name=__codelineno-134-1 href=#__codelineno-134-1></a>TensorShape([4])
</span></code></pre></div> </div> </div> </div> <p>请注意，维度（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。 为了清楚起见，我们在此明确一下： 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。</p> <h3 id=233>2.3.3 矩阵<a class=headerlink href=#233 title="Permanent link">&para;</a></h3> <p>正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。矩阵，在代码中表示为具有两个轴的张量。当调用函数来实例化张量时， 我们可以通过指定两个分量m和n来创建一个形状为mxn的矩阵。</p> <div class="tabbed-set tabbed-alternate" data-tabs=35:2><input checked=checked id=233-pytorch name=__tabbed_35 type=radio><input id=233-tensorflow name=__tabbed_35 type=radio><div class=tabbed-labels><label for=233-pytorch>PYTORCH</label><label for=233-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-135-1><a id=__codelineno-135-1 name=__codelineno-135-1 href=#__codelineno-135-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-135-2><a id=__codelineno-135-2 name=__codelineno-135-2 href=#__codelineno-135-2></a><span class=n>A</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-135-3><a id=__codelineno-135-3 name=__codelineno-135-3 href=#__codelineno-135-3></a><span class=n>A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-136-1><a id=__codelineno-136-1 name=__codelineno-136-1 href=#__codelineno-136-1></a>tensor([[ 0,  1,  2,  3],
</span><span id=__span-136-2><a id=__codelineno-136-2 name=__codelineno-136-2 href=#__codelineno-136-2></a>        [ 4,  5,  6,  7],
</span><span id=__span-136-3><a id=__codelineno-136-3 name=__codelineno-136-3 href=#__codelineno-136-3></a>        [ 8,  9, 10, 11],
</span><span id=__span-136-4><a id=__codelineno-136-4 name=__codelineno-136-4 href=#__codelineno-136-4></a>        [12, 13, 14, 15],
</span><span id=__span-136-5><a id=__codelineno-136-5 name=__codelineno-136-5 href=#__codelineno-136-5></a>        [16, 17, 18, 19]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-137-1><a id=__codelineno-137-1 name=__codelineno-137-1 href=#__codelineno-137-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-137-2><a id=__codelineno-137-2 name=__codelineno-137-2 href=#__codelineno-137-2></a><span class=n>A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>20</span><span class=p>),</span> <span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span><span id=__span-137-3><a id=__codelineno-137-3 name=__codelineno-137-3 href=#__codelineno-137-3></a><span class=n>A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-138-1><a id=__codelineno-138-1 name=__codelineno-138-1 href=#__codelineno-138-1></a>&lt;tf.Tensor: shape=(5, 4), dtype=int32, numpy=
</span><span id=__span-138-2><a id=__codelineno-138-2 name=__codelineno-138-2 href=#__codelineno-138-2></a>array([[ 0,  1,  2,  3],
</span><span id=__span-138-3><a id=__codelineno-138-3 name=__codelineno-138-3 href=#__codelineno-138-3></a>       [ 4,  5,  6,  7],
</span><span id=__span-138-4><a id=__codelineno-138-4 name=__codelineno-138-4 href=#__codelineno-138-4></a>       [ 8,  9, 10, 11],
</span><span id=__span-138-5><a id=__codelineno-138-5 name=__codelineno-138-5 href=#__codelineno-138-5></a>       [12, 13, 14, 15],
</span><span id=__span-138-6><a id=__codelineno-138-6 name=__codelineno-138-6 href=#__codelineno-138-6></a>       [16, 17, 18, 19]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>我们可以通过行索引（row）和列索引（column）来访问矩阵中的标量元素。 为了表示矩阵<code>A</code>中行索引为<code>i</code>、列索引为<code>j</code>的元素，我们通常使用标量记法<span class=arithmatex>\(a_{i,j}\)</span>。</p> <p>当我们交换矩阵的行和列时，结果称为矩阵的转置（transpose）。</p> <div class="tabbed-set tabbed-alternate" data-tabs=36:2><input checked=checked id=233-pytorch_1 name=__tabbed_36 type=radio><input id=233-tensorflow_1 name=__tabbed_36 type=radio><div class=tabbed-labels><label for=233-pytorch_1>PYTORCH</label><label for=233-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-139-1><a id=__codelineno-139-1 name=__codelineno-139-1 href=#__codelineno-139-1></a><span class=n>A</span><span class=o>.</span><span class=n>T</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-140-1><a id=__codelineno-140-1 name=__codelineno-140-1 href=#__codelineno-140-1></a>tensor([[ 0,  4,  8, 12, 16],
</span><span id=__span-140-2><a id=__codelineno-140-2 name=__codelineno-140-2 href=#__codelineno-140-2></a>        [ 1,  5,  9, 13, 17],
</span><span id=__span-140-3><a id=__codelineno-140-3 name=__codelineno-140-3 href=#__codelineno-140-3></a>        [ 2,  6, 10, 14, 18],
</span><span id=__span-140-4><a id=__codelineno-140-4 name=__codelineno-140-4 href=#__codelineno-140-4></a>        [ 3,  7, 11, 15, 19]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-141-1><a id=__codelineno-141-1 name=__codelineno-141-1 href=#__codelineno-141-1></a><span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-142-1><a id=__codelineno-142-1 name=__codelineno-142-1 href=#__codelineno-142-1></a>&lt;tf.Tensor: shape=(4, 5), dtype=int32, numpy=
</span><span id=__span-142-2><a id=__codelineno-142-2 name=__codelineno-142-2 href=#__codelineno-142-2></a>array([[ 0,  4,  8, 12, 16],
</span><span id=__span-142-3><a id=__codelineno-142-3 name=__codelineno-142-3 href=#__codelineno-142-3></a>       [ 1,  5,  9, 13, 17],
</span><span id=__span-142-4><a id=__codelineno-142-4 name=__codelineno-142-4 href=#__codelineno-142-4></a>       [ 2,  6, 10, 14, 18],
</span><span id=__span-142-5><a id=__codelineno-142-5 name=__codelineno-142-5 href=#__codelineno-142-5></a>       [ 3,  7, 11, 15, 19]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=234>2.3.4 张量<a class=headerlink href=#234 title="Permanent link">&para;</a></h3> <p>就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的n维数组的通用方法。 例如，向量是一阶张量，矩阵是二阶张量。</p> <div class="tabbed-set tabbed-alternate" data-tabs=37:2><input checked=checked id=234-pytorch name=__tabbed_37 type=radio><input id=234-tensorflow name=__tabbed_37 type=radio><div class=tabbed-labels><label for=234-pytorch>PYTORCH</label><label for=234-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-143-1><a id=__codelineno-143-1 name=__codelineno-143-1 href=#__codelineno-143-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-143-2><a id=__codelineno-143-2 name=__codelineno-143-2 href=#__codelineno-143-2></a><span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>24</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-143-3><a id=__codelineno-143-3 name=__codelineno-143-3 href=#__codelineno-143-3></a><span class=n>X</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-144-1><a id=__codelineno-144-1 name=__codelineno-144-1 href=#__codelineno-144-1></a>tensor([[[ 0,  1,  2,  3],
</span><span id=__span-144-2><a id=__codelineno-144-2 name=__codelineno-144-2 href=#__codelineno-144-2></a>         [ 4,  5,  6,  7],
</span><span id=__span-144-3><a id=__codelineno-144-3 name=__codelineno-144-3 href=#__codelineno-144-3></a>         [ 8,  9, 10, 11]],
</span><span id=__span-144-4><a id=__codelineno-144-4 name=__codelineno-144-4 href=#__codelineno-144-4></a>
</span><span id=__span-144-5><a id=__codelineno-144-5 name=__codelineno-144-5 href=#__codelineno-144-5></a>        [[12, 13, 14, 15],
</span><span id=__span-144-6><a id=__codelineno-144-6 name=__codelineno-144-6 href=#__codelineno-144-6></a>         [16, 17, 18, 19],
</span><span id=__span-144-7><a id=__codelineno-144-7 name=__codelineno-144-7 href=#__codelineno-144-7></a>         [20, 21, 22, 23]]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-145-1><a id=__codelineno-145-1 name=__codelineno-145-1 href=#__codelineno-145-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-145-2><a id=__codelineno-145-2 name=__codelineno-145-2 href=#__codelineno-145-2></a><span class=n>X</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>24</span><span class=p>),</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span><span id=__span-145-3><a id=__codelineno-145-3 name=__codelineno-145-3 href=#__codelineno-145-3></a><span class=n>X</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-146-1><a id=__codelineno-146-1 name=__codelineno-146-1 href=#__codelineno-146-1></a>&lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=
</span><span id=__span-146-2><a id=__codelineno-146-2 name=__codelineno-146-2 href=#__codelineno-146-2></a>array([[[ 0,  1,  2,  3],
</span><span id=__span-146-3><a id=__codelineno-146-3 name=__codelineno-146-3 href=#__codelineno-146-3></a>        [ 4,  5,  6,  7],
</span><span id=__span-146-4><a id=__codelineno-146-4 name=__codelineno-146-4 href=#__codelineno-146-4></a>        [ 8,  9, 10, 11]],
</span><span id=__span-146-5><a id=__codelineno-146-5 name=__codelineno-146-5 href=#__codelineno-146-5></a>
</span><span id=__span-146-6><a id=__codelineno-146-6 name=__codelineno-146-6 href=#__codelineno-146-6></a>       [[12, 13, 14, 15],
</span><span id=__span-146-7><a id=__codelineno-146-7 name=__codelineno-146-7 href=#__codelineno-146-7></a>        [16, 17, 18, 19],
</span><span id=__span-146-8><a id=__codelineno-146-8 name=__codelineno-146-8 href=#__codelineno-146-8></a>        [20, 21, 22, 23]]], dtype=int32)&gt;
</span></code></pre></div> </div> </div> </div> <p>张量保留了矩阵的所有性质。 张量具有轴、秩和形状。 一个张量可以通过指定它们的轴数来指定轴的数量。 张量的轴数也称为它的阶（order）、秩（rank）或ndim（表示“number of dimensions”）。 矩阵有两个轴（行和列），因此我们说它的阶为2，它的秩为2，它的ndim为2。 同样，我们可以说，张量的阶、秩或ndim等于它的轴数。</p> <h3 id=235>2.3.5 张量算法的基本性质<a class=headerlink href=#235 title="Permanent link">&para;</a></h3> <p>标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。 例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。 同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。 例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。</p> <div class="tabbed-set tabbed-alternate" data-tabs=38:2><input checked=checked id=235-pytorch name=__tabbed_38 type=radio><input id=235-tensorflow name=__tabbed_38 type=radio><div class=tabbed-labels><label for=235-pytorch>PYTORCH</label><label for=235-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-147-1><a id=__codelineno-147-1 name=__codelineno-147-1 href=#__codelineno-147-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span><span id=__span-147-2><a id=__codelineno-147-2 name=__codelineno-147-2 href=#__codelineno-147-2></a><span class=n>B</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>  <span class=c1># 通过分配新内存，将A的一个副本分配给B</span>
</span><span id=__span-147-3><a id=__codelineno-147-3 name=__codelineno-147-3 href=#__codelineno-147-3></a><span class=n>A</span><span class=p>,</span> <span class=n>A</span> <span class=o>+</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-148-1><a id=__codelineno-148-1 name=__codelineno-148-1 href=#__codelineno-148-1></a>(tensor([[ 0.,  1.,  2.,  3.],
</span><span id=__span-148-2><a id=__codelineno-148-2 name=__codelineno-148-2 href=#__codelineno-148-2></a>         [ 4.,  5.,  6.,  7.],
</span><span id=__span-148-3><a id=__codelineno-148-3 name=__codelineno-148-3 href=#__codelineno-148-3></a>         [ 8.,  9., 10., 11.],
</span><span id=__span-148-4><a id=__codelineno-148-4 name=__codelineno-148-4 href=#__codelineno-148-4></a>         [12., 13., 14., 15.],
</span><span id=__span-148-5><a id=__codelineno-148-5 name=__codelineno-148-5 href=#__codelineno-148-5></a>         [16., 17., 18., 19.]]),
</span><span id=__span-148-6><a id=__codelineno-148-6 name=__codelineno-148-6 href=#__codelineno-148-6></a> tensor([[ 0.,  2.,  4.,  6.],
</span><span id=__span-148-7><a id=__codelineno-148-7 name=__codelineno-148-7 href=#__codelineno-148-7></a>         [ 8., 10., 12., 14.],
</span><span id=__span-148-8><a id=__codelineno-148-8 name=__codelineno-148-8 href=#__codelineno-148-8></a>         [16., 18., 20., 22.],
</span><span id=__span-148-9><a id=__codelineno-148-9 name=__codelineno-148-9 href=#__codelineno-148-9></a>         [24., 26., 28., 30.],
</span><span id=__span-148-10><a id=__codelineno-148-10 name=__codelineno-148-10 href=#__codelineno-148-10></a>         [32., 34., 36., 38.]]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-149-1><a id=__codelineno-149-1 name=__codelineno-149-1 href=#__codelineno-149-1></a><span class=n>A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>),</span> <span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span><span id=__span-149-2><a id=__codelineno-149-2 name=__codelineno-149-2 href=#__codelineno-149-2></a><span class=n>B</span> <span class=o>=</span> <span class=n>A</span>  <span class=c1># 不能通过分配新内存将A克隆到B</span>
</span><span id=__span-149-3><a id=__codelineno-149-3 name=__codelineno-149-3 href=#__codelineno-149-3></a><span class=n>A</span><span class=p>,</span> <span class=n>A</span> <span class=o>+</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-150-1><a id=__codelineno-150-1 name=__codelineno-150-1 href=#__codelineno-150-1></a>(&lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
</span><span id=__span-150-2><a id=__codelineno-150-2 name=__codelineno-150-2 href=#__codelineno-150-2></a>array([[ 0.,  1.,  2.,  3.],
</span><span id=__span-150-3><a id=__codelineno-150-3 name=__codelineno-150-3 href=#__codelineno-150-3></a>       [ 4.,  5.,  6.,  7.],
</span><span id=__span-150-4><a id=__codelineno-150-4 name=__codelineno-150-4 href=#__codelineno-150-4></a>       [ 8.,  9., 10., 11.],
</span><span id=__span-150-5><a id=__codelineno-150-5 name=__codelineno-150-5 href=#__codelineno-150-5></a>       [12., 13., 14., 15.],
</span><span id=__span-150-6><a id=__codelineno-150-6 name=__codelineno-150-6 href=#__codelineno-150-6></a>       [16., 17., 18., 19.]], dtype=float32)&gt;,
</span><span id=__span-150-7><a id=__codelineno-150-7 name=__codelineno-150-7 href=#__codelineno-150-7></a> &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
</span><span id=__span-150-8><a id=__codelineno-150-8 name=__codelineno-150-8 href=#__codelineno-150-8></a>array([[ 0.,  2.,  4.,  6.],
</span><span id=__span-150-9><a id=__codelineno-150-9 name=__codelineno-150-9 href=#__codelineno-150-9></a>       [ 8., 10., 12., 14.],
</span><span id=__span-150-10><a id=__codelineno-150-10 name=__codelineno-150-10 href=#__codelineno-150-10></a>       [16., 18., 20., 22.],
</span><span id=__span-150-11><a id=__codelineno-150-11 name=__codelineno-150-11 href=#__codelineno-150-11></a>       [24., 26., 28., 30.],
</span><span id=__span-150-12><a id=__codelineno-150-12 name=__codelineno-150-12 href=#__codelineno-150-12></a>       [32., 34., 36., 38.]], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <p>具体而言，两个矩阵按元素乘法称为哈达玛积（Hadamard product）（数学符号<span class=arithmatex>\(\odot\)</span>），而不是矩阵乘法。 两个矩阵按元素相乘的结果与两个矩阵的形状相同。</p> <div class="tabbed-set tabbed-alternate" data-tabs=39:2><input checked=checked id=235-pytorch_1 name=__tabbed_39 type=radio><input id=235-tensorflow_1 name=__tabbed_39 type=radio><div class=tabbed-labels><label for=235-pytorch_1>PYTORCH</label><label for=235-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-151-1><a id=__codelineno-151-1 name=__codelineno-151-1 href=#__codelineno-151-1></a><span class=n>A</span> <span class=o>*</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-152-1><a id=__codelineno-152-1 name=__codelineno-152-1 href=#__codelineno-152-1></a>tensor([[  0.,   1.,   4.,   9.],
</span><span id=__span-152-2><a id=__codelineno-152-2 name=__codelineno-152-2 href=#__codelineno-152-2></a>        [ 16.,  25.,  36.,  49.],
</span><span id=__span-152-3><a id=__codelineno-152-3 name=__codelineno-152-3 href=#__codelineno-152-3></a>        [ 64.,  81., 100., 121.],
</span><span id=__span-152-4><a id=__codelineno-152-4 name=__codelineno-152-4 href=#__codelineno-152-4></a>        [144., 169., 196., 225.],
</span><span id=__span-152-5><a id=__codelineno-152-5 name=__codelineno-152-5 href=#__codelineno-152-5></a>        [256., 289., 324., 361.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-153-1><a id=__codelineno-153-1 name=__codelineno-153-1 href=#__codelineno-153-1></a><span class=n>A</span> <span class=o>*</span> <span class=n>B</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-154-1><a id=__codelineno-154-1 name=__codelineno-154-1 href=#__codelineno-154-1></a>&lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
</span><span id=__span-154-2><a id=__codelineno-154-2 name=__codelineno-154-2 href=#__codelineno-154-2></a>array([[  0.,   1.,   4.,   9.],
</span><span id=__span-154-3><a id=__codelineno-154-3 name=__codelineno-154-3 href=#__codelineno-154-3></a>       [ 16.,  25.,  36.,  49.],
</span><span id=__span-154-4><a id=__codelineno-154-4 name=__codelineno-154-4 href=#__codelineno-154-4></a>       [ 64.,  81., 100., 121.],
</span><span id=__span-154-5><a id=__codelineno-154-5 name=__codelineno-154-5 href=#__codelineno-154-5></a>       [144., 169., 196., 225.],
</span><span id=__span-154-6><a id=__codelineno-154-6 name=__codelineno-154-6 href=#__codelineno-154-6></a>       [256., 289., 324., 361.]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=236>2.3.6 降维<a class=headerlink href=#236 title="Permanent link">&para;</a></h3> <p>我们可以对任意张量进行的一个有用的操作是计算其元素的和。在代码中可以调用计算求和的函数：</p> <div class="tabbed-set tabbed-alternate" data-tabs=40:2><input checked=checked id=236-pytorch name=__tabbed_40 type=radio><input id=236-tensorflow name=__tabbed_40 type=radio><div class=tabbed-labels><label for=236-pytorch>PYTORCH</label><label for=236-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-155-1><a id=__codelineno-155-1 name=__codelineno-155-1 href=#__codelineno-155-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-155-2><a id=__codelineno-155-2 name=__codelineno-155-2 href=#__codelineno-155-2></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span><span id=__span-155-3><a id=__codelineno-155-3 name=__codelineno-155-3 href=#__codelineno-155-3></a><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-156-1><a id=__codelineno-156-1 name=__codelineno-156-1 href=#__codelineno-156-1></a>(tensor([0., 1., 2., 3.]), tensor(6.))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-157-1><a id=__codelineno-157-1 name=__codelineno-157-1 href=#__codelineno-157-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-157-2><a id=__codelineno-157-2 name=__codelineno-157-2 href=#__codelineno-157-2></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span><span id=__span-157-3><a id=__codelineno-157-3 name=__codelineno-157-3 href=#__codelineno-157-3></a><span class=n>x</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-158-1><a id=__codelineno-158-1 name=__codelineno-158-1 href=#__codelineno-158-1></a>(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)&gt;,
</span><span id=__span-158-2><a id=__codelineno-158-2 name=__codelineno-158-2 href=#__codelineno-158-2></a> &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;)
</span></code></pre></div> </div> </div> </div> <p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。</p> <div class="tabbed-set tabbed-alternate" data-tabs=41:2><input checked=checked id=236-pytorch_1 name=__tabbed_41 type=radio><input id=236-tensorflow_1 name=__tabbed_41 type=radio><div class=tabbed-labels><label for=236-pytorch_1>PYTORCH</label><label for=236-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-159-1><a id=__codelineno-159-1 name=__codelineno-159-1 href=#__codelineno-159-1></a><span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-160-1><a id=__codelineno-160-1 name=__codelineno-160-1 href=#__codelineno-160-1></a>(torch.Size([5, 4]), tensor(190.))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-161-1><a id=__codelineno-161-1 name=__codelineno-161-1 href=#__codelineno-161-1></a><span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-162-1><a id=__codelineno-162-1 name=__codelineno-162-1 href=#__codelineno-162-1></a>(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([40., 45., 50., 55.], dtype=float32)&gt;, TensorShape([4]))
</span></code></pre></div> </div> </div> </div> <p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。</p> <div class="tabbed-set tabbed-alternate" data-tabs=42:2><input checked=checked id=236-pytorch_2 name=__tabbed_42 type=radio><input id=236-tensorflow_2 name=__tabbed_42 type=radio><div class=tabbed-labels><label for=236-pytorch_2>PYTORCH</label><label for=236-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-163-1><a id=__codelineno-163-1 name=__codelineno-163-1 href=#__codelineno-163-1></a><span class=n>A_sum_axis1</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-163-2><a id=__codelineno-163-2 name=__codelineno-163-2 href=#__codelineno-163-2></a><span class=n>A_sum_axis1</span><span class=p>,</span> <span class=n>A_sum_axis1</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-164-1><a id=__codelineno-164-1 name=__codelineno-164-1 href=#__codelineno-164-1></a>(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-165-1><a id=__codelineno-165-1 name=__codelineno-165-1 href=#__codelineno-165-1></a><span class=n>A_sum_axis1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-165-2><a id=__codelineno-165-2 name=__codelineno-165-2 href=#__codelineno-165-2></a><span class=n>A_sum_axis1</span><span class=p>,</span> <span class=n>A_sum_axis1</span><span class=o>.</span><span class=n>shape</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-166-1><a id=__codelineno-166-1 name=__codelineno-166-1 href=#__codelineno-166-1></a>(&lt;tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 6., 22., 38., 54., 70.], dtype=float32)&gt;,
</span><span id=__span-166-2><a id=__codelineno-166-2 name=__codelineno-166-2 href=#__codelineno-166-2></a> TensorShape([5]))
</span></code></pre></div> </div> </div> </div> <p>沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。</p> <div class="tabbed-set tabbed-alternate" data-tabs=43:2><input checked=checked id=236-pytorch_3 name=__tabbed_43 type=radio><input id=236-tensorflow_3 name=__tabbed_43 type=radio><div class=tabbed-labels><label for=236-pytorch_3>PYTORCH</label><label for=236-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-167-1><a id=__codelineno-167-1 name=__codelineno-167-1 href=#__codelineno-167-1></a><span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-168-1><a id=__codelineno-168-1 name=__codelineno-168-1 href=#__codelineno-168-1></a>tensor(190.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-169-1><a id=__codelineno-169-1 name=__codelineno-169-1 href=#__codelineno-169-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-170-1><a id=__codelineno-170-1 name=__codelineno-170-1 href=#__codelineno-170-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=190.0&gt;
</span></code></pre></div> </div> </div> </div> <h4 id=2361>2.3.6.1 非降维求和<a class=headerlink href=#2361 title="Permanent link">&para;</a></h4> <p>但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。</p> <div class="tabbed-set tabbed-alternate" data-tabs=44:2><input checked=checked id=2361-pytorch name=__tabbed_44 type=radio><input id=2361-tensorflow name=__tabbed_44 type=radio><div class=tabbed-labels><label for=2361-pytorch>PYTORCH</label><label for=2361-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-171-1><a id=__codelineno-171-1 name=__codelineno-171-1 href=#__codelineno-171-1></a><span class=n>sum_A</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-171-2><a id=__codelineno-171-2 name=__codelineno-171-2 href=#__codelineno-171-2></a><span class=n>sum_A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-172-1><a id=__codelineno-172-1 name=__codelineno-172-1 href=#__codelineno-172-1></a>tensor([[ 6.],
</span><span id=__span-172-2><a id=__codelineno-172-2 name=__codelineno-172-2 href=#__codelineno-172-2></a>        [22.],
</span><span id=__span-172-3><a id=__codelineno-172-3 name=__codelineno-172-3 href=#__codelineno-172-3></a>        [38.],
</span><span id=__span-172-4><a id=__codelineno-172-4 name=__codelineno-172-4 href=#__codelineno-172-4></a>        [54.],
</span><span id=__span-172-5><a id=__codelineno-172-5 name=__codelineno-172-5 href=#__codelineno-172-5></a>        [70.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-173-1><a id=__codelineno-173-1 name=__codelineno-173-1 href=#__codelineno-173-1></a><span class=n>sum_A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-173-2><a id=__codelineno-173-2 name=__codelineno-173-2 href=#__codelineno-173-2></a><span class=n>sum_A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-174-1><a id=__codelineno-174-1 name=__codelineno-174-1 href=#__codelineno-174-1></a>&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=
</span><span id=__span-174-2><a id=__codelineno-174-2 name=__codelineno-174-2 href=#__codelineno-174-2></a>array([[ 6.],
</span><span id=__span-174-3><a id=__codelineno-174-3 name=__codelineno-174-3 href=#__codelineno-174-3></a>       [22.],
</span><span id=__span-174-4><a id=__codelineno-174-4 name=__codelineno-174-4 href=#__codelineno-174-4></a>       [38.],
</span><span id=__span-174-5><a id=__codelineno-174-5 name=__codelineno-174-5 href=#__codelineno-174-5></a>       [54.],
</span><span id=__span-174-6><a id=__codelineno-174-6 name=__codelineno-174-6 href=#__codelineno-174-6></a>       [70.]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>例如，由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A。</p> <div class="tabbed-set tabbed-alternate" data-tabs=45:2><input checked=checked id=2361-pytorch_1 name=__tabbed_45 type=radio><input id=2361-tensorflow_1 name=__tabbed_45 type=radio><div class=tabbed-labels><label for=2361-pytorch_1>PYTORCH</label><label for=2361-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-175-1><a id=__codelineno-175-1 name=__codelineno-175-1 href=#__codelineno-175-1></a><span class=n>A</span> <span class=o>/</span> <span class=n>sum_A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-176-1><a id=__codelineno-176-1 name=__codelineno-176-1 href=#__codelineno-176-1></a>tensor([[0.0000, 0.1667, 0.3333, 0.5000],
</span><span id=__span-176-2><a id=__codelineno-176-2 name=__codelineno-176-2 href=#__codelineno-176-2></a>        [0.1818, 0.2273, 0.2727, 0.3182],
</span><span id=__span-176-3><a id=__codelineno-176-3 name=__codelineno-176-3 href=#__codelineno-176-3></a>        [0.2105, 0.2368, 0.2632, 0.2895],
</span><span id=__span-176-4><a id=__codelineno-176-4 name=__codelineno-176-4 href=#__codelineno-176-4></a>        [0.2222, 0.2407, 0.2593, 0.2778],
</span><span id=__span-176-5><a id=__codelineno-176-5 name=__codelineno-176-5 href=#__codelineno-176-5></a>        [0.2286, 0.2429, 0.2571, 0.2714]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-177-1><a id=__codelineno-177-1 name=__codelineno-177-1 href=#__codelineno-177-1></a><span class=n>A</span> <span class=o>/</span> <span class=n>sum_A</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-178-1><a id=__codelineno-178-1 name=__codelineno-178-1 href=#__codelineno-178-1></a>&lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
</span><span id=__span-178-2><a id=__codelineno-178-2 name=__codelineno-178-2 href=#__codelineno-178-2></a>array([[0.        , 0.16666667, 0.33333334, 0.5       ],
</span><span id=__span-178-3><a id=__codelineno-178-3 name=__codelineno-178-3 href=#__codelineno-178-3></a>       [0.18181819, 0.22727273, 0.27272728, 0.3181818 ],
</span><span id=__span-178-4><a id=__codelineno-178-4 name=__codelineno-178-4 href=#__codelineno-178-4></a>       [0.21052632, 0.23684211, 0.2631579 , 0.28947368],
</span><span id=__span-178-5><a id=__codelineno-178-5 name=__codelineno-178-5 href=#__codelineno-178-5></a>       [0.22222222, 0.24074075, 0.25925925, 0.2777778 ],
</span><span id=__span-178-6><a id=__codelineno-178-6 name=__codelineno-178-6 href=#__codelineno-178-6></a>       [0.22857143, 0.24285714, 0.25714287, 0.2714286 ]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=237>2.3.7 点积<a class=headerlink href=#237 title="Permanent link">&para;</a></h3> <p>我们已经学习了按元素操作、求和及平均值。 另一个最基本的操作之一是点积。给定两个向量，他们的点积是相应元素的乘积的和（**注意：**我们在本书中使用“点积”一词来表示点积）。</p> <div class="tabbed-set tabbed-alternate" data-tabs=46:2><input checked=checked id=237-pytorch name=__tabbed_46 type=radio><input id=237-tensorflow name=__tabbed_46 type=radio><div class=tabbed-labels><label for=237-pytorch>PYTORCH</label><label for=237-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-179-1><a id=__codelineno-179-1 name=__codelineno-179-1 href=#__codelineno-179-1></a><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span><span id=__span-179-2><a id=__codelineno-179-2 name=__codelineno-179-2 href=#__codelineno-179-2></a><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-180-1><a id=__codelineno-180-1 name=__codelineno-180-1 href=#__codelineno-180-1></a>(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-181-1><a id=__codelineno-181-1 name=__codelineno-181-1 href=#__codelineno-181-1></a><span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span><span id=__span-181-2><a id=__codelineno-181-2 name=__codelineno-181-2 href=#__codelineno-181-2></a><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>tensordot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>axes</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-182-1><a id=__codelineno-182-1 name=__codelineno-182-1 href=#__codelineno-182-1></a>(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)&gt;,
</span><span id=__span-182-2><a id=__codelineno-182-2 name=__codelineno-182-2 href=#__codelineno-182-2></a> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)&gt;,
</span><span id=__span-182-3><a id=__codelineno-182-3 name=__codelineno-182-3 href=#__codelineno-182-3></a> &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;)
</span></code></pre></div> </div> </div> </div> <p>点积在很多场合都很有用。 例如，给定一组由向量<span class=arithmatex>\(\mathbf{x} \in \mathbb{R}^d\)</span>给出的值，和一组由向量<span class=arithmatex>\(\mathbf{w} \in \mathbb{R}^d\)</span>给出的权重，我们可以通过元素<span class=arithmatex>\(x_i\)</span>和<span class=arithmatex>\(w_i\)</span>的乘积的和来得到一个标量：<span class=arithmatex>\(\mathbf{x}^\top \mathbf{w} = \sum_{i=1}^d x_i w_i.\)</span> 注意，我们可以将向量<span class=arithmatex>\(\mathbf{x}\)</span>视为是一个行向量，将<span class=arithmatex>\(\mathbf{w}\)</span>视为是一个列向量，其点积为一个矩阵：<span class=arithmatex>\(\mathbf{x}^\top \mathbf{w} = \mathbf{x}^\top \mathbf{w}.\)</span></p> <h3 id=238->2.3.8 矩阵-向量积<a class=headerlink href=#238- title="Permanent link">&para;</a></h3> <p>现在我们知道如何计算点积，我们可以开始理解矩阵-向量积(matrix-vector product)。回顾分别在标量和向量中定义的矩阵<span class=arithmatex>\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>和向量<span class=arithmatex>\(\mathbf{x} \in \mathbb{R}^n\)</span>。让我们将矩阵<span class=arithmatex>\(\mathbf{A}\)</span>用它的行向量表示：</p> <div class=arithmatex>\[ \mathbf{A} = \begin{bmatrix} \mathbf{a}^\top_1 \\ \mathbf{a}^\top_2 \\ \vdots \\ \mathbf{a}^\top_m \end{bmatrix}, \]</div> <p>其中每个<span class=arithmatex>\(\mathbf{a}^\top_i \in \mathbb{R}^n\)</span>都是一个行向量，表示<span class=arithmatex>\(\mathbf{A}\)</span>的第<span class=arithmatex>\(i\)</span>行。矩阵向量积<span class=arithmatex>\(\mathbf{A}\mathbf{x}\)</span>是一个长度为<span class=arithmatex>\(m\)</span>的列向量，其第<span class=arithmatex>\(i\)</span>个元素是点积<span class=arithmatex>\(\mathbf{a}^\top_i \mathbf{x}\)</span>：</p> <div class=arithmatex>\[ \mathbf{A}\mathbf{x} = \begin{bmatrix} \mathbf{a}^\top_1 \\ \mathbf{a}^\top_2 \\ \vdots \\ \mathbf{a}^\top_m \end{bmatrix} \mathbf{x} = \begin{bmatrix} \mathbf{a}^\top_1 \mathbf{x} \\ \mathbf{a}^\top_2 \mathbf{x} \\ \vdots \\ \mathbf{a}^\top_m \mathbf{x} \end{bmatrix}. \]</div> <p>我们可以吧一个矩阵<span class=arithmatex>\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>看作一个从<span class=arithmatex>\(\mathbb{R}^n\)</span>到<span class=arithmatex>\(\mathbb{R}^m\)</span>的转换。这些转换是非常有用的，例如可以用方阵的乘法来表示旋转。后续章节将讲到，我们也可以使用矩阵-向量积来描述在给定前一层的值时， 求解神经网络每一层所需的复杂计算。</p> <div class="tabbed-set tabbed-alternate" data-tabs=47:2><input checked=checked id=238--pytorch name=__tabbed_47 type=radio><input id=238--tensorflow name=__tabbed_47 type=radio><div class=tabbed-labels><label for=238--pytorch>PYTORCH</label><label for=238--tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-183-1><a id=__codelineno-183-1 name=__codelineno-183-1 href=#__codelineno-183-1></a><span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>mv</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-184-1><a id=__codelineno-184-1 name=__codelineno-184-1 href=#__codelineno-184-1></a>(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-185-1><a id=__codelineno-185-1 name=__codelineno-185-1 href=#__codelineno-185-1></a><span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>matvec</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-186-1><a id=__codelineno-186-1 name=__codelineno-186-1 href=#__codelineno-186-1></a>(&lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=
</span><span id=__span-186-2><a id=__codelineno-186-2 name=__codelineno-186-2 href=#__codelineno-186-2></a>array([[ 0.,  1.,  2.,  3.],
</span><span id=__span-186-3><a id=__codelineno-186-3 name=__codelineno-186-3 href=#__codelineno-186-3></a>       [ 4.,  5.,  6.,  7.],
</span><span id=__span-186-4><a id=__codelineno-186-4 name=__codelineno-186-4 href=#__codelineno-186-4></a>       [ 8.,  9., 10., 11.],
</span><span id=__span-186-5><a id=__codelineno-186-5 name=__codelineno-186-5 href=#__codelineno-186-5></a>       [12., 13., 14., 15.],
</span><span id=__span-186-6><a id=__codelineno-186-6 name=__codelineno-186-6 href=#__codelineno-186-6></a>       [16., 17., 18., 19.]], dtype=float32)&gt;,
</span><span id=__span-186-7><a id=__codelineno-186-7 name=__codelineno-186-7 href=#__codelineno-186-7></a> TensorShape([4]),
</span><span id=__span-186-8><a id=__codelineno-186-8 name=__codelineno-186-8 href=#__codelineno-186-8></a> &lt;tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 14.,  38.,  62.,  86., 110.], dtype=float32)&gt;)
</span></code></pre></div> </div> </div> </div> <h3 id=239->2.3.9 矩阵-矩阵乘法<a class=headerlink href=#239- title="Permanent link">&para;</a></h3> <p>在掌握点积和矩阵-向量积的知识后， 那么**矩阵-矩阵乘法**（matrix-matrix multiplication）应该很简单。</p> <p>假设我们有两个矩阵<span class=arithmatex>\(\mathbf{A} \in \mathbb{R}^{n \times k}\)</span>和<span class=arithmatex>\(\mathbf{B} \in \mathbb{R}^{k \times m}\)</span>：</p> <div class=arithmatex>\[ \mathbf{A} = \begin{bmatrix} \mathbf{a_{11}} &amp; \mathbf{a_{12}} &amp; \cdots &amp; \mathbf{a_{1k}} \\ \mathbf{a_{21}} &amp; \mathbf{a_{22}} &amp; \cdots &amp; \mathbf{a_{2k}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{a_{n1}} &amp; \mathbf{a_{n2}} &amp; \cdots &amp; \mathbf{a_{nk}} \end{bmatrix}, \quad \mathbf{B} = \begin{bmatrix} \mathbf{b_{11}} &amp; \mathbf{b_{12}} &amp; \cdots &amp; \mathbf{b_{1m}} \\ \mathbf{b_{21}} &amp; \mathbf{b_{22}} &amp; \cdots &amp; \mathbf{b_{2m}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{b_{k1}} &amp; \mathbf{b_{k2}} &amp; \cdots &amp; \mathbf{b_{km}} \end{bmatrix}. \]</div> <p>用行向量<span class=arithmatex>\(\mathbf{a}^\top_{i} \in \mathbb{R}^k\)</span>表示矩阵<span class=arithmatex>\(\mathbf{A}\)</span>的第<span class=arithmatex>\(i\)</span>行，用列向量<span class=arithmatex>\(\mathbf{b}_{j} \in \mathbb{R}^k\)</span>表示矩阵<span class=arithmatex>\(\mathbf{B}\)</span>的第<span class=arithmatex>\(j\)</span>列。要生成矩阵积<span class=arithmatex>\(\mathbf{C} = \mathbf{A}\mathbf{B}\)</span>，最简单的方法是考虑<span class=arithmatex>\(\mathbf{A}\)</span>的行向量和<span class=arithmatex>\(\mathbf{B}\)</span>的列向量：</p> <div class=arithmatex>\[ \mathbf{A} = \begin{bmatrix} \mathbf{a}^\top_{1} \\ \mathbf{a}^\top_{2} \\ \vdots \\ \mathbf{a}^\top_{n} \end{bmatrix}, \quad \mathbf{B} = \begin{bmatrix} \mathbf{b}_{1} &amp; \mathbf{b}_{2} &amp; \cdots &amp; \mathbf{b}_{m} \end{bmatrix}. \]</div> <p>当我们简单地将每个元素<span class=arithmatex>\(c_{ij}\)</span>计算为点积<span class=arithmatex>\(\mathbf{a}^\top_i \mathbf{b}_j\)</span>时，我们得到了矩阵<span class=arithmatex>\(\mathbf{C} \in \mathbb{R}^{n \times m}\)</span>：</p> <div class=arithmatex>\[ \mathbf{C} = \begin{bmatrix} \mathbf{a}^\top_{1} \\ \mathbf{a}^\top_{2} \\ \vdots \\ \mathbf{a}^\top_{n} \end{bmatrix} \begin{bmatrix} \mathbf{b}_{1} &amp; \mathbf{b}_{2} &amp; \cdots &amp; \mathbf{b}_{m} \end{bmatrix} = \begin{bmatrix} \mathbf{a}^\top_{1} \mathbf{b}_{1} &amp; \mathbf{a}^\top_{1} \mathbf{b}_{2} &amp; \cdots &amp; \mathbf{a}^\top_{1} \mathbf{b}_{m} \\ \mathbf{a}^\top_{2} \mathbf{b}_{1} &amp; \mathbf{a}^\top_{2} \mathbf{b}_{2} &amp; \cdots &amp; \mathbf{a}^\top_{2} \mathbf{b}_{m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{a}^\top_{n} \mathbf{b}_{1} &amp; \mathbf{a}^\top_{n} \mathbf{b}_{2} &amp; \cdots &amp; \mathbf{a}^\top_{n} \mathbf{b}_{m} \end{bmatrix}. \]</div> <p>我们可以将矩阵-矩阵乘法<span class=arithmatex>\(\\mathbf{A}\mathbf{B}\)</span>看作简单地执行m次矩阵-向量积并将结果拼接在一起，得到一个<span class=arithmatex>\(n \times m\)</span>矩阵。</p> <div class="tabbed-set tabbed-alternate" data-tabs=48:2><input checked=checked id=239--pytorch name=__tabbed_48 type=radio><input id=239--tensorflow name=__tabbed_48 type=radio><div class=tabbed-labels><label for=239--pytorch>PYTORCH</label><label for=239--tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-187-1><a id=__codelineno-187-1 name=__codelineno-187-1 href=#__codelineno-187-1></a><span class=n>B</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span><span id=__span-187-2><a id=__codelineno-187-2 name=__codelineno-187-2 href=#__codelineno-187-2></a><span class=n>torch</span><span class=o>.</span><span class=n>mm</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-188-1><a id=__codelineno-188-1 name=__codelineno-188-1 href=#__codelineno-188-1></a>tensor([[ 6.,  6.,  6.],
</span><span id=__span-188-2><a id=__codelineno-188-2 name=__codelineno-188-2 href=#__codelineno-188-2></a>        [22., 22., 22.],
</span><span id=__span-188-3><a id=__codelineno-188-3 name=__codelineno-188-3 href=#__codelineno-188-3></a>        [38., 38., 38.],
</span><span id=__span-188-4><a id=__codelineno-188-4 name=__codelineno-188-4 href=#__codelineno-188-4></a>        [54., 54., 54.],
</span><span id=__span-188-5><a id=__codelineno-188-5 name=__codelineno-188-5 href=#__codelineno-188-5></a>        [70., 70., 70.]])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-189-1><a id=__codelineno-189-1 name=__codelineno-189-1 href=#__codelineno-189-1></a><span class=n>B</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-189-2><a id=__codelineno-189-2 name=__codelineno-189-2 href=#__codelineno-189-2></a><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-190-1><a id=__codelineno-190-1 name=__codelineno-190-1 href=#__codelineno-190-1></a>&lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=
</span><span id=__span-190-2><a id=__codelineno-190-2 name=__codelineno-190-2 href=#__codelineno-190-2></a>array([[ 6.,  6.,  6.],
</span><span id=__span-190-3><a id=__codelineno-190-3 name=__codelineno-190-3 href=#__codelineno-190-3></a>       [22., 22., 22.],
</span><span id=__span-190-4><a id=__codelineno-190-4 name=__codelineno-190-4 href=#__codelineno-190-4></a>       [38., 38., 38.],
</span><span id=__span-190-5><a id=__codelineno-190-5 name=__codelineno-190-5 href=#__codelineno-190-5></a>       [54., 54., 54.],
</span><span id=__span-190-6><a id=__codelineno-190-6 name=__codelineno-190-6 href=#__codelineno-190-6></a>       [70., 70., 70.]], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=2310>2.3.10 范数<a class=headerlink href=#2310 title="Permanent link">&para;</a></h3> <p>线性代数中最有用的一些运算符是范数（norm）。 非正式地说，向量的范数是表示一个向量有多大。 这里考虑的大小（size）概念不涉及维度，而是分量的大小。</p> <p>在线性代数中，向量范数是将向量映射到标量的函数<span class=arithmatex>\(f\)</span>。 给定任意向量<span class=arithmatex>\(\mathbf{x}\)</span>，向量范数要满足一些属性。第一个性质是：如果我们按常数因子<span class=arithmatex>\(\alpha\)</span>缩放<span class=arithmatex>\(\mathbf{x}\)</span>的所有元素，其范数也会按相同常数因子的绝对值缩放：</p> <div class=arithmatex>\[\| \alpha \mathbf{x} \| = |\alpha| \| \mathbf{x} \|. \]</div> <p>第二个性质是三角不等式（triangle inequality）。 它表明对于任意两个向量<span class=arithmatex>\(\mathbf{x}\)</span>和<span class=arithmatex>\(\mathbf{y}\)</span>，向量之和的范数不大于向量范数的和：</p> <div class=arithmatex>\[ \| \mathbf{x} + \mathbf{y} \| \leq \| \mathbf{x} \| + \| \mathbf{y} \| . \]</div> <p>第三个性质简单地说范数必须非负：</p> <div class=arithmatex>\[ \| \mathbf{x} \| \geq 0. \]</div> <p>这是有道理的，因为在大多数情况下，任何东西的最小大小为零。最后一个性质规定了什么样的向量具有零范数：</p> <div class=arithmatex>\[ \| \mathbf{x} \| = 0 \Leftrightarrow \mathbf{x} = \mathbf{0}. \]</div> <p>范数听起来很像距离的度量。 欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。 事实上，欧几里得距离是一个<span class=arithmatex>\(L_2\)</span>范数: 假设<span class=arithmatex>\(n\)</span>维向量<span class=arithmatex>\(\mathbf{x}\)</span>的坐标为<span class=arithmatex>\(x_1, \ldots, x_n\)</span>, 其<span class=arithmatex>\(L_2\)</span>范数是向量元素平方和的平方根：</p> <div class=arithmatex>\[ \| \mathbf{x} \|_2 = \sqrt{\sum_{i=1}^n x_i^2}, \]</div> <p>其中，在<span class=arithmatex>\(L_2\)</span>范数中常常省略下标2，也就是说<span class=arithmatex>\(\|\mathbf{x}\|\)</span>等价于<span class=arithmatex>\(\|\mathbf{x}\|_2\)</span>。 在代码中，我们可以按如下方式计算向量的<span class=arithmatex>\(L_2\)</span>范数。</p> <div class="tabbed-set tabbed-alternate" data-tabs=49:2><input checked=checked id=2310-pytorch name=__tabbed_49 type=radio><input id=2310-tensorflow name=__tabbed_49 type=radio><div class=tabbed-labels><label for=2310-pytorch>PYTORCH</label><label for=2310-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-191-1><a id=__codelineno-191-1 name=__codelineno-191-1 href=#__codelineno-191-1></a><span class=n>u</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>3.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>4.0</span><span class=p>])</span>
</span><span id=__span-191-2><a id=__codelineno-191-2 name=__codelineno-191-2 href=#__codelineno-191-2></a><span class=n>torch</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>u</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-192-1><a id=__codelineno-192-1 name=__codelineno-192-1 href=#__codelineno-192-1></a>tensor(5.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-193-1><a id=__codelineno-193-1 name=__codelineno-193-1 href=#__codelineno-193-1></a><span class=n>u</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>3.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>4.0</span><span class=p>])</span>
</span><span id=__span-193-2><a id=__codelineno-193-2 name=__codelineno-193-2 href=#__codelineno-193-2></a><span class=n>tf</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>u</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-194-1><a id=__codelineno-194-1 name=__codelineno-194-1 href=#__codelineno-194-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;
</span></code></pre></div> </div> </div> </div> <p>在机器学习中，我们更经常地使用<span class=arithmatex>\(L_2\)</span>范数的平方。 你还会经常遇到<span class=arithmatex>\(L_1\)</span>范数，它表示为向量元素的绝对值之和：</p> <div class=arithmatex>\[ \| \mathbf{x} \|_1 = \sum_{i=1}^n \left|x_i \right|. \]</div> <p>与<span class=arithmatex>\(L_2\)</span>范数相比，<span class=arithmatex>\(L_1\)</span>范数受异常值的影响较小。为了计算<span class=arithmatex>\(L_1\)</span>范数，我们将绝对值函数和按元素求和组合起来。</p> <div class="tabbed-set tabbed-alternate" data-tabs=50:2><input checked=checked id=2310-pytorch_1 name=__tabbed_50 type=radio><input id=2310-tensorflow_1 name=__tabbed_50 type=radio><div class=tabbed-labels><label for=2310-pytorch_1>PYTORCH</label><label for=2310-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-195-1><a id=__codelineno-195-1 name=__codelineno-195-1 href=#__codelineno-195-1></a><span class=n>torch</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>u</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-196-1><a id=__codelineno-196-1 name=__codelineno-196-1 href=#__codelineno-196-1></a>tensor(7.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-197-1><a id=__codelineno-197-1 name=__codelineno-197-1 href=#__codelineno-197-1></a><span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>u</span><span class=p>))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-198-1><a id=__codelineno-198-1 name=__codelineno-198-1 href=#__codelineno-198-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=7.0&gt;
</span></code></pre></div> </div> </div> </div> <p><span class=arithmatex>\(L_2\)</span>范数和<span class=arithmatex>\(L_1\)</span>范数都是更一般形式<span class=arithmatex>\(L_p\)</span>范数的特例：</p> <div class=arithmatex>\[ \| \mathbf{x} \|_p = \left( \sum_{i=1}^n \left|x_i \right|^p \right)^{\frac{1}{p}}. \]</div> <p>类似于向量的<span class=arithmatex>\(L_2\)</span>范数，矩阵<span class=arithmatex>\(\mathbf{X} \in \mathbb{R}^{m \times n}\)</span>的**弗罗贝尼乌斯范数**（Frobenius norm）是矩阵元素平方和的平方根：</p> <div class=arithmatex>\[ \|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}. \]</div> <p>弗罗贝尼乌斯范数满足向量范数的所有性质。 它就像是将矩阵展平成向量后计算其<span class=arithmatex>\(L_2\)</span>范数一样。</p> <div class="tabbed-set tabbed-alternate" data-tabs=51:2><input checked=checked id=2310-pytorch_2 name=__tabbed_51 type=radio><input id=2310-tensorflow_2 name=__tabbed_51 type=radio><div class=tabbed-labels><label for=2310-pytorch_2>PYTORCH</label><label for=2310-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-199-1><a id=__codelineno-199-1 name=__codelineno-199-1 href=#__codelineno-199-1></a><span class=n>torch</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>4</span><span class=p>,</span> <span class=mi>9</span><span class=p>)))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-200-1><a id=__codelineno-200-1 name=__codelineno-200-1 href=#__codelineno-200-1></a>tensor(6.)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-201-1><a id=__codelineno-201-1 name=__codelineno-201-1 href=#__codelineno-201-1></a><span class=n>tf</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>4</span><span class=p>,</span> <span class=mi>9</span><span class=p>)))</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-202-1><a id=__codelineno-202-1 name=__codelineno-202-1 href=#__codelineno-202-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;
</span></code></pre></div> </div> </div> </div> <h4 id=23101>2.3.10.1 范数和目标<a class=headerlink href=#23101 title="Permanent link">&para;</a></h4> <p>在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</p> <h3 id=2311>2.3.11 关于线性代数的更多信息¶<a class=headerlink href=#2311 title="Permanent link">&para;</a></h3> <p>仅用一节，我们就教会了阅读本书所需的、用以理解现代深度学习的线性代数。 线性代数还有很多，其中很多数学对于机器学习非常有用。 例如，矩阵可以分解为因子，这些分解可以显示真实世界数据集中的低维结构。 机器学习的整个子领域都侧重于使用矩阵分解及其向高阶张量的泛化，来发现数据集中的结构并解决预测问题。 当开始动手尝试并在真实数据集上应用了有效的机器学习模型，你会更倾向于学习更多数学。 因此，这一节到此结束，本书将在后面介绍更多数学知识。</p> <h3 id=2312>2.3.12 小结<a class=headerlink href=#2312 title="Permanent link">&para;</a></h3> <ul> <li> <p>标量、向量、矩阵和张量是线性代数中的基本数学对象。</p> </li> <li> <p>向量泛化自标量，矩阵泛化自向量。</p> </li> <li> <p>标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。</p> </li> <li> <p>一个张量可以通过sum和mean沿指定的轴降低维度。</p> </li> <li> <p>两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。</p> </li> <li> <p>在深度学习中，我们经常使用范数，如<span class=arithmatex>\(L_1\)</span>范数、<span class=arithmatex>\(L_2\)</span>范数、范数和Frobenius范数。</p> </li> <li> <p>我们可以对标量、向量、矩阵和张量执行各种操作。</p> </li> </ul> <h2 id=24>2.4 微积分<a class=headerlink href=#24 title="Permanent link">&para;</a></h2> <p>在2500年前，古希腊人把一个多边形分成三角形，并把它们的面积相加，才找到计算多边形面积的方法。 为了求出曲线形状（比如圆）的面积，古希腊人在这样的形状上刻内接多边形。 内接多边形的等长边越多，就越接近圆。 这个过程也被称为逼近法（method of exhaustion）。事实上，逼近法就是积分（integral calculus）的起源。 2000多年后，微积分的另一支，微分（differential calculus）被发明出来。 在微分学最重要的应用是优化问题，即考虑如何把事情做到最好。 正如在 2.3.10.1节中讨论的那样， 这种问题在深度学习中是无处不在的。</p> <p>在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。 通常情况下，变得更好意味着最小化一个损失函数（loss function）， 即一个衡量“模型有多糟糕”这个问题的分数。 最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。 因此，我们可以将拟合模型的任务分解为两个关键问题：</p> <ul> <li>优化（optimization）：用模型拟合观测数据的过程；</li> <li>泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。</li> </ul> <h3 id=241>2.4.1 导数<a class=headerlink href=#241 title="Permanent link">&para;</a></h3> <p>我们首先讨论导数的计算，这是几乎所有深度学习优化算法的关键步骤。 在深度学习中，我们通常选择对于模型参数可微的损失函数。 简而言之，对于每个参数， 如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少。</p> <p>假设我们有一个函数<span class=arithmatex>\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span>，它的输入和输出都是标量。如果函数<span class=arithmatex>\(f\)</span>的导数存在，这个极限被定义为：</p> <div class=arithmatex>\[ f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.\]</div> <p>如果导数<span class=arithmatex>\(f'(a)\)</span>存在，则函数<span class=arithmatex>\(f\)</span>在点<span class=arithmatex>\(a\)</span>是可微的。如果<span class=arithmatex>\(f\)</span>在区间<span class=arithmatex>\([a, b]\)</span>上的每个点都可微，那么<span class=arithmatex>\(f\)</span>在区间<span class=arithmatex>\([a, b]\)</span>上是可微的。如果<span class=arithmatex>\(f\)</span>在区间<span class=arithmatex>\((-\infty, \infty)\)</span>上是可微的，则<span class=arithmatex>\(f\)</span>是可微的。函数<span class=arithmatex>\(f\)</span>的导数<span class=arithmatex>\(f'\)</span>也是一个函数：它将<span class=arithmatex>\(x\)</span>映射到导数<span class=arithmatex>\(f'(x)\)</span>。同样，如果<span class=arithmatex>\(f'\)</span>在某个区间上是可微的，我们可以计算它的导数，它被称为<span class=arithmatex>\(f\)</span>的二阶导数<span class=arithmatex>\(f''\)</span>，以此类推。</p> <p>我们可以将导数<span class=arithmatex>\(f'(x)\)</span>解释为：当<span class=arithmatex>\(x\)</span>在<span class=arithmatex>\(f(x)\)</span>处增加<span class=arithmatex>\(h\)</span>时，<span class=arithmatex>\(f(x)\)</span>相对于<span class=arithmatex>\(h\)</span>的变化率。因此，当<span class=arithmatex>\(h\)</span>接近0时，<span class=arithmatex>\(f(x)\)</span>相对于<span class=arithmatex>\(h\)</span>的变化率是<span class=arithmatex>\(f'(x)\)</span>。我们也可以将导数解释为：<span class=arithmatex>\(f(x)\)</span>的瞬时变化率。例如，如果<span class=arithmatex>\(f(x)\)</span>表示一个人的位置随时间的变化，那么<span class=arithmatex>\(f'(x)\)</span>就是这个人的瞬时速度。如果<span class=arithmatex>\(f(x)\)</span>表示一个人的速度随时间的变化，那么<span class=arithmatex>\(f'(x)\)</span>就是这个人的瞬时加速度。</p> <h3 id=242>2.4.2 偏导数<a class=headerlink href=#242 title="Permanent link">&para;</a></h3> <p>到目前为止，我们只讨论了仅含一个变量的函数的微分。 在深度学习中，函数通常依赖于许多变量。 因此，我们需要将微分的思想推广到多元函数（multivariate function）上。</p> <p>设<span class=arithmatex>\(y = f(x_1, x_2, \ldots, x_n)\)</span>是一个具有<span class=arithmatex>\(n\)</span>个变量的函数。 <span class=arithmatex>\(f\)</span>的偏导数<span class=arithmatex>\(\frac{\partial y}{\partial x_i}\)</span>（partial derivative）衡量了<span class=arithmatex>\(f\)</span>相对于变量<span class=arithmatex>\(x_i\)</span>的变化率，而将其他变量<span class=arithmatex>\(x_j (j \neq i, j = 1, 2, \ldots, n)\)</span>视为常数。 要计算<span class=arithmatex>\(\frac{\partial y}{\partial x_i}\)</span>，我们可以将<span class=arithmatex>\(f(x_1, x_2, \ldots, x_n)\)</span>视为一个关于一个变量<span class=arithmatex>\(x_i\)</span>的函数，而将其他变量<span class=arithmatex>\(x_j (j \neq i, j = 1, 2, \ldots, n)\)</span>视为常数。 然后，我们计算这个函数的导数，就像我们在单变量函数的情况下所做的那样，而将其他变量视为常数。</p> <h3 id=243>2.4.3 梯度<a class=headerlink href=#243 title="Permanent link">&para;</a></h3> <p>我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的梯度（gradient）向量。 具体而言，设函数<span class=arithmatex>\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>的输入是一个<span class=arithmatex>\(n\)</span>维向量<span class=arithmatex>\(\mathbf{x} = [x_1, x_2, \ldots, x_n]^\top\)</span>，它的输出是标量。 <span class=arithmatex>\(f(\mathbf{x})\)</span>相对于<span class=arithmatex>\(\mathbf{x}\)</span>的梯度是一个包含<span class=arithmatex>\(n\)</span>个偏导数的向量：</p> <div class=arithmatex>\[ \nabla_{\mathbf{x}} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1} \\ \frac{\partial f(\mathbf{x})}{\partial x_2} \\ \vdots \\ \frac{\partial f(\mathbf{x})}{\partial x_n} \end{bmatrix}. \]</div> <p>其中 <span class=arithmatex>\(\nabla_{\mathbf{x}} f(\mathbf{x})\)</span> 读作“<span class=arithmatex>\(f(\mathbf{x})\)</span>关于<span class=arithmatex>\(\mathbf{x}\)</span>的梯度”。 在本书中，我们使用梯度和导数这两个术语。 梯度是偏导数的向量，而导数是偏导数的标量。</p> <p>假设<span class=arithmatex>\(x\)</span>为<span class=arithmatex>\(n\)</span>维向量，在微分多元函数时，经常使用以下规则：</p> <ol> <li> <p>对于所有的<span class=arithmatex>\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>，都有<span class=arithmatex>\(\nabla_{\mathbf{x}} \mathbf{A}\mathbf{x} = \mathbf{A}^\top\)</span>。</p> </li> <li> <p>对于所有的<span class=arithmatex>\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>，都有<span class=arithmatex>\(\nabla_{\mathbf{x}} \mathbf{x}^\top\mathbf{A} = \mathbf{A}\)</span>。</p> </li> <li> <p>对于所有的<span class=arithmatex>\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>，都有<span class=arithmatex>\(\nabla_{\mathbf{x}} \mathbf{x}^\top\mathbf{A}\mathbf{x} = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}\)</span>。</p> </li> <li> <p>对于所有的可微的单变量函数<span class=arithmatex>\(f\)</span>，都有<span class=arithmatex>\(\nabla_{\mathbf{x}} f(\mathbf{x}) = \left[\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}\right]^\top\)</span>。</p> </li> </ol> <h3 id=244>2.4.4 链式法则<a class=headerlink href=#244 title="Permanent link">&para;</a></h3> <p>然而，上面方法可能很难找到梯度。 这是因为在深度学习中，多元函数通常是复合（composite）的， 所以难以应用上述任何规则来微分这些函数。 幸运的是，链式法则可以被用来微分复合函数。</p> <p>假设<span class=arithmatex>\(y=f(u)\)</span>和<span class=arithmatex>\(u=g(x)\)</span>是两个函数，其中<span class=arithmatex>\(u\)</span>是<span class=arithmatex>\(x\)</span>的函数，<span class=arithmatex>\(y\)</span>是<span class=arithmatex>\(u\)</span>的函数。 为了找到复合函数<span class=arithmatex>\(y=f(g(x))\)</span>关于<span class=arithmatex>\(x\)</span>的导数，链式法则给出了以下关系：</p> <div class=arithmatex>\[ \frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}. \]</div> <p>现在考虑一个更一般的场景，即函数具有任意数量的变量的情况。 假设可微分函数<span class=arithmatex>\(y\)</span>是<span class=arithmatex>\(x_1, x_2, \ldots, x_n\)</span>的函数，其中<span class=arithmatex>\(x_i\)</span>是<span class=arithmatex>\(t_1, t_2, \ldots, t_m\)</span>的函数。 为了计算导数<span class=arithmatex>\(\frac{dy}{dt_i}\)</span>，我们可以使用链式法则：</p> <div class=arithmatex>\[ \frac{dy}{dt_i} = \frac{dy}{dx_1} \frac{dx_1}{dt_i} + \frac{dy}{dx_2} \frac{dx_2}{dt_i} + \cdots + \frac{dy}{dx_n} \frac{dx_n}{dt_i}. \]</div> <h3 id=245>2.4.5 小结<a class=headerlink href=#245 title="Permanent link">&para;</a></h3> <ul> <li> <p>微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。</p> </li> <li> <p>导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。</p> </li> <li> <p>梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。</p> </li> <li> <p>链式法则可以用来微分复合函数。</p> </li> </ul> <h2 id=25>2.5 自动微分<a class=headerlink href=#25 title="Permanent link">&para;</a></h2> <p>求导是几乎所有深度学习优化算法的关键步骤。 虽然求导的计算很简单，只需要一些基本的微积分。 但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。</p> <p>深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个**计算图**（computational graph）， 来跟踪计算是**哪些数据通过哪些操作组合起来产生输出**。 自动微分使系统能够随后反向传播梯度。 这里，<strong>反向传播</strong>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</p> <h3 id=251>2.5.1 一个简单的例子<a class=headerlink href=#251 title="Permanent link">&para;</a></h3> <p>作为一个演示例子，假设我们想对函数<span class=arithmatex>\(y = 2\mathbf{x}^\top\mathbf{x}\)</span>关于列向量<span class=arithmatex>\(\mathbf{x}\)</span>求导，其中<span class=arithmatex>\(\mathbf{x} \in \mathbb{R}^d\)</span>。 首先，我们创建变量<code>x</code>并为其分配一个初始值。</p> <div class="tabbed-set tabbed-alternate" data-tabs=52:2><input checked=checked id=251-pytorch name=__tabbed_52 type=radio><input id=251-tensorflow name=__tabbed_52 type=radio><div class=tabbed-labels><label for=251-pytorch>PYTORCH</label><label for=251-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-203-1><a id=__codelineno-203-1 name=__codelineno-203-1 href=#__codelineno-203-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span><span id=__span-203-2><a id=__codelineno-203-2 name=__codelineno-203-2 href=#__codelineno-203-2></a>
</span><span id=__span-203-3><a id=__codelineno-203-3 name=__codelineno-203-3 href=#__codelineno-203-3></a><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mf>4.0</span><span class=p>)</span>
</span><span id=__span-203-4><a id=__codelineno-203-4 name=__codelineno-203-4 href=#__codelineno-203-4></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-204-1><a id=__codelineno-204-1 name=__codelineno-204-1 href=#__codelineno-204-1></a>tensor([0., 1., 2., 3.])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-205-1><a id=__codelineno-205-1 name=__codelineno-205-1 href=#__codelineno-205-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span><span id=__span-205-2><a id=__codelineno-205-2 name=__codelineno-205-2 href=#__codelineno-205-2></a>
</span><span id=__span-205-3><a id=__codelineno-205-3 name=__codelineno-205-3 href=#__codelineno-205-3></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span><span id=__span-205-4><a id=__codelineno-205-4 name=__codelineno-205-4 href=#__codelineno-205-4></a><span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-206-1><a id=__codelineno-206-1 name=__codelineno-206-1 href=#__codelineno-206-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>在我们计算<code>y</code>关于<code>x</code>的梯度之前，我们需要一个地方来存储梯度。重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 注意，一个标量函数关于向量<span class=arithmatex>\(x\)</span>的梯度是向量，并且与<span class=arithmatex>\(x\)</span>具有相同的形状。</p> <div class="tabbed-set tabbed-alternate" data-tabs=53:2><input checked=checked id=251-pytorch_1 name=__tabbed_53 type=radio><input id=251-tensorflow_1 name=__tabbed_53 type=radio><div class=tabbed-labels><label for=251-pytorch_1>PYTORCH</label><label for=251-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-207-1><a id=__codelineno-207-1 name=__codelineno-207-1 href=#__codelineno-207-1></a><span class=n>x</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-208-1><a id=__codelineno-208-1 name=__codelineno-208-1 href=#__codelineno-208-1></a>tensor([0., 1., 2., 3.], requires_grad=True)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-209-1><a id=__codelineno-209-1 name=__codelineno-209-1 href=#__codelineno-209-1></a><span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></code></pre></div> <p>```text</p> </div> </div> </div> <p>现在计算<span class=arithmatex>\(y\)</span></p> <div class="tabbed-set tabbed-alternate" data-tabs=54:2><input checked=checked id=251-pytorch_2 name=__tabbed_54 type=radio><input id=251-tensorflow_2 name=__tabbed_54 type=radio><div class=tabbed-labels><label for=251-pytorch_2>PYTORCH</label><label for=251-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-210-1><a id=__codelineno-210-1 name=__codelineno-210-1 href=#__codelineno-210-1></a><span class=n>y</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span><span id=__span-210-2><a id=__codelineno-210-2 name=__codelineno-210-2 href=#__codelineno-210-2></a><span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-211-1><a id=__codelineno-211-1 name=__codelineno-211-1 href=#__codelineno-211-1></a>tensor(28., grad_fn=&lt;MulBackward0&gt;)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-212-1><a id=__codelineno-212-1 name=__codelineno-212-1 href=#__codelineno-212-1></a><span class=n>y</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>tf</span><span class=o>.</span><span class=n>tensordot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>axes</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-212-2><a id=__codelineno-212-2 name=__codelineno-212-2 href=#__codelineno-212-2></a><span class=n>y</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-213-1><a id=__codelineno-213-1 name=__codelineno-213-1 href=#__codelineno-213-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=28.0&gt;
</span></code></pre></div> </div> </div> </div> <p>x是一个长度为4的向量，计算<code>x</code>和<code>x</code>的内积，得到了我们赋值给<code>y</code>的标量输出。接下来，我们可以通过调用反向传播函数来自动计算<code>y</code>关于<code>x</code>每个分量的梯度，然后打印这些梯度。</p> <div class="tabbed-set tabbed-alternate" data-tabs=55:2><input checked=checked id=251-pytorch_3 name=__tabbed_55 type=radio><input id=251-tensorflow_3 name=__tabbed_55 type=radio><div class=tabbed-labels><label for=251-pytorch_3>PYTORCH</label><label for=251-tensorflow_3>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-214-1><a id=__codelineno-214-1 name=__codelineno-214-1 href=#__codelineno-214-1></a><span class=n>y</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-214-2><a id=__codelineno-214-2 name=__codelineno-214-2 href=#__codelineno-214-2></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-215-1><a id=__codelineno-215-1 name=__codelineno-215-1 href=#__codelineno-215-1></a>tensor([ 0.,  4.,  8., 12.])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-216-1><a id=__codelineno-216-1 name=__codelineno-216-1 href=#__codelineno-216-1></a><span class=n>x_grad</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span><span id=__span-216-2><a id=__codelineno-216-2 name=__codelineno-216-2 href=#__codelineno-216-2></a><span class=n>x_grad</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-217-1><a id=__codelineno-217-1 name=__codelineno-217-1 href=#__codelineno-217-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <p>函数<span class=arithmatex>\(y = 2\mathbf{x}^\top\mathbf{x}\)</span>关于<span class=arithmatex>\(\mathbf{x}\)</span>的梯度应为<span class=arithmatex>\(4\mathbf{x}\)</span>。 让我们快速验证我们想到的结果是否正确。</p> <div class="tabbed-set tabbed-alternate" data-tabs=56:2><input checked=checked id=251-pytorch_4 name=__tabbed_56 type=radio><input id=251-tensorflow_4 name=__tabbed_56 type=radio><div class=tabbed-labels><label for=251-pytorch_4>PYTORCH</label><label for=251-tensorflow_4>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-218-1><a id=__codelineno-218-1 name=__codelineno-218-1 href=#__codelineno-218-1></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span> <span class=o>==</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-219-1><a id=__codelineno-219-1 name=__codelineno-219-1 href=#__codelineno-219-1></a>tensor([True, True, True, True])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-220-1><a id=__codelineno-220-1 name=__codelineno-220-1 href=#__codelineno-220-1></a><span class=n>x_grad</span> <span class=o>==</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>x</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-221-1><a id=__codelineno-221-1 name=__codelineno-221-1 href=#__codelineno-221-1></a>&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])&gt;
</span></code></pre></div> </div> </div> </div> <p>现在计算<code>x</code>的另一个函数。</p> <div class="tabbed-set tabbed-alternate" data-tabs=57:2><input checked=checked id=251-pytorch_5 name=__tabbed_57 type=radio><input id=251-tensorflow_5 name=__tabbed_57 type=radio><div class=tabbed-labels><label for=251-pytorch_5>PYTORCH</label><label for=251-tensorflow_5>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-222-1><a id=__codelineno-222-1 name=__codelineno-222-1 href=#__codelineno-222-1></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span><span id=__span-222-2><a id=__codelineno-222-2 name=__codelineno-222-2 href=#__codelineno-222-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span><span id=__span-222-3><a id=__codelineno-222-3 name=__codelineno-222-3 href=#__codelineno-222-3></a><span class=n>y</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-222-4><a id=__codelineno-222-4 name=__codelineno-222-4 href=#__codelineno-222-4></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-223-1><a id=__codelineno-223-1 name=__codelineno-223-1 href=#__codelineno-223-1></a>tensor([1., 1., 1., 1.])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-224-1><a id=__codelineno-224-1 name=__codelineno-224-1 href=#__codelineno-224-1></a><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>t</span><span class=p>:</span>
</span><span id=__span-224-2><a id=__codelineno-224-2 name=__codelineno-224-2 href=#__codelineno-224-2></a>    <span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-224-3><a id=__codelineno-224-3 name=__codelineno-224-3 href=#__codelineno-224-3></a><span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>  <span class=c1># 被新计算的梯度覆盖</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-225-1><a id=__codelineno-225-1 name=__codelineno-225-1 href=#__codelineno-225-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=252>2.5.2 非标量变量的反向传播<a class=headerlink href=#252 title="Permanent link">&para;</a></h3> <p>当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。</p> <p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。</p> <div class="tabbed-set tabbed-alternate" data-tabs=58:2><input checked=checked id=252-pytorch name=__tabbed_58 type=radio><input id=252-tensorflow name=__tabbed_58 type=radio><div class=tabbed-labels><label for=252-pytorch>PYTORCH</label><label for=252-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-226-1><a id=__codelineno-226-1 name=__codelineno-226-1 href=#__codelineno-226-1></a><span class=c1># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span>
</span><span id=__span-226-2><a id=__codelineno-226-2 name=__codelineno-226-2 href=#__codelineno-226-2></a><span class=c1># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span>
</span><span id=__span-226-3><a id=__codelineno-226-3 name=__codelineno-226-3 href=#__codelineno-226-3></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span><span id=__span-226-4><a id=__codelineno-226-4 name=__codelineno-226-4 href=#__codelineno-226-4></a><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-226-5><a id=__codelineno-226-5 name=__codelineno-226-5 href=#__codelineno-226-5></a><span class=c1># 等价于y.backward(torch.ones(len(x)))</span>
</span><span id=__span-226-6><a id=__codelineno-226-6 name=__codelineno-226-6 href=#__codelineno-226-6></a><span class=n>y</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-226-7><a id=__codelineno-226-7 name=__codelineno-226-7 href=#__codelineno-226-7></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-227-1><a id=__codelineno-227-1 name=__codelineno-227-1 href=#__codelineno-227-1></a>tensor([0., 2., 4., 6.])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-228-1><a id=__codelineno-228-1 name=__codelineno-228-1 href=#__codelineno-228-1></a><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>t</span><span class=p>:</span>
</span><span id=__span-228-2><a id=__codelineno-228-2 name=__codelineno-228-2 href=#__codelineno-228-2></a>    <span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-228-3><a id=__codelineno-228-3 name=__codelineno-228-3 href=#__codelineno-228-3></a><span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span><span class=c1># 等价于y=tf.reduce_sum(x*x)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-229-1><a id=__codelineno-229-1 name=__codelineno-229-1 href=#__codelineno-229-1></a>&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 2., 4., 6.], dtype=float32)&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=253>2.5.3 分离计算<a class=headerlink href=#253 title="Permanent link">&para;</a></h3> <p>有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。</p> <p>这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。</p> <div class="tabbed-set tabbed-alternate" data-tabs=59:2><input checked=checked id=253-pytorch name=__tabbed_59 type=radio><input id=253-tensorflow name=__tabbed_59 type=radio><div class=tabbed-labels><label for=253-pytorch>PYTORCH</label><label for=253-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-230-1><a id=__codelineno-230-1 name=__codelineno-230-1 href=#__codelineno-230-1></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span><span id=__span-230-2><a id=__codelineno-230-2 name=__codelineno-230-2 href=#__codelineno-230-2></a><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-230-3><a id=__codelineno-230-3 name=__codelineno-230-3 href=#__codelineno-230-3></a><span class=n>u</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span><span id=__span-230-4><a id=__codelineno-230-4 name=__codelineno-230-4 href=#__codelineno-230-4></a><span class=n>z</span> <span class=o>=</span> <span class=n>u</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-230-5><a id=__codelineno-230-5 name=__codelineno-230-5 href=#__codelineno-230-5></a>
</span><span id=__span-230-6><a id=__codelineno-230-6 name=__codelineno-230-6 href=#__codelineno-230-6></a><span class=n>z</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-230-7><a id=__codelineno-230-7 name=__codelineno-230-7 href=#__codelineno-230-7></a><span class=n>x</span><span class=o>.</span><span class=n>grad</span> <span class=o>==</span> <span class=n>u</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-231-1><a id=__codelineno-231-1 name=__codelineno-231-1 href=#__codelineno-231-1></a>tensor([True, True, True, True])
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-232-1><a id=__codelineno-232-1 name=__codelineno-232-1 href=#__codelineno-232-1></a><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>t</span><span class=p>:</span>
</span><span id=__span-232-2><a id=__codelineno-232-2 name=__codelineno-232-2 href=#__codelineno-232-2></a>    <span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-232-3><a id=__codelineno-232-3 name=__codelineno-232-3 href=#__codelineno-232-3></a>    <span class=n>u</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>stop_gradient</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span><span id=__span-232-4><a id=__codelineno-232-4 name=__codelineno-232-4 href=#__codelineno-232-4></a>    <span class=n>z</span> <span class=o>=</span> <span class=n>u</span> <span class=o>*</span> <span class=n>x</span>
</span><span id=__span-232-5><a id=__codelineno-232-5 name=__codelineno-232-5 href=#__codelineno-232-5></a><span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span> <span class=o>==</span> <span class=n>u</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-233-1><a id=__codelineno-233-1 name=__codelineno-233-1 href=#__codelineno-233-1></a>&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=254-python>2.5.4 Python控制流的梯度计算<a class=headerlink href=#254-python title="Permanent link">&para;</a></h3> <p>使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。</p> <div class="tabbed-set tabbed-alternate" data-tabs=60:2><input checked=checked id=254-python-pytorch name=__tabbed_60 type=radio><input id=254-python-tensorflow name=__tabbed_60 type=radio><div class=tabbed-labels><label for=254-python-pytorch>PYTORCH</label><label for=254-python-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-234-1><a id=__codelineno-234-1 name=__codelineno-234-1 href=#__codelineno-234-1></a><span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>a</span><span class=p>):</span>
</span><span id=__span-234-2><a id=__codelineno-234-2 name=__codelineno-234-2 href=#__codelineno-234-2></a>    <span class=n>b</span> <span class=o>=</span> <span class=n>a</span> <span class=o>*</span> <span class=mi>2</span>
</span><span id=__span-234-3><a id=__codelineno-234-3 name=__codelineno-234-3 href=#__codelineno-234-3></a>    <span class=k>while</span> <span class=n>b</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span> <span class=o>&lt;</span> <span class=mi>1000</span><span class=p>:</span>
</span><span id=__span-234-4><a id=__codelineno-234-4 name=__codelineno-234-4 href=#__codelineno-234-4></a>        <span class=n>b</span> <span class=o>=</span> <span class=n>b</span> <span class=o>*</span> <span class=mi>2</span>
</span><span id=__span-234-5><a id=__codelineno-234-5 name=__codelineno-234-5 href=#__codelineno-234-5></a>    <span class=k>if</span> <span class=n>b</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-234-6><a id=__codelineno-234-6 name=__codelineno-234-6 href=#__codelineno-234-6></a>        <span class=n>c</span> <span class=o>=</span> <span class=n>b</span>
</span><span id=__span-234-7><a id=__codelineno-234-7 name=__codelineno-234-7 href=#__codelineno-234-7></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-234-8><a id=__codelineno-234-8 name=__codelineno-234-8 href=#__codelineno-234-8></a>        <span class=n>c</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>*</span> <span class=n>b</span>
</span><span id=__span-234-9><a id=__codelineno-234-9 name=__codelineno-234-9 href=#__codelineno-234-9></a>    <span class=k>return</span> <span class=n>c</span>
</span></code></pre></div> </div> <div class=tabbed-block> <p>```python def f(a): b = a * 2 while tf.norm(b) &lt; 1000: b = b * 2 if tf.reduce_sum(b) &gt; 0: c = b else: c = 100 * b return c</p> </div> </div> </div> <p>让我们计算梯度</p> <div class="tabbed-set tabbed-alternate" data-tabs=61:2><input checked=checked id=254-python-pytorch_1 name=__tabbed_61 type=radio><input id=254-python-tensorflow_1 name=__tabbed_61 type=radio><div class=tabbed-labels><label for=254-python-pytorch_1>PYTORCH</label><label for=254-python-tensorflow_1>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-235-1><a id=__codelineno-235-1 name=__codelineno-235-1 href=#__codelineno-235-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=p>(),</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-235-2><a id=__codelineno-235-2 name=__codelineno-235-2 href=#__codelineno-235-2></a><span class=n>d</span> <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span><span id=__span-235-3><a id=__codelineno-235-3 name=__codelineno-235-3 href=#__codelineno-235-3></a><span class=n>d</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-236-1><a id=__codelineno-236-1 name=__codelineno-236-1 href=#__codelineno-236-1></a><span class=n>a</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>()))</span>
</span><span id=__span-236-2><a id=__codelineno-236-2 name=__codelineno-236-2 href=#__codelineno-236-2></a><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>t</span><span class=p>:</span>
</span><span id=__span-236-3><a id=__codelineno-236-3 name=__codelineno-236-3 href=#__codelineno-236-3></a>    <span class=n>d</span> <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span><span id=__span-236-4><a id=__codelineno-236-4 name=__codelineno-236-4 href=#__codelineno-236-4></a><span class=n>d_grad</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>d</span><span class=p>,</span> <span class=n>a</span><span class=p>)</span>
</span><span id=__span-236-5><a id=__codelineno-236-5 name=__codelineno-236-5 href=#__codelineno-236-5></a><span class=n>d_grad</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-237-1><a id=__codelineno-237-1 name=__codelineno-237-1 href=#__codelineno-237-1></a>&lt;tf.Tensor: shape=(), dtype=float32, numpy=51200.0&gt;
</span></code></pre></div> </div> </div> </div> <p>我们现在可以分析上面定义的f函数。 请注意，它在其输入a中是分段线性的。 换言之，对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a，因此可以用d/a验证梯度是否正确。</p> <div class="tabbed-set tabbed-alternate" data-tabs=62:2><input checked=checked id=254-python-pytorch_2 name=__tabbed_62 type=radio><input id=254-python-tensorflow_2 name=__tabbed_62 type=radio><div class=tabbed-labels><label for=254-python-pytorch_2>PYTORCH</label><label for=254-python-tensorflow_2>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-238-1><a id=__codelineno-238-1 name=__codelineno-238-1 href=#__codelineno-238-1></a><span class=n>a</span><span class=o>.</span><span class=n>grad</span> <span class=o>==</span> <span class=n>d</span> <span class=o>/</span> <span class=n>a</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-239-1><a id=__codelineno-239-1 name=__codelineno-239-1 href=#__codelineno-239-1></a>tensor(True)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-240-1><a id=__codelineno-240-1 name=__codelineno-240-1 href=#__codelineno-240-1></a><span class=n>d_grad</span> <span class=o>==</span> <span class=n>d</span> <span class=o>/</span> <span class=n>a</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-241-1><a id=__codelineno-241-1 name=__codelineno-241-1 href=#__codelineno-241-1></a>&lt;tf.Tensor: shape=(), dtype=bool, numpy=True&gt;
</span></code></pre></div> </div> </div> </div> <h3 id=255>2.5.5 小结<a class=headerlink href=#255 title="Permanent link">&para;</a></h3> <p>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。</p> <h2 id=26>2.6 概率与统计<a class=headerlink href=#26 title="Permanent link">&para;</a></h2> <p>简单地说，机器学习就是做出预测。</p> <p>根据病人的临床病史，我们可能想预测他们在下一年心脏病发作的概率。 在飞机喷气发动机的异常检测中，我们想要评估一组发动机读数为正常运行情况的概率有多大。 在强化学习中，我们希望智能体（agent）能在一个环境中智能地行动。 这意味着我们需要考虑在每种可行的行为下获得高奖励的概率。 当我们建立推荐系统时，我们也需要考虑概率。 例如，假设我们为一家大型在线书店工作，我们可能希望估计某些用户购买特定图书的概率。 为此，我们需要使用概率学。 有完整的课程、专业、论文、职业、甚至院系，都致力于概率学的工作。 所以很自然地，我们在这部分的目标不是教授整个科目。 相反，我们希望教给读者基础的概率知识，使读者能够开始构建第一个深度学习模型， 以便读者可以开始自己探索它。</p> <h3 id=261>2.6.1 基本概率论<a class=headerlink href=#261 title="Permanent link">&para;</a></h3> <p>在统计学中，我们把从概率分布中抽取样本的过程称为抽样（sampling）。 笼统来说，可以把分布（distribution）看作对事件的概率分配， 稍后我们将给出的更正式定义。 将概率分配给一些离散选择的分布称为多项分布（multinomial distribution）。</p> <h4 id=2611>2.6.1.1 概率论公理<a class=headerlink href=#2611 title="Permanent link">&para;</a></h4> <h4 id=2612>2.6.1.2 随机变量<a class=headerlink href=#2612 title="Permanent link">&para;</a></h4> <h3 id=262>2.6.2 处理多个随机变量<a class=headerlink href=#262 title="Permanent link">&para;</a></h3> <h4 id=2621>2.6.2.1 联合概率<a class=headerlink href=#2621 title="Permanent link">&para;</a></h4> <h4 id=2622>2.6.2.2 条件概率<a class=headerlink href=#2622 title="Permanent link">&para;</a></h4> <h4 id=2623>2.6.2.3 贝叶斯定理<a class=headerlink href=#2623 title="Permanent link">&para;</a></h4> <p>使用条件概率的定义，我们可以得出统计学中最有用的方程之一： Bayes定理（Bayes’ theorem）。 根据**乘法法则**（multiplication rule ）可得到 <span class=arithmatex>\(P(A, B) = P(B \mid A) P(A)\)</span>。 根据**对称性**，<span class=arithmatex>\(P(A, B) = P(A \mid B) P(B)\)</span>。 因此，<span class=arithmatex>\(P(A \mid B) P(B) = P(B \mid A) P(A)\)</span>。 通过重新排列，我们得到**贝叶斯定理**： $$ P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$</p> <h4 id=2624>2.6.2.4 边际化 （边际概率，边际分布）<a class=headerlink href=#2624 title="Permanent link">&para;</a></h4> <p><strong>求和法则</strong></p> <h4 id=2625>2.6.2.5 独立性<a class=headerlink href=#2625 title="Permanent link">&para;</a></h4> <p>另一个有用属性是依赖（dependence）与独立（independence）。 如果两个随机变量A 和B是独立的，意味着事件A的发生跟B事件的发生无关。</p> <h4 id=2626>2.6.2.6 应用<a class=headerlink href=#2626 title="Permanent link">&para;</a></h4> <h3 id=263>2.6.3 期望和方差<a class=headerlink href=#263 title="Permanent link">&para;</a></h3> <h3 id=264>2.6.4 小结<a class=headerlink href=#264 title="Permanent link">&para;</a></h3> <ul> <li> <p>我们可以从概率分布中采样。</p> </li> <li> <p>我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。</p> </li> <li> <p>期望和方差为概率分布的关键特征的概括提供了实用的度量形式。</p> </li> </ul> <h2 id=27>2.7 文档<a class=headerlink href=#27 title="Permanent link">&para;</a></h2> <p>由于篇幅限制，本书不可能介绍每一个PyTorch函数和类。 API文档、其他教程和示例提供了本书之外的大量文档。 本节提供了一些查看PyTorch API的指导。</p> <h3 id=271>2.7.1 查找模块中的所有函数和类<a class=headerlink href=#271 title="Permanent link">&para;</a></h3> <p>要查找模块中的所有函数和类，我们可以使用<code>dir</code>函数。 例如，我们可以查询随机数生成模块中的所有属性：</p> <div class="tabbed-set tabbed-alternate" data-tabs=63:2><input checked=checked id=271-pytorch name=__tabbed_63 type=radio><input id=271-tensorflow name=__tabbed_63 type=radio><div class=tabbed-labels><label for=271-pytorch>PYTORCH</label><label for=271-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-242-1><a id=__codelineno-242-1 name=__codelineno-242-1 href=#__codelineno-242-1></a><span class=kn>import</span> <span class=nn>torch</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-243-1><a id=__codelineno-243-1 name=__codelineno-243-1 href=#__codelineno-243-1></a><span class=nb>dir</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-244-1><a id=__codelineno-244-1 name=__codelineno-244-1 href=#__codelineno-244-1></a>[&#39;Bernoulli&#39;,
</span><span id=__span-244-2><a id=__codelineno-244-2 name=__codelineno-244-2 href=#__codelineno-244-2></a>&#39;Beta&#39;,
</span><span id=__span-244-3><a id=__codelineno-244-3 name=__codelineno-244-3 href=#__codelineno-244-3></a>&#39;Binomial&#39;,
</span><span id=__span-244-4><a id=__codelineno-244-4 name=__codelineno-244-4 href=#__codelineno-244-4></a>&#39;Categorical&#39;,
</span><span id=__span-244-5><a id=__codelineno-244-5 name=__codelineno-244-5 href=#__codelineno-244-5></a>&#39;Cauchy&#39;,
</span><span id=__span-244-6><a id=__codelineno-244-6 name=__codelineno-244-6 href=#__codelineno-244-6></a>&#39;Chi2&#39;,
</span><span id=__span-244-7><a id=__codelineno-244-7 name=__codelineno-244-7 href=#__codelineno-244-7></a>&#39;Dirichlet&#39;,
</span><span id=__span-244-8><a id=__codelineno-244-8 name=__codelineno-244-8 href=#__codelineno-244-8></a>&#39;Distribution&#39;,
</span><span id=__span-244-9><a id=__codelineno-244-9 name=__codelineno-244-9 href=#__codelineno-244-9></a>&#39;Exponential&#39;,
</span><span id=__span-244-10><a id=__codelineno-244-10 name=__codelineno-244-10 href=#__codelineno-244-10></a>&#39;ExponentialFamily&#39;,
</span><span id=__span-244-11><a id=__codelineno-244-11 name=__codelineno-244-11 href=#__codelineno-244-11></a>&#39;FisherSnedecor&#39;,
</span><span id=__span-244-12><a id=__codelineno-244-12 name=__codelineno-244-12 href=#__codelineno-244-12></a>&#39;Gamma&#39;,
</span><span id=__span-244-13><a id=__codelineno-244-13 name=__codelineno-244-13 href=#__codelineno-244-13></a>&#39;Geometric&#39;,
</span><span id=__span-244-14><a id=__codelineno-244-14 name=__codelineno-244-14 href=#__codelineno-244-14></a>&#39;Gumbel&#39;,
</span><span id=__span-244-15><a id=__codelineno-244-15 name=__codelineno-244-15 href=#__codelineno-244-15></a>&#39;HalfCauchy&#39;,
</span><span id=__span-244-16><a id=__codelineno-244-16 name=__codelineno-244-16 href=#__codelineno-244-16></a>&#39;HalfNormal&#39;,
</span><span id=__span-244-17><a id=__codelineno-244-17 name=__codelineno-244-17 href=#__codelineno-244-17></a>&#39;Independent&#39;,
</span><span id=__span-244-18><a id=__codelineno-244-18 name=__codelineno-244-18 href=#__codelineno-244-18></a>&#39;KLDivLoss&#39;,
</span><span id=__span-244-19><a id=__codelineno-244-19 name=__codelineno-244-19 href=#__codelineno-244-19></a>&#39;LKJCholesky&#39;,
</span><span id=__span-244-20><a id=__codelineno-244-20 name=__codelineno-244-20 href=#__codelineno-244-20></a>&#39;Laplace&#39;,
</span><span id=__span-244-21><a id=__codelineno-244-21 name=__codelineno-244-21 href=#__codelineno-244-21></a>&#39;LowRankMultivariateNormal&#39;,
</span><span id=__span-244-22><a id=__codelineno-244-22 name=__codelineno-244-22 href=#__codelineno-244-22></a>&#39;MixtureSameFamily&#39;,
</span><span id=__span-244-23><a id=__codelineno-244-23 name=__codelineno-244-23 href=#__codelineno-244-23></a>&#39;Multinomial&#39;,
</span><span id=__span-244-24><a id=__codelineno-244-24 name=__codelineno-244-24 href=#__codelineno-244-24></a>&#39;MultivariateNormal&#39;,
</span><span id=__span-244-25><a id=__codelineno-244-25 name=__codelineno-244-25 href=#__codelineno-244-25></a>&#39;NegativeBinomial&#39;,
</span><span id=__span-244-26><a id=__codelineno-244-26 name=__codelineno-244-26 href=#__codelineno-244-26></a>&#39;Normal&#39;,
</span><span id=__span-244-27><a id=__codelineno-244-27 name=__codelineno-244-27 href=#__codelineno-244-27></a>&#39;OneHotCategorical&#39;,
</span><span id=__span-244-28><a id=__codelineno-244-28 name=__codelineno-244-28 href=#__codelineno-244-28></a>&#39;Pareto&#39;,
</span><span id=__span-244-29><a id=__codelineno-244-29 name=__codelineno-244-29 href=#__codelineno-244-29></a>&#39;Poisson&#39;,
</span><span id=__span-244-30><a id=__codelineno-244-30 name=__codelineno-244-30 href=#__codelineno-244-30></a>&#39;RelaxedBernoulli&#39;,
</span><span id=__span-244-31><a id=__codelineno-244-31 name=__codelineno-244-31 href=#__codelineno-244-31></a>&#39;RelaxedOneHotCategorical&#39;,
</span><span id=__span-244-32><a id=__codelineno-244-32 name=__codelineno-244-32 href=#__codelineno-244-32></a>&#39;StudentT&#39;,
</span><span id=__span-244-33><a id=__codelineno-244-33 name=__codelineno-244-33 href=#__codelineno-244-33></a>&#39;TransformedDistribution&#39;,
</span><span id=__span-244-34><a id=__codelineno-244-34 name=__codelineno-244-34 href=#__codelineno-244-34></a>&#39;Uniform&#39;,
</span><span id=__span-244-35><a id=__codelineno-244-35 name=__codelineno-244-35 href=#__codelineno-244-35></a>&#39;VonMises&#39;,
</span><span id=__span-244-36><a id=__codelineno-244-36 name=__codelineno-244-36 href=#__codelineno-244-36></a>&#39;Weibull&#39;,
</span><span id=__span-244-37><a id=__codelineno-244-37 name=__codelineno-244-37 href=#__codelineno-244-37></a>&#39;Zipf&#39;,
</span><span id=__span-244-38><a id=__codelineno-244-38 name=__codelineno-244-38 href=#__codelineno-244-38></a>&#39;__builtins__&#39;,
</span><span id=__span-244-39><a id=__codelineno-244-39 name=__codelineno-244-39 href=#__codelineno-244-39></a>&#39;__cached__&#39;,
</span><span id=__span-244-40><a id=__codelineno-244-40 name=__codelineno-244-40 href=#__codelineno-244-40></a>&#39;__doc__&#39;,
</span><span id=__span-244-41><a id=__codelineno-244-41 name=__codelineno-244-41 href=#__codelineno-244-41></a>&#39;__file__&#39;,
</span><span id=__span-244-42><a id=__codelineno-244-42 name=__codelineno-244-42 href=#__codelineno-244-42></a>&#39;__loader__&#39;,
</span><span id=__span-244-43><a id=__codelineno-244-43 name=__codelineno-244-43 href=#__codelineno-244-43></a>&#39;__name__&#39;,
</span><span id=__span-244-44><a id=__codelineno-244-44 name=__codelineno-244-44 href=#__codelineno-244-44></a>&#39;__package__&#39;,
</span><span id=__span-244-45><a id=__codelineno-244-45 name=__codelineno-244-45 href=#__codelineno-244-45></a>&#39;__path__&#39;,
</span><span id=__span-244-46><a id=__codelineno-244-46 name=__codelineno-244-46 href=#__codelineno-244-46></a>&#39;__spec__&#39;,
</span><span id=__span-244-47><a id=__codelineno-244-47 name=__codelineno-244-47 href=#__codelineno-244-47></a>&#39;_lazy_init&#39;,
</span><span id=__span-244-48><a id=__codelineno-244-48 name=__codelineno-244-48 href=#__codelineno-244-48></a>&#39;_standard_gamma&#39;,
</span><span id=__span-244-49><a id=__codelineno-244-49 name=__codelineno-244-49 href=#__codelineno-244-49></a>&#39;constraints&#39;,
</span><span id=__span-244-50><a id=__codelineno-244-50 name=__codelineno-244-50 href=#__codelineno-244-50></a>&#39;functional&#39;,
</span><span id=__span-244-51><a id=__codelineno-244-51 name=__codelineno-244-51 href=#__codelineno-244-51></a>&#39;kl_divergence&#39;,
</span><span id=__span-244-52><a id=__codelineno-244-52 name=__codelineno-244-52 href=#__codelineno-244-52></a>&#39;lazy_property&#39;,
</span><span id=__span-244-53><a id=__codelineno-244-53 name=__codelineno-244-53 href=#__codelineno-244-53></a>&#39;register_kl&#39;,
</span><span id=__span-244-54><a id=__codelineno-244-54 name=__codelineno-244-54 href=#__codelineno-244-54></a>&#39;transform_to&#39;,
</span><span id=__span-244-55><a id=__codelineno-244-55 name=__codelineno-244-55 href=#__codelineno-244-55></a>&#39;utils&#39;]
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-245-1><a id=__codelineno-245-1 name=__codelineno-245-1 href=#__codelineno-245-1></a><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-246-1><a id=__codelineno-246-1 name=__codelineno-246-1 href=#__codelineno-246-1></a><span class=nb>dir</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>distributions</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-247-1><a id=__codelineno-247-1 name=__codelineno-247-1 href=#__codelineno-247-1></a>    [&#39;Bernoulli&#39;,
</span><span id=__span-247-2><a id=__codelineno-247-2 name=__codelineno-247-2 href=#__codelineno-247-2></a>&#39;Beta&#39;,
</span><span id=__span-247-3><a id=__codelineno-247-3 name=__codelineno-247-3 href=#__codelineno-247-3></a>&#39;Binomial&#39;,
</span><span id=__span-247-4><a id=__codelineno-247-4 name=__codelineno-247-4 href=#__codelineno-247-4></a>&#39;Categorical&#39;,
</span><span id=__span-247-5><a id=__codelineno-247-5 name=__codelineno-247-5 href=#__codelineno-247-5></a>&#39;Cauchy&#39;,
</span><span id=__span-247-6><a id=__codelineno-247-6 name=__codelineno-247-6 href=#__codelineno-247-6></a>&#39;Chi2&#39;,
</span><span id=__span-247-7><a id=__codelineno-247-7 name=__codelineno-247-7 href=#__codelineno-247-7></a>&#39;Dirichlet&#39;,
</span><span id=__span-247-8><a id=__codelineno-247-8 name=__codelineno-247-8 href=#__codelineno-247-8></a>&#39;Distribution&#39;,
</span><span id=__span-247-9><a id=__codelineno-247-9 name=__codelineno-247-9 href=#__codelineno-247-9></a>&#39;Exponential&#39;,
</span><span id=__span-247-10><a id=__codelineno-247-10 name=__codelineno-247-10 href=#__codelineno-247-10></a>&#39;ExponentialFamily&#39;,
</span><span id=__span-247-11><a id=__codelineno-247-11 name=__codelineno-247-11 href=#__codelineno-247-11></a>&#39;FisherSnedecor&#39;,
</span><span id=__span-247-12><a id=__codelineno-247-12 name=__codelineno-247-12 href=#__codelineno-247-12></a>&#39;Gamma&#39;,
</span><span id=__span-247-13><a id=__codelineno-247-13 name=__codelineno-247-13 href=#__codelineno-247-13></a>&#39;Geometric&#39;,
</span><span id=__span-247-14><a id=__codelineno-247-14 name=__codelineno-247-14 href=#__codelineno-247-14></a>&#39;Gumbel&#39;,
</span><span id=__span-247-15><a id=__codelineno-247-15 name=__codelineno-247-15 href=#__codelineno-247-15></a>&#39;HalfCauchy&#39;,
</span><span id=__span-247-16><a id=__codelineno-247-16 name=__codelineno-247-16 href=#__codelineno-247-16></a>&#39;HalfNormal&#39;,
</span><span id=__span-247-17><a id=__codelineno-247-17 name=__codelineno-247-17 href=#__codelineno-247-17></a>&#39;Independent&#39;,
</span><span id=__span-247-18><a id=__codelineno-247-18 name=__codelineno-247-18 href=#__codelineno-247-18></a>&#39;KLDivLoss&#39;,
</span><span id=__span-247-19><a id=__codelineno-247-19 name=__codelineno-247-19 href=#__codelineno-247-19></a>&#39;LKJCholesky&#39;,
</span><span id=__span-247-20><a id=__codelineno-247-20 name=__codelineno-247-20 href=#__codelineno-247-20></a>&#39;Laplace&#39;,
</span><span id=__span-247-21><a id=__codelineno-247-21 name=__codelineno-247-21 href=#__codelineno-247-21></a>&#39;LowRankMultivariateNormal&#39;,
</span><span id=__span-247-22><a id=__codelineno-247-22 name=__codelineno-247-22 href=#__codelineno-247-22></a>&#39;MixtureSameFamily&#39;,
</span><span id=__span-247-23><a id=__codelineno-247-23 name=__codelineno-247-23 href=#__codelineno-247-23></a>&#39;Multinomial&#39;,
</span><span id=__span-247-24><a id=__codelineno-247-24 name=__codelineno-247-24 href=#__codelineno-247-24></a>&#39;MultivariateNormal&#39;,
</span><span id=__span-247-25><a id=__codelineno-247-25 name=__codelineno-247-25 href=#__codelineno-247-25></a>&#39;NegativeBinomial&#39;,
</span><span id=__span-247-26><a id=__codelineno-247-26 name=__codelineno-247-26 href=#__codelineno-247-26></a>&#39;Normal&#39;,
</span><span id=__span-247-27><a id=__codelineno-247-27 name=__codelineno-247-27 href=#__codelineno-247-27></a>&#39;OneHotCategorical&#39;,
</span><span id=__span-247-28><a id=__codelineno-247-28 name=__codelineno-247-28 href=#__codelineno-247-28></a>&#39;Pareto&#39;,
</span><span id=__span-247-29><a id=__codelineno-247-29 name=__codelineno-247-29 href=#__codelineno-247-29></a>&#39;Poisson&#39;,
</span><span id=__span-247-30><a id=__codelineno-247-30 name=__codelineno-247-30 href=#__codelineno-247-30></a>&#39;RelaxedBernoulli&#39;,
</span><span id=__span-247-31><a id=__codelineno-247-31 name=__codelineno-247-31 href=#__codelineno-247-31></a>&#39;RelaxedOneHotCategorical&#39;,
</span><span id=__span-247-32><a id=__codelineno-247-32 name=__codelineno-247-32 href=#__codelineno-247-32></a>&#39;StudentT&#39;,
</span><span id=__span-247-33><a id=__codelineno-247-33 name=__codelineno-247-33 href=#__codelineno-247-33></a>&#39;TransformedDistribution&#39;,
</span><span id=__span-247-34><a id=__codelineno-247-34 name=__codelineno-247-34 href=#__codelineno-247-34></a>&#39;Uniform&#39;,
</span><span id=__span-247-35><a id=__codelineno-247-35 name=__codelineno-247-35 href=#__codelineno-247-35></a>&#39;VonMises&#39;,
</span><span id=__span-247-36><a id=__codelineno-247-36 name=__codelineno-247-36 href=#__codelineno-247-36></a>&#39;Weibull&#39;,
</span><span id=__span-247-37><a id=__codelineno-247-37 name=__codelineno-247-37 href=#__codelineno-247-37></a>&#39;Zipf&#39;,
</span><span id=__span-247-38><a id=__codelineno-247-38 name=__codelineno-247-38 href=#__codelineno-247-38></a>&#39;__builtins__&#39;,
</span><span id=__span-247-39><a id=__codelineno-247-39 name=__codelineno-247-39 href=#__codelineno-247-39></a>&#39;__cached__&#39;,
</span><span id=__span-247-40><a id=__codelineno-247-40 name=__codelineno-247-40 href=#__codelineno-247-40></a>&#39;__doc__&#39;,
</span><span id=__span-247-41><a id=__codelineno-247-41 name=__codelineno-247-41 href=#__codelineno-247-41></a>&#39;__file__&#39;,
</span><span id=__span-247-42><a id=__codelineno-247-42 name=__codelineno-247-42 href=#__codelineno-247-42></a>&#39;__loader__&#39;,
</span><span id=__span-247-43><a id=__codelineno-247-43 name=__codelineno-247-43 href=#__codelineno-247-43></a>&#39;__name__&#39;,
</span><span id=__span-247-44><a id=__codelineno-247-44 name=__codelineno-247-44 href=#__codelineno-247-44></a>&#39;__package__&#39;,
</span><span id=__span-247-45><a id=__codelineno-247-45 name=__codelineno-247-45 href=#__codelineno-247-45></a>&#39;__path__&#39;,
</span><span id=__span-247-46><a id=__codelineno-247-46 name=__codelineno-247-46 href=#__codelineno-247-46></a>&#39;__spec__&#39;,
</span><span id=__span-247-47><a id=__codelineno-247-47 name=__codelineno-247-47 href=#__codelineno-247-47></a>&#39;_lazy_init&#39;,
</span><span id=__span-247-48><a id=__codelineno-247-48 name=__codelineno-247-48 href=#__codelineno-247-48></a>&#39;_standard_gamma&#39;,
</span><span id=__span-247-49><a id=__codelineno-247-49 name=__codelineno-247-49 href=#__codelineno-247-49></a>&#39;constraints&#39;,
</span><span id=__span-247-50><a id=__codelineno-247-50 name=__codelineno-247-50 href=#__codelineno-247-50></a>&#39;functional&#39;,
</span><span id=__span-247-51><a id=__codelineno-247-51 name=__codelineno-247-51 href=#__codelineno-247-51></a>&#39;kl_divergence&#39;,
</span><span id=__span-247-52><a id=__codelineno-247-52 name=__codelineno-247-52 href=#__codelineno-247-52></a>&#39;lazy_property&#39;,
</span><span id=__span-247-53><a id=__codelineno-247-53 name=__codelineno-247-53 href=#__codelineno-247-53></a>&#39;register_kl&#39;,
</span><span id=__span-247-54><a id=__codelineno-247-54 name=__codelineno-247-54 href=#__codelineno-247-54></a>&#39;transform_to&#39;,
</span><span id=__span-247-55><a id=__codelineno-247-55 name=__codelineno-247-55 href=#__codelineno-247-55></a>&#39;utils&#39;]
</span></code></pre></div> </div> </div> </div> <p>通常可以忽略以“__”（双下划线）开始和结束的函数，它们是Python中的特殊对象， 或以单个“_”（单下划线）开始的函数，它们通常是内部函数。 根据剩余的函数名或属性名，我们可能会猜测这个模块提供了各种生成随机数的方法， 包括从均匀分布（uniform）、正态分布（normal）和多项分布（multinomial）中采样。</p> <h3 id=272>2.7.2 查找特定函数和类的所有成员<a class=headerlink href=#272 title="Permanent link">&para;</a></h3> <p>我们可以使用<code>help</code>函数来查找特定函数或类的所有成员。 让我们从<code>torch.randn</code>函数开始。 如果我们不知道如何使用它，我们可以调用它并使用<code>help</code>函数来查找用法。</p> <div class="tabbed-set tabbed-alternate" data-tabs=64:2><input checked=checked id=272-pytorch name=__tabbed_64 type=radio><input id=272-tensorflow name=__tabbed_64 type=radio><div class=tabbed-labels><label for=272-pytorch>PYTORCH</label><label for=272-tensorflow>TENSORFLOW</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-248-1><a id=__codelineno-248-1 name=__codelineno-248-1 href=#__codelineno-248-1></a><span class=n>help</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-249-1><a id=__codelineno-249-1 name=__codelineno-249-1 href=#__codelineno-249-1></a>Help on built-in function randn:
</span><span id=__span-249-2><a id=__codelineno-249-2 name=__codelineno-249-2 href=#__codelineno-249-2></a>
</span><span id=__span-249-3><a id=__codelineno-249-3 name=__codelineno-249-3 href=#__codelineno-249-3></a>randn(...)
</span><span id=__span-249-4><a id=__codelineno-249-4 name=__codelineno-249-4 href=#__codelineno-249-4></a>    randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor
</span><span id=__span-249-5><a id=__codelineno-249-5 name=__codelineno-249-5 href=#__codelineno-249-5></a>
</span><span id=__span-249-6><a id=__codelineno-249-6 name=__codelineno-249-6 href=#__codelineno-249-6></a>    Returns a tensor filled with random numbers from a normal distribution
</span><span id=__span-249-7><a id=__codelineno-249-7 name=__codelineno-249-7 href=#__codelineno-249-7></a>    with mean `0` and variance `1` (also called the standard normal
</span><span id=__span-249-8><a id=__codelineno-249-8 name=__codelineno-249-8 href=#__codelineno-249-8></a>    distribution).
</span><span id=__span-249-9><a id=__codelineno-249-9 name=__codelineno-249-9 href=#__codelineno-249-9></a>
</span><span id=__span-249-10><a id=__codelineno-249-10 name=__codelineno-249-10 href=#__codelineno-249-10></a>    .. math::
</span><span id=__span-249-11><a id=__codelineno-249-11 name=__codelineno-249-11 href=#__codelineno-249-11></a>        \text{{out}}_{{i}} \sim \mathcal{{N}}(0, 1)
</span><span id=__span-249-12><a id=__codelineno-249-12 name=__codelineno-249-12 href=#__codelineno-249-12></a>
</span><span id=__span-249-13><a id=__codelineno-249-13 name=__codelineno-249-13 href=#__codelineno-249-13></a>    The shape of the tensor is defined by the variable argument `size`.
</span><span id=__span-249-14><a id=__codelineno-249-14 name=__codelineno-249-14 href=#__codelineno-249-14></a>
</span><span id=__span-249-15><a id=__codelineno-249-15 name=__codelineno-249-15 href=#__codelineno-249-15></a>    Args:
</span><span id=__span-249-16><a id=__codelineno-249-16 name=__codelineno-249-16 href=#__codelineno-249-16></a>        {input}
</span><span id=__span-249-17><a id=__codelineno-249-17 name=__codelineno-249-17 href=#__codelineno-249-17></a>
</span><span id=__span-249-18><a id=__codelineno-249-18 name=__codelineno-249-18 href=#__codelineno-249-18></a>    Example::
</span><span id=__span-249-19><a id=__codelineno-249-19 name=__codelineno-249-19 href=#__codelineno-249-19></a>
</span><span id=__span-249-20><a id=__codelineno-249-20 name=__codelineno-249-20 href=#__codelineno-249-20></a>        &gt;&gt;&gt; torch.randn(4)
</span><span id=__span-249-21><a id=__codelineno-249-21 name=__codelineno-249-21 href=#__codelineno-249-21></a>        tensor([-2.1436,  0.9966,  0.6479, -0.4219])
</span><span id=__span-249-22><a id=__codelineno-249-22 name=__codelineno-249-22 href=#__codelineno-249-22></a>        &gt;&gt;&gt; torch.randn(2, 3)
</span><span id=__span-249-23><a id=__codelineno-249-23 name=__codelineno-249-23 href=#__codelineno-249-23></a>        tensor([[ 1.2074, -0.9477, -0.1569],
</span><span id=__span-249-24><a id=__codelineno-249-24 name=__codelineno-249-24 href=#__codelineno-249-24></a>                [ 0.1886, -0.3753, -0.1837]])
</span><span id=__span-249-25><a id=__codelineno-249-25 name=__codelineno-249-25 href=#__codelineno-249-25></a>    &quot;&quot;&quot;
</span><span id=__span-249-26><a id=__codelineno-249-26 name=__codelineno-249-26 href=#__codelineno-249-26></a>    return torch._C._VariableFunctions.randn(size, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad)
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-250-1><a id=__codelineno-250-1 name=__codelineno-250-1 href=#__codelineno-250-1></a><span class=n>help</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code><span id=__span-251-1><a id=__codelineno-251-1 name=__codelineno-251-1 href=#__codelineno-251-1></a>Help on built-in function randn:
</span><span id=__span-251-2><a id=__codelineno-251-2 name=__codelineno-251-2 href=#__codelineno-251-2></a>
</span><span id=__span-251-3><a id=__codelineno-251-3 name=__codelineno-251-3 href=#__codelineno-251-3></a>randn(...)
</span><span id=__span-251-4><a id=__codelineno-251-4 name=__codelineno-251-4 href=#__codelineno-251-4></a>    randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor
</span><span id=__span-251-5><a id=__codelineno-251-5 name=__codelineno-251-5 href=#__codelineno-251-5></a>
</span><span id=__span-251-6><a id=__codelineno-251-6 name=__codelineno-251-6 href=#__codelineno-251-6></a>    Returns a tensor filled with random numbers from a normal distribution
</span><span id=__span-251-7><a id=__codelineno-251-7 name=__codelineno-251-7 href=#__codelineno-251-7></a>    with mean `0` and variance `1` (also called the standard normal
</span><span id=__span-251-8><a id=__codelineno-251-8 name=__codelineno-251-8 href=#__codelineno-251-8></a>    distribution).
</span><span id=__span-251-9><a id=__codelineno-251-9 name=__codelineno-251-9 href=#__codelineno-251-9></a>
</span><span id=__span-251-10><a id=__codelineno-251-10 name=__codelineno-251-10 href=#__codelineno-251-10></a>    .. math::
</span><span id=__span-251-11><a id=__codelineno-251-11 name=__codelineno-251-11 href=#__codelineno-251-11></a>        \text{{out}}_{{i}} \sim \mathcal{{N}}(0, 1)
</span><span id=__span-251-12><a id=__codelineno-251-12 name=__codelineno-251-12 href=#__codelineno-251-12></a>
</span><span id=__span-251-13><a id=__codelineno-251-13 name=__codelineno-251-13 href=#__codelineno-251-13></a>    The shape of the tensor is defined by the variable argument `size`.
</span><span id=__span-251-14><a id=__codelineno-251-14 name=__codelineno-251-14 href=#__codelineno-251-14></a>
</span><span id=__span-251-15><a id=__codelineno-251-15 name=__codelineno-251-15 href=#__codelineno-251-15></a>    Args:
</span><span id=__span-251-16><a id=__codelineno-251-16 name=__codelineno-251-16 href=#__codelineno-251-16></a>        {input}
</span><span id=__span-251-17><a id=__codelineno-251-17 name=__codelineno-251-17 href=#__codelineno-251-17></a>
</span><span id=__span-251-18><a id=__codelineno-251-18 name=__codelineno-251-18 href=#__codelineno-251-18></a>    Example::
</span><span id=__span-251-19><a id=__codelineno-251-19 name=__codelineno-251-19 href=#__codelineno-251-19></a>
</span><span id=__span-251-20><a id=__codelineno-251-20 name=__codelineno-251-20 href=#__codelineno-251-20></a>        &gt;&gt;&gt; torch.randn(4)
</span><span id=__span-251-21><a id=__codelineno-251-21 name=__codelineno-251-21 href=#__codelineno-251-21></a>        tensor([-2.1436,  0.9966,  0.6479, -0.4219])
</span><span id=__span-251-22><a id=__codelineno-251-22 name=__codelineno-251-22 href=#__codelineno-251-22></a>        &gt;&gt;&gt; torch.randn(2, 3)
</span><span id=__span-251-23><a id=__codelineno-251-23 name=__codelineno-251-23 href=#__codelineno-251-23></a>        tensor([[ 1.2074, -0.9477, -0.1569],
</span><span id=__span-251-24><a id=__codelineno-251-24 name=__codelineno-251-24 href=#__codelineno-251-24></a>                [ 0.1886, -0.3753, -0.1837]])
</span><span id=__span-251-25><a id=__codelineno-251-25 name=__codelineno-251-25 href=#__codelineno-251-25></a>    &quot;&quot;&quot;
</span><span id=__span-251-26><a id=__codelineno-251-26 name=__codelineno-251-26 href=#__codelineno-251-26></a>    return torch._C._VariableFunctions.randn(size, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad)
</span></code></pre></div> </div> </div> </div> <h3 id=273>2.7.3 小结<a class=headerlink href=#273 title="Permanent link">&para;</a></h3> <ul> <li> <p>官方文档提供了本书之外的大量描述和示例。</p> </li> <li> <p>可以通过调用dir和help函数或在Jupyter记事本中使用?和??查看API的用法文档。</p> </li> </ul> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5 9v12H1V9h4m4 12a2 2 0 0 1-2-2V9c0-.55.22-1.05.59-1.41L14.17 1l1.06 1.06c.27.27.44.64.44 1.05l-.03.32L14.69 8H21a2 2 0 0 1 2 2v2c0 .26-.05.5-.14.73l-3.02 7.05C19.54 20.5 18.83 21 18 21H9m0-2h9.03L21 12v-2h-8.79l1.13-5.32L9 9.03V19Z"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 15V3h4v12h-4M15 3a2 2 0 0 1 2 2v10c0 .55-.22 1.05-.59 1.41L9.83 23l-1.06-1.06c-.27-.27-.44-.64-.44-1.06l.03-.31.95-4.57H3a2 2 0 0 1-2-2v-2c0-.26.05-.5.14-.73l3.02-7.05C4.46 3.5 5.17 3 6 3h9m0 2H5.97L3 12v2h8.78l-1.13 5.32L15 14.97V5Z"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve this page by using our <a href=... target=_blank rel=noopener>feedback form</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=页脚> <a href=../../CH1-INTRO/ch1-intro/ class="md-footer__link md-footer__link--prev" aria-label="上一页: CH1-引言"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 320 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> 上一页 </span> <div class=md-ellipsis> CH1-引言 </div> </div> </a> <a href=../../CH3-LNN/ch3-lnn/ class="md-footer__link md-footer__link--next" aria-label="下一页: CH3-线性神经网络"> <div class=md-footer__title> <span class=md-footer__direction> 下一页 </span> <div class=md-ellipsis> CH3-线性神经网络 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 320 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2023 ~ now | 🚀 Shuaiwen Cui (Shaun) </div> </div> <div class=md-social> <a href=http://www.cuishuaiwen.com/ target=_blank rel=noopener title=www.cuishuaiwen.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1V472c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2 0-3.3-.1-1.4.1-2.8.1-4.2.1H392c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9.1-2.8v-69.6H32c-18 0-32-14-32-32.1 0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7l255.4 224.5c8 7 12 15 11 24z"/></svg> </a> <a href=https://github.com/Shuaiwen-Cui target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://www.linkedin.com/in/shaun-shuaiwen-cui/ target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> <a href=https://www.researchgate.net/profile/Shuaiwen-Cui target=_blank rel=noopener title=www.researchgate.net class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M0 32v448h448V32H0zm262.2 334.4c-6.6 3-33.2 6-50-14.2-9.2-10.6-25.3-33.3-42.2-63.6-8.9 0-14.7 0-21.4-.6v46.4c0 23.5 6 21.2 25.8 23.9v8.1c-6.9-.3-23.1-.8-35.6-.8-13.1 0-26.1.6-33.6.8v-8.1c15.5-2.9 22-1.3 22-23.9V225c0-22.6-6.4-21-22-23.9V193c25.8 1 53.1-.6 70.9-.6 31.7 0 55.9 14.4 55.9 45.6 0 21.1-16.7 42.2-39.2 47.5 13.6 24.2 30 45.6 42.2 58.9 7.2 7.8 17.2 14.7 27.2 14.7v7.3zm22.9-135c-23.3 0-32.2-15.7-32.2-32.2V167c0-12.2 8.8-30.4 34-30.4s30.4 17.9 30.4 17.9l-10.7 7.2s-5.5-12.5-19.7-12.5c-7.9 0-19.7 7.3-19.7 19.7v26.8c0 13.4 6.6 23.3 17.9 23.3 14.1 0 21.5-10.9 21.5-26.8h-17.9v-10.7h30.4c0 20.5 4.7 49.9-34 49.9zm-116.5 44.7c-9.4 0-13.6-.3-20-.8v-69.7c6.4-.6 15-.6 22.5-.6 23.3 0 37.2 12.2 37.2 34.5 0 21.9-15 36.6-39.7 36.6z"/></svg> </a> <a href=https://orcid.org/0000-0003-4447-6687 target=_blank rel=noopener title=orcid.org class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M294.75 188.19h-45.92V342h47.47c67.62 0 83.12-51.34 83.12-76.91 0-41.64-26.54-76.9-84.67-76.9zM256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm-80.79 360.76h-29.84v-207.5h29.84zm-14.92-231.14a19.57 19.57 0 1 1 19.57-19.57 19.64 19.64 0 0 1-19.57 19.57zM300 369h-81V161.26h80.6c76.73 0 110.44 54.83 110.44 103.85C410 318.39 368.38 369 300 369z"/></svg> </a> <a href=https://twitter.com/ShuaiwenC target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../../../assets/javascripts/workers/search.f886a092.min.js", "tags": {"Default": "default-tag", "Hardware": "hardware-tag", "Software": "software-tag"}, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../../../../assets/javascripts/bundle.d7c377c4.min.js></script> <script src=/CODING/PYTHON/Book1_Python-For-Beginners_编程不难.pdf></script> <script src=../../../../../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../../../javascripts/embed-pdf.js></script> <script src=../../../../../assets/javascripts/custom.2340dcd7.min.js></script> </body> </html>